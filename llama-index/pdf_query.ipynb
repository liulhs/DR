{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./index/pdf\")\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import (\n",
    "    QueryPipeline,\n",
    "    InputComponent,\n",
    "    ArgPackComponent,\n",
    ")\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
    "\n",
    "# First, we create an input component to capture the user query\n",
    "input_component = InputComponent()\n",
    "\n",
    "# Next, we use the LLM to rewrite a user query\n",
    "rewrite = (\n",
    "    \"Please write a query to a semantic search engine using the current conversation.\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{chat_history_str}\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"Latest message: {query_str}\\n\"\n",
    "    'Query:\"\"\"\\n'\n",
    ")\n",
    "rewrite_template = PromptTemplate(rewrite)\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# we will retrieve two times, so we need to pack the retrieved nodes into a single list\n",
    "argpack_component = ArgPackComponent()\n",
    "\n",
    "# using that, we will retrieve...\n",
    "retriever = index.as_retriever(similarity_top_k=6)\n",
    "\n",
    "# then postprocess/rerank with Colbert\n",
    "reranker = ColbertRerank(top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then lastly, we need to create a response using the nodes AND chat history\n",
    "from typing import Any, Dict, List, Optional\n",
    "from llama_index.core.bridge.pydantic import Field\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.query_pipeline import CustomQueryComponent\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "\n",
    "DEFAULT_CONTEXT_PROMPT = (\n",
    "    \"Here is some context that may be relevant:\\n\"\n",
    "    \"-----\\n\"\n",
    "    \"{node_context}\\n\"\n",
    "    \"-----\\n\"\n",
    "    \"Please write a response to the following question, using the above context: \\n\"\n",
    "    \"{query_str}\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "class ResponseWithChatHistory(CustomQueryComponent):\n",
    "    llm: OpenAI = Field(..., description=\"OpenAI LLM\")\n",
    "    system_prompt: Optional[str] = Field(\n",
    "        default=None, description=\"System prompt to use for the LLM\"\n",
    "    )\n",
    "    context_prompt: str = Field(\n",
    "        default=DEFAULT_CONTEXT_PROMPT,\n",
    "        description=\"Context prompt to use for the LLM\",\n",
    "    )\n",
    "\n",
    "    def _validate_component_inputs(\n",
    "        self, input: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Validate component inputs during run_component.\"\"\"\n",
    "        # NOTE: this is OPTIONAL but we show you where to do validation as an example\n",
    "        return input\n",
    "\n",
    "    @property\n",
    "    def _input_keys(self) -> set:\n",
    "        \"\"\"Input keys dict.\"\"\"\n",
    "        # NOTE: These are required inputs. If you have optional inputs please override\n",
    "        # `optional_input_keys_dict`\n",
    "        return {\"chat_history\", \"nodes\", \"query_str\"}\n",
    "\n",
    "    @property\n",
    "    def _output_keys(self) -> set:\n",
    "        return {\"response\"}\n",
    "\n",
    "    def _prepare_context(\n",
    "        self,\n",
    "        chat_history: List[ChatMessage],\n",
    "        nodes: List[NodeWithScore],\n",
    "        query_str: str,\n",
    "    ) -> List[ChatMessage]:\n",
    "        node_context = \"\"\n",
    "        for idx, node in enumerate(nodes):\n",
    "            node_text = node.get_content(metadata_mode=\"llm\")\n",
    "            node_context += f\"Context Chunk {idx}:\\n{node_text}\\n\\n\"\n",
    "\n",
    "        formatted_context = self.context_prompt.format(\n",
    "            node_context=node_context, query_str=query_str\n",
    "        )\n",
    "        user_message = ChatMessage(role=\"user\", content=formatted_context)\n",
    "\n",
    "        chat_history.append(user_message)\n",
    "\n",
    "        if self.system_prompt is not None:\n",
    "            chat_history = [\n",
    "                ChatMessage(role=\"system\", content=self.system_prompt)\n",
    "            ] + chat_history\n",
    "\n",
    "        return chat_history\n",
    "\n",
    "    def _run_component(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Run the component.\"\"\"\n",
    "        chat_history = kwargs[\"chat_history\"]\n",
    "        nodes = kwargs[\"nodes\"]\n",
    "        query_str = kwargs[\"query_str\"]\n",
    "\n",
    "        prepared_context = self._prepare_context(\n",
    "            chat_history, nodes, query_str\n",
    "        )\n",
    "\n",
    "        response = llm.chat(prepared_context)\n",
    "\n",
    "        return {\"response\": response}\n",
    "\n",
    "    async def _arun_component(self, **kwargs: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Run the component asynchronously.\"\"\"\n",
    "        # NOTE: Optional, but async LLM calls are easy to implement\n",
    "        chat_history = kwargs[\"chat_history\"]\n",
    "        nodes = kwargs[\"nodes\"]\n",
    "        query_str = kwargs[\"query_str\"]\n",
    "\n",
    "        prepared_context = self._prepare_context(\n",
    "            chat_history, nodes, query_str\n",
    "        )\n",
    "\n",
    "        response = await llm.achat(prepared_context)\n",
    "\n",
    "        return {\"response\": response}\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    \"\"\"Object representing a single knowledge pdf file.\"\"\"\n",
    "\n",
    "    answer: str = Field(..., description=\"Your Answer to the given query\")\n",
    "    file_name: str = Field(..., description=\"PDF file's file name where the answer can be found, fill in with empty string if you couldn't find it\")\n",
    "    page_number: int = Field(..., description=\"Page number where the answer can be found, fill in with 0 if you couldn't find it\")\n",
    "\n",
    "output_parser = PydanticOutputParser(AnswerFormat)\n",
    "json_prompt_str = \"\"\"\\\n",
    "Then please output with the following JSON format:\n",
    "\"\"\"\n",
    "json_prompt_str = output_parser.format(json_prompt_str)\n",
    "\n",
    "response_component = ResponseWithChatHistory(\n",
    "    llm=llm,\n",
    "    system_prompt=(\n",
    "        \"You are a Q&A system. You will be provided with the previous chat history, \"\n",
    "        \"as well as possibly relevant context, to assist in answering a user message.\"\n",
    "    )+json_prompt_str,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = QueryPipeline(\n",
    "    modules={\n",
    "        \"input\": input_component,\n",
    "        \"rewrite_template\": rewrite_template,\n",
    "        \"llm\": llm,\n",
    "        \"rewrite_retriever\": retriever,\n",
    "        \"query_retriever\": retriever,\n",
    "        \"join\": argpack_component,\n",
    "        \"reranker\": reranker,\n",
    "        \"response_component\": response_component,\n",
    "        \"output_parser\": output_parser,\n",
    "    },\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# run both retrievers -- once with the hallucinated query, once with the real query\n",
    "pipeline.add_link(\n",
    "    \"input\", \"rewrite_template\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "pipeline.add_link(\n",
    "    \"input\",\n",
    "    \"rewrite_template\",\n",
    "    src_key=\"chat_history_str\",\n",
    "    dest_key=\"chat_history_str\",\n",
    ")\n",
    "pipeline.add_link(\"rewrite_template\", \"llm\")\n",
    "pipeline.add_link(\"llm\", \"rewrite_retriever\")\n",
    "pipeline.add_link(\"input\", \"query_retriever\", src_key=\"query_str\")\n",
    "\n",
    "# each input to the argpack component needs a dest key -- it can be anything\n",
    "# then, the argpack component will pack all the inputs into a single list\n",
    "pipeline.add_link(\"rewrite_retriever\", \"join\", dest_key=\"rewrite_nodes\")\n",
    "pipeline.add_link(\"query_retriever\", \"join\", dest_key=\"query_nodes\")\n",
    "\n",
    "# reranker needs the packed nodes and the query string\n",
    "pipeline.add_link(\"join\", \"reranker\", dest_key=\"nodes\")\n",
    "pipeline.add_link(\n",
    "    \"input\", \"reranker\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "\n",
    "# synthesizer needs the reranked nodes and query str\n",
    "pipeline.add_link(\"reranker\", \"response_component\", dest_key=\"nodes\")\n",
    "pipeline.add_link(\n",
    "    \"input\", \"response_component\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "pipeline.add_link(\n",
    "    \"input\",\n",
    "    \"response_component\",\n",
    "    src_key=\"chat_history\",\n",
    "    dest_key=\"chat_history\",\n",
    ")\n",
    "pipeline.add_link(\"response_component\", \"output_parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "pipeline_memory = ChatMemoryBuffer.from_defaults(token_limit=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: When is lab4's due date?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AnswerFormat' object has no attribute 'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m pipeline_memory\u001b[38;5;241m.\u001b[39mput(user_msg)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(user_msg))\n\u001b[1;32m---> 23\u001b[0m pipeline_memory\u001b[38;5;241m.\u001b[39mput(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(response\u001b[38;5;241m.\u001b[39mmessage))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Jason\\anaconda3\\envs\\llama\\Lib\\site-packages\\pydantic\\main.py:811\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AnswerFormat' object has no attribute 'message'"
     ]
    }
   ],
   "source": [
    "user_inputs = [\n",
    "    \"When is lab4's due date?\"\n",
    "]\n",
    "for msg in user_inputs:\n",
    "    # get memory\n",
    "    chat_history = pipeline_memory.get()\n",
    "\n",
    "    # prepare inputs\n",
    "    chat_history_str = \"\\n\".join([str(x) for x in chat_history])\n",
    "\n",
    "    # run pipeline\n",
    "    response = pipeline.run(\n",
    "        query_str=msg,\n",
    "        chat_history=chat_history,\n",
    "        chat_history_str=chat_history_str,\n",
    "    )\n",
    "\n",
    "    # update memory\n",
    "    user_msg = ChatMessage(role=\"user\", content=msg)\n",
    "    pipeline_memory.put(user_msg)\n",
    "    print(str(user_msg))\n",
    "\n",
    "    pipeline_memory.put(response.message)\n",
    "    print(str(response.message))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     27\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Non-invasive Condition Monitoring (SEN)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 28\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mquery_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[1;32mIn[107], line 3\u001b[0m, in \u001b[0;36mquery_llm\u001b[1;34m(user_input, pipeline_memory, pipeline)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_llm\u001b[39m(user_input, pipeline_memory, pipeline):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# get memory\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     chat_history \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# prepare inputs\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     chat_history_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m chat_history])\n",
      "File \u001b[1;32mc:\\Users\\Jason\\anaconda3\\envs\\llama\\Lib\\site-packages\\llama_index\\core\\memory\\chat_memory_buffer.py:119\u001b[0m, in \u001b[0;36mChatMemoryBuffer.get\u001b[1;34m(self, input, initial_token_count, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m message_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chat_history)\n\u001b[0;32m    118\u001b[0m cur_messages \u001b[38;5;241m=\u001b[39m chat_history[\u001b[38;5;241m-\u001b[39mmessage_count:]\n\u001b[1;32m--> 119\u001b[0m token_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_token_count_for_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_messages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m initial_token_count\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m token_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_limit \u001b[38;5;129;01mand\u001b[39;00m message_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    122\u001b[0m     message_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Jason\\anaconda3\\envs\\llama\\Lib\\site-packages\\llama_index\\core\\memory\\chat_memory_buffer.py:149\u001b[0m, in \u001b[0;36mChatMemoryBuffer._token_count_for_messages\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(messages) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 149\u001b[0m msg_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_fn(msg_str))\n",
      "File \u001b[1;32mc:\\Users\\Jason\\anaconda3\\envs\\llama\\Lib\\site-packages\\llama_index\\core\\memory\\chat_memory_buffer.py:149\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(messages) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 149\u001b[0m msg_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_fn(msg_str))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "def query_llm(user_input, pipeline_memory, pipeline):\n",
    "    # get memory\n",
    "    chat_history = pipeline_memory.get()\n",
    "\n",
    "    # prepare inputs\n",
    "    chat_history_str = \"\\n\".join([str(x) for x in chat_history])\n",
    "\n",
    "    # run pipeline\n",
    "    response = pipeline.run(\n",
    "        query_str=user_input,\n",
    "        chat_history=chat_history,\n",
    "        chat_history_str=chat_history_str,\n",
    "    )\n",
    "\n",
    "    # update memory\n",
    "    user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "    pipeline_memory.put(user_msg)\n",
    "\n",
    "    pipeline_memory.put(response.answer)\n",
    "\n",
    "    # return the result as a tuple\n",
    "    result_tuple = (response.answer, response.file_name, response.page_number)\n",
    "\n",
    "    return result_tuple\n",
    "\n",
    "# Example usage:\n",
    "user_input = \"What is Non-invasive Condition Monitoring (SEN)\"\n",
    "result = query_llm(user_input, pipeline_memory, pipeline)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EE641",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
