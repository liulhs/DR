{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/anaconda3/envs/coursistant/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./index/pdf\")\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import (\n",
    "    QueryPipeline,\n",
    "    InputComponent,\n",
    "    ArgPackComponent,\n",
    ")\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
    "\n",
    "# First, we create an input component to capture the user query\n",
    "input_component = InputComponent()\n",
    "\n",
    "# Next, we use the LLM to rewrite a user query\n",
    "rewrite = (\n",
    "    \"Please write a query to a semantic search engine using the current conversation.\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{chat_history_str}\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"Latest message: {query_str}\\n\"\n",
    "    'Query:\"\"\"\\n'\n",
    ")\n",
    "rewrite_template = PromptTemplate(rewrite)\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# we will retrieve two times, so we need to pack the retrieved nodes into a single list\n",
    "argpack_component = ArgPackComponent()\n",
    "\n",
    "# using that, we will retrieve...\n",
    "retriever = index.as_retriever(similarity_top_k=6)\n",
    "\n",
    "# then postprocess/rerank with Colbert\n",
    "reranker = ColbertRerank(top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/anaconda3/envs/coursistant/lib/python3.12/site-packages/pydantic/json_schema.py:2158: PydanticJsonSchemaWarning: Default value default=PydanticUndefined description='Your Answer to the given query' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
      "/home/jason/anaconda3/envs/coursistant/lib/python3.12/site-packages/pydantic/json_schema.py:2158: PydanticJsonSchemaWarning: Default value default=PydanticUndefined description=\"PDF file's file name where the answer can be found, fill in with empty string if you couldn't find it\" extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
      "/home/jason/anaconda3/envs/coursistant/lib/python3.12/site-packages/pydantic/json_schema.py:2158: PydanticJsonSchemaWarning: Default value default=PydanticUndefined description=\"Page number where the answer can be found, fill in with 0 if you couldn't find it\" extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    }
   ],
   "source": [
    "# then lastly, we need to create a response using the nodes AND chat history\n",
    "from typing import Any, Dict, List, Optional\n",
    "from llama_index.core.bridge.pydantic import Field\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.query_pipeline import CustomQueryComponent\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "\n",
    "DEFAULT_CONTEXT_PROMPT = (\n",
    "    \"Here is some context that may be relevant:\\n\"\n",
    "    \"-----\\n\"\n",
    "    \"{node_context}\\n\"\n",
    "    \"-----\\n\"\n",
    "    \"Please write a response to the following question, using the above context: \\n\"\n",
    "    \"{query_str}\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "class ResponseWithChatHistory(CustomQueryComponent):\n",
    "    llm: OpenAI = Field(..., description=\"OpenAI LLM\")\n",
    "    system_prompt: Optional[str] = Field(\n",
    "        default=None, description=\"System prompt to use for the LLM\"\n",
    "    )\n",
    "    context_prompt: str = Field(\n",
    "        default=DEFAULT_CONTEXT_PROMPT,\n",
    "        description=\"Context prompt to use for the LLM\",\n",
    "    )\n",
    "\n",
    "    def _validate_component_inputs(\n",
    "        self, input: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Validate component inputs during run_component.\"\"\"\n",
    "        # NOTE: this is OPTIONAL but we show you where to do validation as an example\n",
    "        return input\n",
    "\n",
    "    @property\n",
    "    def _input_keys(self) -> set:\n",
    "        \"\"\"Input keys dict.\"\"\"\n",
    "        # NOTE: These are required inputs. If you have optional inputs please override\n",
    "        # `optional_input_keys_dict`\n",
    "        return {\"chat_history\", \"nodes\", \"query_str\"}\n",
    "\n",
    "    @property\n",
    "    def _output_keys(self) -> set:\n",
    "        return {\"response\"}\n",
    "\n",
    "    def _prepare_context(\n",
    "        self,\n",
    "        chat_history: List[ChatMessage],\n",
    "        nodes: List[NodeWithScore],\n",
    "        query_str: str,\n",
    "    ) -> List[ChatMessage]:\n",
    "        node_context = \"\"\n",
    "        for idx, node in enumerate(nodes):\n",
    "            node_text = node.get_content(metadata_mode=\"llm\")\n",
    "            node_context += f\"Context Chunk {idx}:\\n{node_text}\\n\\n\"\n",
    "\n",
    "        formatted_context = self.context_prompt.format(\n",
    "            node_context=node_context, query_str=query_str\n",
    "        )\n",
    "        user_message = ChatMessage(role=\"user\", content=formatted_context)\n",
    "\n",
    "        chat_history.append(user_message)\n",
    "\n",
    "        if self.system_prompt is not None:\n",
    "            chat_history = [\n",
    "                ChatMessage(role=\"system\", content=self.system_prompt)\n",
    "            ] + chat_history\n",
    "\n",
    "        return chat_history\n",
    "\n",
    "    def _run_component(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Run the component.\"\"\"\n",
    "        chat_history = kwargs[\"chat_history\"]\n",
    "        nodes = kwargs[\"nodes\"]\n",
    "        query_str = kwargs[\"query_str\"]\n",
    "\n",
    "        prepared_context = self._prepare_context(\n",
    "            chat_history, nodes, query_str\n",
    "        )\n",
    "\n",
    "        response = llm.chat(prepared_context)\n",
    "\n",
    "        return {\"response\": response}\n",
    "\n",
    "    async def _arun_component(self, **kwargs: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Run the component asynchronously.\"\"\"\n",
    "        # NOTE: Optional, but async LLM calls are easy to implement\n",
    "        chat_history = kwargs[\"chat_history\"]\n",
    "        nodes = kwargs[\"nodes\"]\n",
    "        query_str = kwargs[\"query_str\"]\n",
    "\n",
    "        prepared_context = self._prepare_context(\n",
    "            chat_history, nodes, query_str\n",
    "        )\n",
    "\n",
    "        response = await llm.achat(prepared_context)\n",
    "\n",
    "        return {\"response\": response}\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    \"\"\"Object representing a single knowledge pdf file.\"\"\"\n",
    "\n",
    "    answer: str = Field(..., description=\"Your Answer to the given query\")\n",
    "    file_name: str = Field(..., description=\"PDF file's file name where the answer can be found, fill in with empty string if you couldn't find it\")\n",
    "    page_number: int = Field(..., description=\"Page number where the answer can be found, fill in with 0 if you couldn't find it\")\n",
    "\n",
    "output_parser = PydanticOutputParser(AnswerFormat)\n",
    "json_prompt_str = \"\"\"\\\n",
    "Then please output with the following JSON format:\n",
    "\"\"\"\n",
    "json_prompt_str = output_parser.format(json_prompt_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then please output with the following JSON format:\n",
      "\n",
      "\n",
      "\n",
      "Here's a JSON schema to follow:\n",
      "{{\"description\": \"Object representing a single knowledge pdf file.\", \"properties\": {{\"answer\": {{\"title\": \"Answer\", \"type\": \"string\"}}, \"file_name\": {{\"title\": \"File Name\", \"type\": \"string\"}}, \"page_number\": {{\"title\": \"Page Number\", \"type\": \"integer\"}}}}, \"title\": \"AnswerFormat\", \"type\": \"object\"}}\n",
      "\n",
      "Output a valid JSON object but do not repeat the schema.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(json_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response_component = ResponseWithChatHistory(\n",
    "    llm=llm,\n",
    "    system_prompt=(\n",
    "        \"You are a Q&A system. You will be provided with the previous chat history, \"\n",
    "        \"as well as possibly relevant context, to assist in answering a user message.\\n\"+json_prompt_str\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = QueryPipeline(\n",
    "    modules={\n",
    "        \"input\": input_component,\n",
    "        \"rewrite_template\": rewrite_template,\n",
    "        \"llm\": llm,\n",
    "        \"rewrite_retriever\": retriever,\n",
    "        \"query_retriever\": retriever,\n",
    "        \"join\": argpack_component,\n",
    "        \"reranker\": reranker,\n",
    "        \"response_component\": response_component,\n",
    "        \"output_parser\": output_parser,\n",
    "    },\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# run both retrievers -- once with the hallucinated query, once with the real query\n",
    "pipeline.add_link(\n",
    "    \"input\", \"rewrite_template\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "pipeline.add_link(\n",
    "    \"input\",\n",
    "    \"rewrite_template\",\n",
    "    src_key=\"chat_history_str\",\n",
    "    dest_key=\"chat_history_str\",\n",
    ")\n",
    "pipeline.add_link(\"rewrite_template\", \"llm\")\n",
    "pipeline.add_link(\"llm\", \"rewrite_retriever\")\n",
    "pipeline.add_link(\"input\", \"query_retriever\", src_key=\"query_str\")\n",
    "\n",
    "# each input to the argpack component needs a dest key -- it can be anything\n",
    "# then, the argpack component will pack all the inputs into a single list\n",
    "pipeline.add_link(\"rewrite_retriever\", \"join\", dest_key=\"rewrite_nodes\")\n",
    "pipeline.add_link(\"query_retriever\", \"join\", dest_key=\"query_nodes\")\n",
    "\n",
    "# reranker needs the packed nodes and the query string\n",
    "pipeline.add_link(\"join\", \"reranker\", dest_key=\"nodes\")\n",
    "pipeline.add_link(\n",
    "    \"input\", \"reranker\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "\n",
    "# synthesizer needs the reranked nodes and query str\n",
    "pipeline.add_link(\"reranker\", \"response_component\", dest_key=\"nodes\")\n",
    "pipeline.add_link(\n",
    "    \"input\", \"response_component\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "pipeline.add_link(\n",
    "    \"input\",\n",
    "    \"response_component\",\n",
    "    src_key=\"chat_history\",\n",
    "    dest_key=\"chat_history\",\n",
    ")\n",
    "pipeline.add_link(\"response_component\", \"output_parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "pipeline_memory = ChatMemoryBuffer.from_defaults(token_limit=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: What are some requirements for the final project?\n",
      "answer='The requirements for the final project include: 1) Ten documents that support the validity and the value of your solution. 2) Incorporation of your custom xDot IIoT. 3) Incorporation of AWS backend with Thingsboard IO. 4) Integration of AWS to analyze the data and respond, which may include simple analytics or machine learning. Additionally, the project proposal slide deck should include: Slide 1: Title and Team members, Slide 2: Summary of the real-world problem - why IoT is the best, Slide 3: How your hardware and software are going to be used, Slide 4: Schedule and Who is going to be doing what (as much detail), Slide 5: List of References (at least 3, including websites and conference and journal papers – need a complete reference – summary of the evidence).' file_name='L17-f23.pdf' page_number=2\n"
     ]
    }
   ],
   "source": [
    "user_inputs = [\n",
    "    \"What are some requirements for the final project?\"\n",
    "]\n",
    "for msg in user_inputs:\n",
    "    # get memory\n",
    "    chat_history = pipeline_memory.get()\n",
    "\n",
    "    # prepare inputs\n",
    "    chat_history_str = \"\\n\".join([str(x) for x in chat_history])\n",
    "\n",
    "    # run pipeline\n",
    "    response = pipeline.run(\n",
    "        query_str=msg,\n",
    "        chat_history=chat_history,\n",
    "        chat_history_str=chat_history_str,\n",
    "    )\n",
    "\n",
    "    # update memory\n",
    "    user_msg = ChatMessage(role=\"user\", content=msg)\n",
    "    pipeline_memory.put(user_msg)\n",
    "    print(user_msg)\n",
    "    \n",
    "    assistant_msg = ChatMessage(role=\"assistant\", content=response.answer)\n",
    "    pipeline_memory.put(assistant_msg)\n",
    "    print(response)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The due date for Lab 4 is September 16 at 11:59pm for the report and September 18 for the demo video on YouTube.\n",
      "lab4-f23.pdf\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def query_llm(user_input, pipeline_memory, pipeline):\n",
    "    # get memory\n",
    "    chat_history = pipeline_memory.get()\n",
    "\n",
    "    # prepare inputs\n",
    "    chat_history_str = \"\\n\".join([str(x) for x in chat_history])\n",
    "\n",
    "    # run pipeline\n",
    "    response = pipeline.run(\n",
    "        query_str=user_input,\n",
    "        chat_history=chat_history,\n",
    "        chat_history_str=chat_history_str,\n",
    "    )\n",
    "\n",
    "    # update memory\n",
    "    user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "    pipeline_memory.put(user_msg)\n",
    "\n",
    "    assistant_msg = ChatMessage(role=\"assistant\", content=response.answer)\n",
    "    pipeline_memory.put(assistant_msg)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example usage:\n",
    "user_input = \"What is the Due date of lab4?\"\n",
    "result = query_llm(user_input, pipeline_memory, pipeline)\n",
    "print(result.answer)\n",
    "print(result.file_name)\n",
    "print(result.page_number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EE641",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
