00:00:00 Clustering is one of the most common forms of unsupervised learning, and often helpful in understanding the characteristics of our data.
00:00:06 Unsupervised learning tasks are quite different from supervised prediction tasks.
00:00:12 In supervised learning, we have something in particular we'd like to predict, for example, whether an email is spam or the price of a stock tomorrow.
00:00:18 This prediction variable is called the target, y.
00:00:24 In order to predict it, we observe a number of features, x, that we think will be helpful.
00:00:30 The learning task, then, is to learn a mapping from x to y.
00:00:36 In unsupervised learning, in contrast, we have no particular target variable, y.
00:00:42 Instead, the task is just to understand the data, identify patterns, and so on.
00:00:48 Thus, we assume we have only the set of features, x, that we'd like to use to understand these patterns.
00:00:54 Unsupervised learning is useful in many situations, most commonly to find patterns in the data and explain how the data came about in a way that a human can understand.
00:01:00 However, even if our ultimate task is something like a more supervised predictive problem, unsupervised learning can be helpful.
00:01:08 For example, if some of our data are missing features, so meaning we couldn't observe that feature's value, then unsupervised learning models can be used to fill in these missing values in a process called imputation.
00:01:17 Similarly, unsupervised model information could be used as features, either to augment the original features or replace them with our simplified explanation of the data.
00:01:25 In these slides, we'll look at the concept of clustering, or understanding the data by automatically organizing them into groups with shared characteristics.
00:01:34 For example, in these data, we can readily recognize that there are three groups to the data and summarize the data set in some sense by just describing these groups.
00:01:42 Clustering models group the data, partitioning the data points and placing similar data together in each group.
00:01:51 Clustering models group the data, partitioning the data points and placing similar data together 
00:01:59 and dissimilar data in different groups.
00:02:08 However, the concept of grouping is a product of our human understanding of the world and often a holistic understanding of the entire dataset.
00:02:17 So what makes a sensible grouping might vary by the dataset.
00:02:25 For example, when we look at these two-dimensional points, we immediately see an explanation that the data are organized into about three groups, like so, and these groups share similarity in their general location in this feature space.
00:02:34 So points over here belong to this group, points over here belong to this group, and so on.
00:02:42 A different set of data, however, might not vary by location exactly, but our brains can still see patterns that define the similarity which we can turn into groupings or clusters.
00:02:51 So these clusters are not defined by location in the space precisely but by some holistic property.
00:03:00 of a collection of points.
00:03:07 In the third example, we might see that the data have different properties in different parts of the space, for example, a different density in one part of the space than another.
00:03:15 That density, then, might suggest a grouping of the data into two sets, one of which has sparse data and one of which has dense data.
00:03:22 Complex learning is closely related to data compression concepts.
00:03:30 In particular, clustering is related to vector quantization, a staple of lossy compression algorithms.
00:03:37 Suppose we need to transmit a large data set, for example, a large image with many pixels.
00:03:45 We can chunk the image into vectors of some fixed size, for example, grouping all the pixels into 2x2 squares, and then we can interpret those as length-4 vectors and transmit them in some order.
00:03:52 Now, some of these squares may be very similar.
00:04:00 For example, this patch and this one are nearly identical.
00:04:06 So instead of sending the pixel values twice, we can simply say, use patch 13, or whatever, and send fewer data.
00:04:13 Equivalently, we could first transmit all the patches we plan to use first, as a dictionary, and then we'd send a sequence of patches in the image by listing the index of the closest entry in our dictionary.
00:04:20 So this process is called vector quantization.
00:04:26 Each 4D feature vector is quantized into one entry of the dictionary.
00:04:33 Over here is a 2D visualization of that procedure.
00:04:40 So each data point, a dot, is being approximated by its closest dictionary entry, a plus.
00:04:46 So each dictionary entry claim all the data that are nearer to it than to any other plus.
00:04:53 The process of selecting good dictionary vectors, so where to locate the pluses in the sequence,
00:05:00 space, give a collection of data vectors that we plan to transmit, and that's just clustering.
00:05:06 We're grouping the data by visual similarity, usually just Euclidean similarity, and each partition is assigned one entry in the dictionary.
00:05:13 Here's a more trivial version, working with just one-dimensional or scalar data values.
00:05:19 Each pixel, then, is considered a data point.
00:05:26 It takes on one of 256 values.
00:05:32 And then to compress the data, we'd like to transmit fewer values.
00:05:39 So we group the data by their value.
00:05:46 We might divide the space up into eight parts, or four parts, or only two parts.
00:05:52 This quantizes the values of the data, the levels of gray, in the recovered image, so that we send fewer bits over here, giving us compression, but end up with only an approximation to the original data.
