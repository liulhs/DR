00:00:00 We'll examine the multivariate Gaussian distribution and some of its properties and geometry.
00:00:07 The Gaussian distribution in one dimension is a classic distribution over a single scalar random variable X and parameterized by a mean mu and a standard deviation sigma, or equivalently its variance, sigma squared.
00:00:15 The distribution is just given by an exponentiated quadratic, so there's a normalization constant that assures it integrates to 1, and then e to the minus 1 half times a quadratic form, X minus mu squared divided by the variance sigma squared.
00:00:22 This distribution has a classic bell-shaped curve.
00:00:30 It's symmetric.
00:00:37 Its mean is the center point, also the optimum of the maximum of the function.
00:00:45 And the variance tells us about the spread.
00:00:52 So the plus...
00:01:00 or minus 2 sigma region will give us a 95% confidence interval.
00:01:06 So the larger sigma is, the wider this bell-shaped curve is.
00:01:13 Also useful, the classic maximum likelihood estimators.
00:01:20 Given some data, if we want to estimate the distribution, we can estimate the mean as being the empirical mean of the data points by taking the average of the data and estimate the variance as being the empirical average of the centered data points squared.
00:01:26 So we take x, subtract the empirical mean, square those points, and average them.
00:01:33 That'll give us the variance.
00:01:40 A multivariate Gaussian model is simply an extension of this model to vector-valued random variables.
00:01:46 So now x will be a vector with d values, and the distribution will be parameterized by a mean mu, which will also be a length d vector.
00:01:53 And a covariance matrix sigma, which will be a d by d matrix.
00:02:00 Again, it has the same kind of exponentiated quadratic form.
00:02:05 So here's a constant to ensure that the whole thing integrates to 1.
00:02:10 And then it's e to the minus 1 by 2.
00:02:15 And then instead of the x minus mu squared over sigma squared, we have x minus mu.
00:02:20 This is a row vector.
00:02:25 Sigma inverse, a d by d matrix, times x minus mu transpose.
00:02:30 So this is a column vector.
00:02:35 So a row times a square matrix times a column vector will give us a scalar number which evaluates to the probability of the value x.
00:02:40 This function, if plotted in the multiple dimensional features, say we have feature x1 and feature x2, this function will be, again, a bell-shaped curve, but now in two dimensions.
00:02:45 So here we have the same kind of symmetric bell-shaped mode.
00:02:50 If we take a single cut of this at some probability, we can plot it as a contour plot.
00:02:55 So here.
00:03:00 might be x1, here might be x2.
00:03:04 And the plot of constant probability will be an ellipse for a Gaussian.
00:03:08 So the mean vector mu will be a two-dimensional point in the center.
00:03:12 And the covariance will, again, define the spread and now the shape of this ellipsoidal function.
00:03:17 The maximum likelihood estimators are quite similar to the univariate case.
00:03:21 So the mean vector mu will now be the empirical average of the data vectors x.
00:03:25 So remember, mu is the same size as x.
00:03:30 So we just average the vectors.
00:03:34 The covariance matrix, sigma, will also be an empirical average.
00:03:38 We take the data and center them by removing the empirical mean.
00:03:42 And then what we average is the vector-vector product, so the outer product, x minus mu transpose times x minus mu.
00:03:47 The x minus mu transpose is a column vector.
00:03:51 x minus mu is a row vector.
00:03:55 And so their outer product.
00:03:59 product is a d by d matrix.
00:04:06 If we average those over all the data, we'll get the empirical covariance matrix sigma.
00:04:13 It's useful to see an example of this for independent variables x1 and x2.
00:04:19 If we have a variable x1 that's Gaussian, it'll have a normalization constant and then a Gaussian distribution with mean mu1 and variance sigma1 squared.
00:04:26 x2 being Gaussian means it has the same form, but say, with mean 2 and variance sigma2 squared.
00:04:33 If we now create a new vector variable, x, just by concatenating x1 and x2, we can ask what the distribution of x is.
00:04:40 We'll assume that x1 and x2 are independent, so that means their joint distribution is the product of their individual distributions.
00:04:46 And what we see is we get the product of their normalization constants times e to the minus 1 by 2.
00:04:53 And now, there'll be one term.
00:05:00 x1-mu1 over sigma1 squared, and another term, x2-mu2 squared over sigma2 squared, added together, since the product of these two things will add in the exponent.
00:05:20 If we just vectorize that, we can describe that as a x-mu, sigma inverse x-mu, where now x1-mu1 and x2-mu2 is given by the vector difference x-mu, and the scaling of the sigmas is accomplished by creating a diagonal covariance matrix.
00:05:40 So, sigma1 squared in the upper left, sigma2 squared in the lower right, and this inverse will simply be the same thing point-wise inverse, and when we multiply that by x-mu, transpose x-mu, we will get this term.
00:06:00 x1 minus mu1 over sigma 1 squared, and this term, x2 minus mu2 over sigma 2 squared, added together.
00:06:06 If we plot this on two features, we will find that this covariance matrix has an axis-aligned ellipsoidal shape, with the spread in the axis being proportional to the size of sigma.
00:06:13 So here I've drawn sigma 1, 1 being larger than sigma 2, 2.
00:06:20 So the spread in the x1 direction is larger than the spread in the x2 direction.
00:06:26 We'll see more examples of this later.
00:06:33 More generally, we can understand the geometry of the Gaussian through the eigendecomposition of the matrix sigma.
00:06:40 Consider a curve of constant probability.
00:06:46 The constant probability curve in a Gaussian means that the exponent will be constant, since the other terms are all just scalar normalization constants.
00:06:53 So this curve, this red curve.
00:07:00 is given by delta squared equal a constant, where delta squared is this exponentiated term.
00:07:10 So, x minus mu, sigma inverse, x minus mu transpose.
00:07:20 So, now let's understand what the shape of this function is by thinking about an eigendecomposition of the matrix sigma.
00:07:30 So, an eigendecomposition decomposes sigma into an eigenvector matrix mu, an eigenvalue matrix lambda that's diagonal, and the same eigenvector matrix mu transpose.
00:07:40 So, these mus are orthonormal vectors, which means that the product of mu i and mu j is either 0 if i is not equal to j, or 1 if i equals to j.
00:07:50 So, then we can envision this as sigma being a matrix U, which involves the eigenvectors in column-wise form.
00:08:00 a diagonal matrix lambda, and u transpose, which now has the same vectors but transposed.
00:08:10 If we now just define a variable y i as the projection of the vector x minus mu onto the ith eigenvector mu i.
00:08:20 So that projection is given by mu i transpose, mu i dot producted with x minus mu.
00:08:30 And then you can see that this delta squared has a very simple form in terms of these y's.
00:08:40 It's just y1 squared scaled by lambda 1 plus y2 squared scaled by lambda 2.
00:08:50 And this is precisely the equation of an ellipse with one axis of size 1 over lambda 1 and the other axis of size 1 over lambda 2.
00:09:00 lambda 1 and lambda 2.
00:09:12 Geometrically, this means that this ellipsoid of constant probability will be an ellipse with an axis in the u1 direction and an orthogonal axis in the u2 direction with the length of those axes being proportional to the eigenvalues or rather the square root of the eigenvalues associated with each eigenvector.
00:09:24 This eigen decomposition view can also be used in a constructive way to sample values of Gaussians.
00:09:36 I think this is a useful illustration of how this decomposition affects the shape of the Gaussian.
00:09:48 Let's start by generating random vectors X using the simple MATLAB randn function which gives us m samples of independent
00:10:00 Gaussians of vectors of two features.
00:10:06 So this is a matrix of completely independent m by 2 entries of Gaussians with mean 0 and variance 1.
00:10:12 You can see that the data are independent.
00:10:18 They have a spherical shape and variance 1 in each direction.
00:10:24 We can then scale x by multiplying it by some matrix where this is the scaling term in the x1 feature.
00:10:30 This is the scaling term in the x2 feature.
00:10:36 So when we do that, we see that what happens is the x1 feature values get spread out.
00:10:42 The x2 feature values are not spread out because their scaling factor is 1.
00:10:48 And so now we have this elongated axis-aligned ellipse with a longer x1 feature than x2.
00:10:54 In mathematical terms, this thing where
00:11:00 by is the square root of the eigenvector matrix on the previous slide.
00:11:15 We can then multiply by a rotation matrix, so here's a matrix that rotates by angle theta, and this is precisely taking the role of this eigenvector matrix U.
00:11:30 So this rotation is defining now the orthogonal axes, the first eigenvector and second eigenvector, U1 and U2.
00:11:45 Finally, if we add the same vector mu to all the data points, we'll shift the values, the center of those values, up so that they have average value mu instead of average value zero, and this will give us a Gaussian distribution with centered at the vector point mu, so mu1, mu2, and we
00:12:00 with a covariance describing an ellipsoidal shape with value U lambda U transpose, where U transpose is this rotation matrix, and lambda is given by the square of this scaling matrix.
00:12:30 So this illustrates both how we can generate a collection of samples that are drawn from a multivariate Gaussian with arbitrary covariance matrix sigma, just by decomposing sigma into its eigen decomposition and then following this procedure, and also how to understand the geometry and what the role of the eigenvectors and eigenvalues are of the covariance matrix in determining the shape that this Gaussian ellipsoidal
00:13:00 a contour takes on.
00:13:10 In summary, multivariate Gaussian distributions generalize the classic 1D Gaussian bell-shaped contour to vector-valued random variables by giving them a vector-valued mean of the same dimension as the data and a covariance matrix of size d squared that describes the shape and size of the bell-shaped mode.
00:13:20 For independent variables, we find that we have a diagonal covariance.
00:13:30 Diagonal covariance leads to an axis-aligned ellipsoidal shape.
00:13:40 If the diagonal entries are equal, that means that the size in each of these directions is the same, and we see that the contours take on a spherical shape.
00:13:50 More generally, for an arbitrary covariance matrix sigma, we get an ellipsoidal probability.
00:14:00 contour that may have some rotational shape, and you can understand the size and orientation of that ellipse by looking at the eigenvectors and the eigenvalues of the covariance matrix, with the eigenvectors defining the directions of the principal axes of this ellipse, and the scale determined by the value of the eigenvalues.
00:14:24 We can also follow this procedure in reverse to generate data from a multivariate Gaussian by first drawing data from independent unit normals, and then performing scaling and rotation and shifts to give us an arbitrary Gaussian.
