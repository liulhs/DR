00:00:00 For linear regression with the mean squared error loss, it turns out that we can also compute the optimal values of theta in closed form.
00:00:10 Consider the following simple problem in which we have only two data points to fit a line.
00:00:20 Our two data points give us two equations corresponding to each data, each of which has two unknowns, theta 0 and theta 1, and we can just solve this set of linear equations.
00:00:30 As you may recall from linear algebra, in matrix form, we can simply write this down, y equals theta x, and solve it, which corresponds to the matrix inverse of x transpose.
00:00:40 More generally, of course, x is not square and thus not invertible, but if m is greater than n, this corresponds to there being no linear function that hits all the data exactly, and instead we'll solve directly for the minimum of the mean squared error function.
00:00:50 The minimum of J corresponds to the minimum of theta.
00:01:00 points to the point where the gradient of J equals 0.
00:01:05 So computing this and setting it equal to 0, we can distribute x inside the parentheses and move theta x transpose x to the other side.
00:01:10 x transpose x is a square matrix.
00:01:16 Assuming it's full rank, we can solve by multiplying by its inverse.
00:01:21 And we find that the minimum of J is given at theta equal to y transpose x.
00:01:27 x transpose x inverse.
00:01:32 This quantity, x x transpose x inverse, is called the Moore-Penrose pseudo-inverse of x transpose.
00:01:38 If x transpose is square and full rank, it's exactly equal to x transpose inverse.
00:01:43 But in most practical situations, we have more data than features.
00:01:49 So meaning m greater than n, in which case the pseudo-inverse gives the minimum mean squared error fit.
00:01:54 Now that we have a linear algebraic square, let's definition the square.
00:01:59 form for the optimal value of theta, this is easy to solve in MATLAB.
00:02:10 We again write our Y and X matrix, and we can solve this either manually by simply executing the form of the equation, Y transpose X inverse of X transpose X, or by using a special MATLAB operator, the matrix right divide operation.
00:02:20 This operator explicitly solves a set of linear equations defined by Y equals theta times X transpose.
00:02:30 While this may look less obviously equivalent to our derivation, it lets MATLAB use a more numerically stable solution technique.
00:02:40 One useful interpretation of the stationary point equation we solved gives them the name the normal equations.
00:02:50 Consider the vector of target values, Y1 through Ym, an m-dimensional space where.
00:03:00 M is the number of data points.
00:03:06 We can also regard the data matrix as a collection of column vectors.
00:03:13 So Xi will be a vector consisting of the i-th feature for all M data points.
00:03:19 This is, again, a vector in M-dimensional space.
00:03:26 So here, X1 is the vector of feature one across all the data, X2 is the vector of feature two across all the data, and so forth.
00:03:33 Now, any linear combination, a linear predictor, must live in the span of these vectors.
00:03:39 So our vector of predictions at all the data points is also an M-dimensional vector, which lives somewhere in this subspace, spanned by X1, X2.
00:03:46 The normal equations say that the error residual, the difference between Y and Y hat, when dot-producted with the matrix X, equals 0.
00:03:53 So that means this error vector is orthogonal to every column of X.
00:03:59 that the difference between y, the vector of true targets, and y hat are predictions, that error vector is orthogonal to the space spanned by x.
00:04:07 So far we've been looking at mean squared error, which is a useful and widely used loss function.
00:04:14 One particularly nice aspect of it is that we can solve it exactly and fairly easily in closed form.
00:04:22 However, mean squared error is not always the best choice.
00:04:29 Take the following data, say maybe y is house price where x is house size, and suppose we make a fit to it, in which case we learn the following model, the linear model of x.
00:04:37 Now suppose we add a data point that doesn't follow the general trend.
00:04:44 So here's an outlier, it's perhaps a particularly large house for which almost nothing was paid.
00:04:52 Perhaps we've entered the data incorrectly, perhaps the householder simply sold to a relative.
00:04:59 very cheaply.
00:05:05 The error residual on this point is very large.
00:05:11 So here it's 16, so the squared error is 16 squared.
00:05:17 Because it's squared, reducing an error of 16 is much more important than reducing, say, 16 errors of size 1.
00:05:23 Viewed another way, mean squared error is in some senses equivalent to a Gaussian assumption on error residuals.
00:05:29 And if we look at the histogram of error residuals in this model, it's clearly not Gaussian.
00:05:35 There's one mode over here near 0, and another mode over here.
00:05:41 It's extremely far away, far out in the tails and outlier.
00:05:47 If we optimize our model to minimize mean squared error with this data point included, we'll get a significant distortion in the model.
00:05:53 So instead of learning the black line, we'll learn the blue line, because reducing the error on this...
00:05:59 large squared error is extremely important and is worthwhile to distort ourselves from several of these other things that have smaller error.
00:06:06 So now let's see how a different loss function might behave.
00:06:13 Suppose instead of the mean squared error, we look at the mean absolute error, sometimes called the L1 error.
00:06:19 So the sum or average of absolute differences between the target and our prediction.
00:06:26 So here again in black is our squared error minimum using the original data points.
00:06:33 If we fit this using mean absolute error instead, we get the green line, which is quite similar and still a quite good looking fit to the data.
00:06:40 If we now include this outlier and retrain using the mean absolute error, we find that we get very little distortion.
00:06:46 So in this case, we still have an error of 16, but we don't square that.
00:06:53 So the error of 16 is worth the...
00:07:00 same amount as 16 smaller errors of size 1.
00:07:07 So it's not as important to our optimization procedure to reduce this big error.
00:07:15 We often visualize the behavior of our loss functions by showing the total loss as a function of the error magnitude.
00:07:22 So we find that mean squared error increases quadratically, so that small errors are relatively unimportant, but outliers or extreme large errors have a very big impact.
00:07:30 Mean absolute error, on the other hand, the red curve, increases only linearly, so large outlier errors will have less impact.
00:07:37 If we really want to reduce large errors, we could even design an alternative loss function that accounts for this.
00:07:45 So for example, this blue one, which looks quadratic near zero, but as we get to higher and higher errors, asymptotes to some constant value.
00:07:52 So that no matter how wrong our prediction, no matter how large the error, the value in our
00:08:00 our loss function will be bounded by some constant.
00:08:07 Of course, optimizing these more arbitrary loss functions means that we can no longer use a closed form solution the way we did with mean squared error.
00:08:14 But we can still optimize our loss function using something like gradient descent.
