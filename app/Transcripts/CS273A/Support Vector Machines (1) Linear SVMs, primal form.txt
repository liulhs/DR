00:00:00 One of the most popular types of linear classifier is the support vector machine.
00:00:04 Suppose we're training a linear classifier for a set of data.
00:00:09 There may be a number of classifiers that all separate the data, in other words, have no training error.
00:00:13 How can we choose among these?
00:00:18 If we look at these two examples, we see that although the one on the left correctly classifies all the data, it does not look safe.
00:00:23 Small perturbations in the data or the decision boundary could cause an incorrect prediction.
00:00:27 So we may not trust it on test data, which would differ slightly from our training dataset.
00:00:32 The one on the right, by contrast, looks more stable.
00:00:36 There's a margin around the decision boundary in which no data are located.
00:00:41 We already saw a bit of this with surrogate losses.
00:00:46 Using a smooth loss, we'll prefer to move data away from the boundary.
00:00:50 But in some sense, this was an accidental side effect.
00:00:55 If a margin is what's desirable, how can we...
00:00:59 create an optimization that explicitly tries to maximize it.
00:01:07 To optimize the margin of our classifier, we'll have to relate it directly to the parameter vector.
00:01:14 Here, instead of our usual theta parameters, I'll usually use w to indicate the weights associated with each feature, one through n, and to denote the constant term by b without listing the constant feature.
00:01:22 It'll become clear shortly why it's useful to treat the constant term separately.
00:01:29 To define the margin, we'll start by assuming that our parameters get all the data correct, and we'll enforce this point later.
00:01:37 Another thing to note, for any setting of the parameters, the decision boundary is invariant to rescaling.
00:01:44 For example, if this region is defined by the linear response being positive, then multiplying the parameters by 10 doesn't change the decision.
00:01:52 For this reason, let's
00:01:59 define the margin using an additional property that is not scale invariant.
00:02:06 We'll define that not only are the positive class a region with positive response, they're in a region with positive response at least plus 1.
00:02:13 Similarly, the negative data are in a region with response at most minus 1.
00:02:19 Since these are isosurfaces of a linear response, they're also hyperplanes, just like and parallel to the decision boundary.
00:02:26 Then we can define this margin explicitly in terms of these hyperplanes.
00:02:33 Since no data are inside them, we'll define the margin as the distance between them.
00:02:39 I've drawn this with data on the hyperplanes, but since we're going to maximize the margin, if there are no data on the plus and minus 1 hyperplanes, we'll simply end up changing the parameters to expand the margin until this is true.
00:02:46 So you can also think of the margin as twice the distance from the decision boundary to the nearest training point.
00:02:53 The points on the margin are referred to as the support vectors.
00:02:59 Now, we can compute the margin geometrically in terms of the parameters.
00:03:08 The first thing to note is that the vector, defined by the weights w, the coefficients and the features, is perpendicular to the decision boundary.
00:03:17 To see this quickly, just note for any two points x and x' that are both on the boundary, the linear response is zero.
00:03:25 So rearranging these equations, we see that w and the vector x'-x, which lies along the boundary, are perpendicular.
00:03:34 Now let's pick any point x- that's in the part of the space where the negative data are allowed to lie, specifically on the minus one hyperplane.
00:03:42 This is any point, not necessarily a data point.
00:03:51 Take x-plus to be a point as close as possible to x-minus in the plus one space.
00:04:00 orthogonal to these planes, the vector from x-minus to x-plus is some scalar multiple r of the vector w.
00:04:12 We also know that, since x-minus is on the negative hyperplane, and x-plus is on the positive hyperplane, their linear responses are minus 1 and plus 1, respectively.
00:04:24 Some simple geometry will complete the analysis.
00:04:36 We can compute the value of this scalar r by plugging our definition x-plus equals x-minus plus r times w into the positive hyperplane equation, giving this, then rearranging and plugging in the negative hyperplane equation here, giving the value of r.
00:04:48 And the margin is just the distance from x-minus to x-plus.
00:05:00 In other words, the length of r times w.
00:05:07 Evaluating this is just 2 times the length of the vector w.
00:05:15 So, for any parameters w and b satisfying our assumptions, in other words, all the positive data in the plus 1 region, all the negative data in the minus 1 region, the margin of w can be defined as 2 over its length.
00:05:22 So, now let's go about actually optimizing the parameters.
00:05:30 This is a constrained optimization problem.
00:05:37 We want to maximize our margin equation, subject to the constraints that all our data lie in the specified region, in other words, on the correct side of their respective margin.
00:05:45 Finding the value of w that maximizes 1 over w's length, we can equivalently find the value of w that minimizes w's length.
00:05:52 The vector w with smallest length will also be the w that has the largest inverse length.
00:06:00 Similarly, minimizing squared length instead doesn't change the location of the minimizer.
00:06:06 So we'll reform our maximization as a minimization of the sum of Wj squared.
00:06:13 To enforce the data constraints, we have one constraint per data point.
00:06:20 If a point i is positive, then we need that the linear response be greater than plus one.
00:06:26 If a point i is negative, we need for the linear response to be less than minus one.
00:06:33 Framed this way, the margin problem is an example of a classic optimization problem called a quadratic program.
00:06:40 We're minimizing a quadratic function of the parameters, the sum of W squared, subject to a collection of m linear constraints on the parameters, one for each data point that it lies in the correct region.
00:06:46 Framing it this way makes it easy to apply a suite of optimization techniques and algorithms designed for quadratic programs.
00:06:53 For reference, we call this formula.
00:07:00 our max margin classifier, the primal problem, a point we'll contrast with later.
00:07:07 It will also be convenient to compact these constraints into a single phrase that will work for both positive and negative data.
00:07:15 Here, the target y is either plus 1 or minus 1.
00:07:22 If it's plus 1, the linear response should also be positive and greater than 1, so the product will be also.
00:07:30 If y is minus 1, then the linear response should be negative and large, so the product will, again, be greater than 1.
00:07:37 In other words, our linear response should be larger than 1 in magnitude and match the sign of y.
00:07:45 Let's see a simple example, small enough that we can plot both the feature and parameter spaces.
00:07:52 Let's say we have one scalar feature x and three data points from the negative class at x equals minus 3 and minus 1 and the positive class at x equals minus 3.
00:08:00 x equals 2.
00:08:06 We can see that these data are easily separable, in fact, there are many linear classifiers that will separate them, with different weights a and constant or biased terms b.
00:08:13 For example, here's one, here's another, here's a third.
00:08:20 In fact, any classifier that transitions sign, crosses zero, between the blue and red points will predict all three points correctly.
00:08:26 So let's find the classifier with largest margin.
00:08:33 We can write our margin constraints, one for each data point, and visualize them in the two-dimensional parameter space, A and B.
00:08:40 Rearranging these constraints, we have that the point at minus 3 constrains our parameter B to lie below this line.
00:08:46 Our second point, at minus 1, to lie below this line.
00:08:53 And our third point...
00:09:00 constraint at 2 to lie above this line.
00:09:10 So the set of parameters that satisfy our margin constraints is shown in shaded green.
00:09:20 Notice that, as one would expect, the constraint here for x equals minus 3 is superseded by the constraint here for x equals minus 1.
00:09:30 Any point within this shaded region corresponds to a valid perceptron with a margin.
00:09:40 For example, setting A equals 1, B equals 0, this point here, corresponds to a linear classifier in the feature space, here, with a margin, meaning the region between response minus 1 and plus 1, shown between these dashed vertical lines.
00:09:50 The SVM problem is now to minimize the non-constant weight magnitude.
00:10:00 It's just the parameter a.
00:10:07 The closest point in our constraint set to the a equals 0 line is this vertex here, with a equals 4 3rds and b equals minus 1 3rd.
00:10:15 Notice that two constraints, here and here, are tight in the sense that our parameters lie on the constraint.
00:10:23 This means that our solution would change if those constraints were not present.
00:10:31 These are exactly the constraints due to points that lie on the margin.
00:10:38 For example, if the negative example here were not present, the solution would change to widen the margin.
00:10:46 In the next lecture, we'll see more about optimizing the support vector machine, including its dual form and a formulation for non-separable data.
