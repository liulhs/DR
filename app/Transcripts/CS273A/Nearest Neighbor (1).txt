00:00:00 The first machine learning technique we'll look at are nearest neighbor methods.
00:00:08 While at first they may not appear to do much learning per se, in practice they can be quite useful and will illustrate some of the issues we'll be dealing with.
00:00:17 Recall that these are supervised learning problems, so we'll be trying to predict a target Y using some collection or vector of features X.
00:00:25 Our learner will take in the features and will spit out a prediction Y hat, and our learner will be parameterized in some way so that observing data will change its input-output behavior.
00:00:34 Let's start with a regression problem.
00:00:42 In regression, for the purposes of plotting, we'll assume that we just have a single scalar feature X and the target Y, and therefore we can plot each of our observed data points as a point with an XY coordinate.
00:00:51 And when we look at this scatter plot...
00:00:59 that suggests some relationship between x and y that we can use so that given a new x point that we're asked to predict at, this relationship will suggest what value we should predict the y value for that feature x at.
00:01:12 The nearest neighbor predictor follows a very simple formula.
00:01:24 We simply save all of the training data points, the red dots in the scatter plot, and our predictor just follows a very simple procedure, which is that given a new feature x, it looks in that data set, finds the data point with the closest value of x, and predicts the y value that was associated with that training point.
00:01:36 So given x new, we find the xi training data point that's closest and predict its associated yi.
00:01:48 While defined as a procedure, this
00:02:00 implicitly defines a function of x, since every x has a prediction associated with it.
00:02:10 If we evaluate that procedure at each possible feature x, we get a function that looks like this.
00:02:20 So, whenever we're closest to this data point, we predict its y value.
00:02:30 The moment that we become halfway between that data point and the next data point, we transition abruptly to the new nearest data point's y value.
00:02:40 Notice that as a regression, this function is locally constant, since as we move very slightly, we continue to have the same nearest neighbor, so we continue to predict exactly the same y value, followed by an abrupt transition whenever we change our nearest data point.
00:02:50 In classification, a supervised learning problem in which the target values y are discrete-valued, instead of plotting them as we did in the
00:03:00 a scatterplot in regression, we'll simply indicate them using a symbol or a color.
00:03:05 So here, 1 and 0.
00:03:10 And that allows us to pretend that we have two features.
00:03:16 So for our purposes of illustration, we'll imagine that we have a feature x1 and another feature x2.
00:03:21 And in this case, the problem is the same.
00:03:27 Given a new feature vector x, we'd like to predict whether that associated y will be a 0 or a 1.
00:03:32 The nearest neighbor rule here simply looks and chooses the data point that's closest in terms of distance and predicts the value associated with it.
00:03:38 So here, we need a distance on a vector space.
00:03:43 So we'll typically choose the Euclidean distance, which is just the sum of squared distances over the features.
00:03:49 Again, we can evaluate this procedure at every possible point x.
00:03:54 And that will define a function.
00:04:00 points x where we're closest to a 1, we'll predict 1.
00:04:06 At all points x where we're closest to a 0, we'll predict 0.
00:04:13 And between them, there'll be an abrupt change called the decision boundary.
00:04:20 The decision boundary indicates the set of points at which on one side we're predicting 1 and on the other side we're predicting 0.
00:04:26 Geometrically, it's clear that just like in the regression case, there'll be a set of points where each data point is in fact the closest point.
00:04:33 So in the neighborhood of this data point, I'll be using it for prediction, and so whenever I'm closest to it, I'll be predicting its value.
00:04:40 Similarly, in the neighborhood of this data point, it will be the closest point and I will be predicting its value.
00:04:46 If we think about what this turns into, we see that each data point can be thought of breaking up the space into a Voronoi tessellation.
00:04:53 The set of points that are closest to a 1 is called a Voronoi tessellation, and the set of points that are closest to a 0 is called a Voronoi tessellation.
00:05:00 to that data point than any other data point.
00:05:08 So, for example, all points in this section of the space are closer to this data point than to any other example data point.
00:05:17 Similarly, points in this set of space are closer to this data point than to any of the other data.
00:05:25 The decision boundary, then, is simply the set of these boundaries that border one value on one side and a different value on the other side.
00:05:34 That will be the set of points that are transitioning from predicting a 1 to predicting a 0.
00:05:42 It's easy to see that this decision boundary must be piecewise linear, since if we look at any pair of data points, we can think about what the set of points that are equidistant between them, which will be the point at which we transition from being closer to this 0 to that 1.
00:05:51 If we draw a straight line between those two points,
00:06:00 the points that are equidistant between them must lie on the perpendicular to that line.
00:06:10 So, locally, this point is halfway between the two data points, and if we deviate slightly perpendicular to the line between them, we'll stay equidistant between those two points.
00:06:20 Then, if we simply follow that line a little bit further, eventually we will become closer to this data point than we are to one of those two other points.
00:06:30 At that point, our new boundary will be equidistant between a new pair of data points and will be perpendicular to the line that connects them.
00:06:40 Continuing on, we see more boundary emerging as we draw the perpendiculars between more pairs of points until the entire boundary has been drawn.
00:06:50 Again, the decision boundary will then...
00:07:00 be the set of these lines that form a transition between a region where we would be predicting 1 and a region where we would be predicting 0.
00:07:09 If we have more data points, this procedure stays essentially the same.
00:07:18 But the decision boundary itself may get more complex because every pair of points produces a possible decision boundary being the line between them.
00:07:27 So the more points we have, the more possible segments there are to form our decision boundary.
00:07:36 Of course, this also depends on the layout of the points, but in general, for nearest neighbor methods, more data points can lead to a more complex decision boundary.
