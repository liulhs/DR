{"docstore/metadata": {"2a73d618-a0fb-47fa-a08f-2e07785d5d8f": {"doc_hash": "e10c1517837a7616e0dbf4579ed6076df6b40e5e7ae9138636159312d2198d74"}, "766a02fc-7e51-4158-976f-d61779c530f0": {"doc_hash": "ff277ceebdd15cbca6837d2fb2153cb733aedfc2477d8c785f869f2e30f9a303", "ref_doc_id": "2a73d618-a0fb-47fa-a08f-2e07785d5d8f"}, "3002568c-7732-4f1b-8307-bab5cdf97c06": {"doc_hash": "049404f4380d06e3ccb49940a60e9a1f6adc9107545b58e2e13901dc4fc8040f", "ref_doc_id": "2a73d618-a0fb-47fa-a08f-2e07785d5d8f"}, "32c73033-a333-4db8-977f-b09051c4bdff": {"doc_hash": "18585c990f801fb75fd1c3155c609ba1a080671b0bd9a03d86e447cc90149570", "ref_doc_id": "3c5e79d0-be54-4b95-b6b4-2da42b62a478"}, "a061ad5d-9101-4671-ba12-ae7fa7d15d7b": {"doc_hash": "9b7400a3585928be0cbb40d301e8a94d800e1a39b168309a4f27cb0232501a38", "ref_doc_id": "3c5e79d0-be54-4b95-b6b4-2da42b62a478"}, "25a3b692-f483-4101-83e4-1d758d859434": {"doc_hash": "1969eb8a16b86e791bb9d2482da3942f226ebee061ca9ec591d019ee66159c20", "ref_doc_id": "3c5e79d0-be54-4b95-b6b4-2da42b62a478"}, "3c5e79d0-be54-4b95-b6b4-2da42b62a478": {"doc_hash": "da4374e265f1fef0d256feb75845cd02a3190812087dc6dbfb7508dcf60f94a9"}, "b7d4f842-9ceb-41ec-9eaa-3df2fc47b14e": {"doc_hash": "03d5c35c7d9bb6d2a3db46edc0171ce3b4952d562d4a9c2c5ef3d6d00ab0a121", "ref_doc_id": "221b314e-7748-4d7f-a195-4bec511e846c"}, "72c351e9-ce3b-450f-b1a2-97ebfc0c207b": {"doc_hash": "1dd413183b7b02f4f82388e18ccbcdcd4dce0532cfd24caed0ff5a3dab9c2bfa", "ref_doc_id": "221b314e-7748-4d7f-a195-4bec511e846c"}, "fd80c443-2757-4b89-8b1b-520870a1da44": {"doc_hash": "69bb67dcddb88569c1363185d80549c783eb56b94f4875902157047d62ad6ed6", "ref_doc_id": "221b314e-7748-4d7f-a195-4bec511e846c"}, "221b314e-7748-4d7f-a195-4bec511e846c": {"doc_hash": "d869abb3377f9a983345fd5d58e332b7bc33676f4e3d6b98057ddda2713de17e"}, "2902509e-8b06-48fe-beb9-1c5670be2ee8": {"doc_hash": "ff277ceebdd15cbca6837d2fb2153cb733aedfc2477d8c785f869f2e30f9a303", "ref_doc_id": "fe380da3-000a-4742-9443-b5eaa35e5042"}, "d42ebddf-5646-4be6-9f84-fbd85492d3cd": {"doc_hash": "049404f4380d06e3ccb49940a60e9a1f6adc9107545b58e2e13901dc4fc8040f", "ref_doc_id": "fe380da3-000a-4742-9443-b5eaa35e5042"}, "fe380da3-000a-4742-9443-b5eaa35e5042": {"doc_hash": "e10c1517837a7616e0dbf4579ed6076df6b40e5e7ae9138636159312d2198d74"}, "d829baee-5699-4398-bb50-885e65afa561": {"doc_hash": "18585c990f801fb75fd1c3155c609ba1a080671b0bd9a03d86e447cc90149570", "ref_doc_id": "503fe2e6-0597-4404-b5b3-103cb2632808"}, "93057f03-ade9-4a36-9ea6-7df0a4d1106d": {"doc_hash": "9b7400a3585928be0cbb40d301e8a94d800e1a39b168309a4f27cb0232501a38", "ref_doc_id": "503fe2e6-0597-4404-b5b3-103cb2632808"}, "10370ebc-6551-4687-b1a9-4b18cc2af6c8": {"doc_hash": "1969eb8a16b86e791bb9d2482da3942f226ebee061ca9ec591d019ee66159c20", "ref_doc_id": "503fe2e6-0597-4404-b5b3-103cb2632808"}, "503fe2e6-0597-4404-b5b3-103cb2632808": {"doc_hash": "da4374e265f1fef0d256feb75845cd02a3190812087dc6dbfb7508dcf60f94a9"}, "c517231c-9e19-46b5-8bf2-85e8ae8a05c9": {"doc_hash": "03d5c35c7d9bb6d2a3db46edc0171ce3b4952d562d4a9c2c5ef3d6d00ab0a121", "ref_doc_id": "88aa12d8-ab91-4cb0-8e27-c2c81c318ca4"}, "b422bef4-3051-4a00-97a9-414a2d12bd9f": {"doc_hash": "1dd413183b7b02f4f82388e18ccbcdcd4dce0532cfd24caed0ff5a3dab9c2bfa", "ref_doc_id": "88aa12d8-ab91-4cb0-8e27-c2c81c318ca4"}, "d9ff7407-55fa-47c6-8a43-11b28d350374": {"doc_hash": "69bb67dcddb88569c1363185d80549c783eb56b94f4875902157047d62ad6ed6", "ref_doc_id": "88aa12d8-ab91-4cb0-8e27-c2c81c318ca4"}, "88aa12d8-ab91-4cb0-8e27-c2c81c318ca4": {"doc_hash": "d869abb3377f9a983345fd5d58e332b7bc33676f4e3d6b98057ddda2713de17e"}, "a03953c5-d372-4086-a721-98efd52726e7": {"doc_hash": "ff277ceebdd15cbca6837d2fb2153cb733aedfc2477d8c785f869f2e30f9a303", "ref_doc_id": "7c770b46-ce8a-4e25-87ba-fc85fb1ec708"}, "47382864-30b1-4572-9790-abb13713f7a0": {"doc_hash": "049404f4380d06e3ccb49940a60e9a1f6adc9107545b58e2e13901dc4fc8040f", "ref_doc_id": "7c770b46-ce8a-4e25-87ba-fc85fb1ec708"}, "7c770b46-ce8a-4e25-87ba-fc85fb1ec708": {"doc_hash": "e10c1517837a7616e0dbf4579ed6076df6b40e5e7ae9138636159312d2198d74"}, "bae5941d-5d4b-423c-9892-b48c472bd4c3": {"doc_hash": "18585c990f801fb75fd1c3155c609ba1a080671b0bd9a03d86e447cc90149570", "ref_doc_id": "a29404b2-5ccb-4a7a-975c-a1efa702e8f5"}, "4ae56c85-2dd6-48f6-8109-552daf5e7dd0": {"doc_hash": "9b7400a3585928be0cbb40d301e8a94d800e1a39b168309a4f27cb0232501a38", "ref_doc_id": "a29404b2-5ccb-4a7a-975c-a1efa702e8f5"}, "010cd150-1ff8-42c4-9717-d51f5140971b": {"doc_hash": "1969eb8a16b86e791bb9d2482da3942f226ebee061ca9ec591d019ee66159c20", "ref_doc_id": "a29404b2-5ccb-4a7a-975c-a1efa702e8f5"}, "a29404b2-5ccb-4a7a-975c-a1efa702e8f5": {"doc_hash": "da4374e265f1fef0d256feb75845cd02a3190812087dc6dbfb7508dcf60f94a9"}, "dad14003-d5da-40a7-b80b-46d1abc32ff4": {"doc_hash": "03d5c35c7d9bb6d2a3db46edc0171ce3b4952d562d4a9c2c5ef3d6d00ab0a121", "ref_doc_id": "f92fa923-f522-44ef-b8c6-dd3f4169548d"}, "7845ea4a-c84a-460e-852e-098c51ebb80e": {"doc_hash": "1dd413183b7b02f4f82388e18ccbcdcd4dce0532cfd24caed0ff5a3dab9c2bfa", "ref_doc_id": "f92fa923-f522-44ef-b8c6-dd3f4169548d"}, "9d53e701-d53a-4475-a745-ea08ec4c58a9": {"doc_hash": "69bb67dcddb88569c1363185d80549c783eb56b94f4875902157047d62ad6ed6", "ref_doc_id": "f92fa923-f522-44ef-b8c6-dd3f4169548d"}, "f92fa923-f522-44ef-b8c6-dd3f4169548d": {"doc_hash": "d869abb3377f9a983345fd5d58e332b7bc33676f4e3d6b98057ddda2713de17e"}, "9cf60006-b177-4589-83fb-9339f1c67a43": {"doc_hash": "effb1f12aacf446092a22520d6213ad176424a28cb207790f4f83ef5292efb71", "ref_doc_id": "11814e8b-07cf-4a83-9007-407e89485cc9"}, "35ea3d05-4902-42cd-8839-4438234fd328": {"doc_hash": "f89e26fd19f9622eb77ebe49c0de26b4be3f07c59275d09cedb5744fd0be0058", "ref_doc_id": "11814e8b-07cf-4a83-9007-407e89485cc9"}, "11814e8b-07cf-4a83-9007-407e89485cc9": {"doc_hash": "9ec17d15923037f8db947a19eaa3995737c05dfd059eb95bf752775de06cc993"}, "5839e6a5-eaa0-431c-830d-d14b4a8e8bbc": {"doc_hash": "d579df9bea198129bdf0fe20fa027730427f4c4fabeb1ffdf61bfe2ad5b02fa7", "ref_doc_id": "0f7f690f-5604-4e6a-b4f4-2e1b47e7fc3d"}, "5bb0498f-0b64-450b-b834-436966d126b5": {"doc_hash": "c05b8439d9a0a88fa65350b1c08d9bd0a88319629800593b68b3966f84a54076", "ref_doc_id": "0f7f690f-5604-4e6a-b4f4-2e1b47e7fc3d"}, "0f7f690f-5604-4e6a-b4f4-2e1b47e7fc3d": {"doc_hash": "4f199ab66a0cbf459668116002f3499129d6ceba9f43c88eaa82becb2fe7b4ec"}, "1fbe23d1-72ad-4516-9c9b-c65f3b22feb8": {"doc_hash": "16d97b351ffa443a0dcc607ba33dbcbf235283bce27bcb93eed6c033d94cb861", "ref_doc_id": "e26737a4-a6e0-44fb-a472-ccbc0760a1e6"}, "9058178c-17c3-404e-b039-0bcf2cd0b162": {"doc_hash": "58f16b26e992dbd158edb11160e23a9e40085992b67bccc1ec273239a16aa734", "ref_doc_id": "e26737a4-a6e0-44fb-a472-ccbc0760a1e6"}, "e3689ead-3f6f-4393-8383-1ac59c5fbe8e": {"doc_hash": "e7e9ad7729a605728f15805799ff2e47090e98e2cced92df79f949f2b1b69598", "ref_doc_id": "e26737a4-a6e0-44fb-a472-ccbc0760a1e6"}, "e26737a4-a6e0-44fb-a472-ccbc0760a1e6": {"doc_hash": "3c1c613859a8491a5e02627b811aefa87b16f2f38e1ae84a6e7b22e77943947e"}, "65ec63f8-fd43-4e05-8f2d-82873c9eb735": {"doc_hash": "50ea15f74a664a713c12caa3f4db9653719a46fe2e39ffb35396f9766dac644a", "ref_doc_id": "44025363-bc17-4899-ab4b-c6b259f41354"}, "3434e657-16e4-435a-a482-0d46aaf64678": {"doc_hash": "5e6650246ee6c6d2d383b618573eb9e2d06614f167785197ccf9ac6f5db1a0b5", "ref_doc_id": "44025363-bc17-4899-ab4b-c6b259f41354"}, "72b188f8-cdff-43b0-9b9c-77022445092c": {"doc_hash": "fb5efdd4c7fcad7fdbb34c2a613fc2e92ec1480958c6e96dc566a460190e5f8b", "ref_doc_id": "44025363-bc17-4899-ab4b-c6b259f41354"}, "875d49c4-5294-4efa-a5c9-d540ddbb1170": {"doc_hash": "8fc826ae53a4b112226dc04b7a87424a7a64e52adc9d5e18a0c0ed9b5995b21f", "ref_doc_id": "44025363-bc17-4899-ab4b-c6b259f41354"}, "957ee664-a093-4a05-9dde-66c96ebded45": {"doc_hash": "22cd064be9367df8c669f425322d3ac08eda9dcb2f6dc64eb657e6a738a009c4", "ref_doc_id": "44025363-bc17-4899-ab4b-c6b259f41354"}, "8c9b09bb-01bf-49d0-a246-c0da6c9d834e": {"doc_hash": "200bf788e8a6ae268e6d61d3f8a6d4cfa85fc08280bd8c8f119809620e0c9645", "ref_doc_id": "44025363-bc17-4899-ab4b-c6b259f41354"}, "f7e0e680-382b-4c10-99b3-1cddbe2f63a5": {"doc_hash": "ad7e58967842a1635c32ce35dbf1ea9c37150b5d5f940cf8a21c68d8d765148e", "ref_doc_id": "44025363-bc17-4899-ab4b-c6b259f41354"}, "44025363-bc17-4899-ab4b-c6b259f41354": {"doc_hash": "a4f250dd6dbfb9d844c1fa60744f885e1a9b121af2059bcaf55d74567ec97919"}, "e035af73-c916-403a-b52f-9683e76d0f68": {"doc_hash": "7f9c6d465a6fd21885d62269cc79efe8fb8d6814ed08ce11cea7f4173c848bca", "ref_doc_id": "ea3fb7fe-a2b7-4d16-a969-fc97e37328ee"}, "8c180bb5-173e-47f4-96ff-f5d0479a1d51": {"doc_hash": "8f5cef81ff683733b41d3be8cd813d36259dcb8b73caf0bcda3d14818e8c5cce", "ref_doc_id": "ea3fb7fe-a2b7-4d16-a969-fc97e37328ee"}, "7b3d0846-c6bd-4b63-812d-ff0d33d2d739": {"doc_hash": "596d062489e5b1ce16c1e46f71644b03912f493dd93fb01593091df25a2b650e", "ref_doc_id": "ea3fb7fe-a2b7-4d16-a969-fc97e37328ee"}, "4213762f-d15f-480f-b4ce-c48c0fa7a006": {"doc_hash": "83d4bfc7f369b134ef2c8ecd7f54faa97ee40bca8728ed280c2a12873b835ba6", "ref_doc_id": "ea3fb7fe-a2b7-4d16-a969-fc97e37328ee"}, "ea3fb7fe-a2b7-4d16-a969-fc97e37328ee": {"doc_hash": "7d359be2d6e5b53f900454cd4f12ffc532eb287d686bf14818b2a102f3577ac1"}, "54936631-28a6-416c-84b9-8e7dae428d9c": {"doc_hash": "3c2a05f0babf7494ee9830f3db586dce5c6e07f2ebc7ad9a3c360b98cf9026e8", "ref_doc_id": "bd947843-9be4-4dc4-84df-ca245ad4e9a6"}, "7ed96378-5b61-47b0-99ef-cc6a0c4f1172": {"doc_hash": "4b608bcf619cc1c651e388d9823c5566fc432734436eaf9c11cd1daeedd40cf6", "ref_doc_id": "bd947843-9be4-4dc4-84df-ca245ad4e9a6"}, "485e98b9-2f0e-47cd-b50a-93425a1710b7": {"doc_hash": "3263fa286144a96d8971a712d006d663350a946d3a0d5317ac39a5f890fcfd74", "ref_doc_id": "bd947843-9be4-4dc4-84df-ca245ad4e9a6"}, "c828cb87-65fd-41f7-b117-64e0e8cc6c6d": {"doc_hash": "7f2715ba9a085ad7edb846f934febd6dc52ebe4ab0b553513500175379d08ea4", "ref_doc_id": "bd947843-9be4-4dc4-84df-ca245ad4e9a6"}, "33fed07a-e4f5-4204-a3e1-616c099f2f56": {"doc_hash": "cc6c0a46e5926af732e01448fad4d2892737839329716f8184a206d75e528fa6", "ref_doc_id": "bd947843-9be4-4dc4-84df-ca245ad4e9a6"}, "bd947843-9be4-4dc4-84df-ca245ad4e9a6": {"doc_hash": "ae76dfe7bf5142afd9bcede947d2d1664bb22bd8cb91a069c276247d142f8620"}, "292e19a9-c5ed-444c-867c-2bac8cfbc822": {"doc_hash": "1369744837ce6efa1c4885babbc386ee8b1516f3223f14aaa7ed0c21b44cf094", "ref_doc_id": "11490156-0a15-4f83-b095-50e657d53446"}, "3552e3f4-2f63-4406-935c-99c458285615": {"doc_hash": "b925e98cd5d963862c88ad278770cb67ed26e7846a56f3f7b787807be3c199e0", "ref_doc_id": "11490156-0a15-4f83-b095-50e657d53446"}, "7826277c-9417-4413-9b9a-8e6bc35f1f42": {"doc_hash": "7b84d6b19d3183b4155261f9c5490af22b9f8fcd28d392945c091fa107966dcd", "ref_doc_id": "11490156-0a15-4f83-b095-50e657d53446"}, "7ef4dca3-1aaf-4e15-a582-37b278e9a79c": {"doc_hash": "7ca5253d320d8d62a025596dac2212b0cec9ee095729e9b9e9adffd39a871fba", "ref_doc_id": "11490156-0a15-4f83-b095-50e657d53446"}, "11490156-0a15-4f83-b095-50e657d53446": {"doc_hash": "949742e25526889534eedb79f94dbdaf74f1196fc3069aaefa6fb7c9c3f923bd"}, "c576900f-0b9f-4cb8-bea1-87f320e17447": {"doc_hash": "84bc2c04dc46c8d06ffb6bfdccbbfbaaeb9ecbc9b09323f1986b35e7042ec2f0", "ref_doc_id": "4cbd714d-1dc7-4772-8616-809dd14dfa8c"}, "149361b9-81a6-4f31-acd8-7ea4a8304c64": {"doc_hash": "27e955fedd193bd4bd8986aeba6df80f6334226da9b3f4f90b19f446126f99ea", "ref_doc_id": "4cbd714d-1dc7-4772-8616-809dd14dfa8c"}, "4cbd714d-1dc7-4772-8616-809dd14dfa8c": {"doc_hash": "54b0c2bb9aafe256f5c752f929540d58365d0f54c54a7b3b0ac125e0c674f99d"}, "5fa99543-adb9-4c6b-9e25-8a327c1ed2af": {"doc_hash": "c9bdec4953811d8cb3c75859ff05267d0efcf65b9c36e0cea229e41e470db086", "ref_doc_id": "a9bd8381-f1ca-4988-9f69-905b663fcd63"}, "1dca1e48-c968-4bba-940e-f5b9d6a7019b": {"doc_hash": "e3377abcb0a889aaa7cb21359a8961ca6eebc6d540eb20c5c1610554931affa4", "ref_doc_id": "a9bd8381-f1ca-4988-9f69-905b663fcd63"}, "ff47ee7d-d1cb-4b0e-af68-2d9c63118990": {"doc_hash": "21bbab4650e76b18939020299ddbce68337f484aca189329916f6e34ae2f09bc", "ref_doc_id": "a9bd8381-f1ca-4988-9f69-905b663fcd63"}, "292270a7-ab53-42ef-83d9-fc1d0bee6e04": {"doc_hash": "b90ad3ff70942e28d9ca688e32c9ae6806b58befcbee47a5a93fc0af2d4c37d4", "ref_doc_id": "a9bd8381-f1ca-4988-9f69-905b663fcd63"}, "a9bd8381-f1ca-4988-9f69-905b663fcd63": {"doc_hash": "9c66147e4b778de82ed3589b3baef6fa462c7c78a8898a4e393f21dcf781cce0"}, "deb239fa-0355-44af-831e-3e846245ebfe": {"doc_hash": "e2dba19a664add2082906176aee9a76055c90d0f32889da3097ea44631b1bf93", "ref_doc_id": "bfd7ab5a-ef0c-467c-81d7-3b0f04b82a7e"}, "f738bc61-20ba-4adb-a594-ea9e61a81636": {"doc_hash": "458cd6120888a5e77404ac50216b88b8a1c557956b3493dfc429afaec5b6c0b5", "ref_doc_id": "bfd7ab5a-ef0c-467c-81d7-3b0f04b82a7e"}, "ef3d2650-b6d0-4a66-81e4-acfed19b2906": {"doc_hash": "8ab0b26d582d78d1d18f9b2803e7d013dabec79a1565eb4da7be381943668378", "ref_doc_id": "bfd7ab5a-ef0c-467c-81d7-3b0f04b82a7e"}, "bfd7ab5a-ef0c-467c-81d7-3b0f04b82a7e": {"doc_hash": "4daa41264bf7a7461f85152d878e37def8e438a040b2f4de4e99f0d58dba3a38"}, "4b6c04fb-57ae-431f-98be-966ce6ac433b": {"doc_hash": "413ee06879b2ad0cd094bb41ce5159c252841b17d8991a9f115caeffc0af916a", "ref_doc_id": "fdf25942-8649-42aa-8b88-e46636dafd71"}, "5149e57b-559b-4ff0-911e-1d550e65289b": {"doc_hash": "98a6c3dd51536b44a6fd239bf5f18b0272a587cb1959bb6dec5da576d72d3111", "ref_doc_id": "fdf25942-8649-42aa-8b88-e46636dafd71"}, "fdf25942-8649-42aa-8b88-e46636dafd71": {"doc_hash": "758090465317e3c1ccbf8f062a5148968790130bb06852b9fc3ea0dde23c0153"}, "8f0bee1b-c68d-453d-bc81-ecbf8ac787f3": {"doc_hash": "a603b2d022349e0f89ef4a68c94d79331a1a10d9c777909e92c8d2b525b989a4", "ref_doc_id": "d68407ec-67f4-4efb-b3c6-b4fe4a6be662"}, "f3b790f1-080e-42c1-b2f2-f119dfffbd4d": {"doc_hash": "bf2403c1f9a9a7452d666e5b06379e12d7195d213820024ade99a154002d0cfd", "ref_doc_id": "d68407ec-67f4-4efb-b3c6-b4fe4a6be662"}, "d68407ec-67f4-4efb-b3c6-b4fe4a6be662": {"doc_hash": "80ff7e84579176ce5a844ddd49a8f4fca8e10d9cd5b891e731d034b1e2f272ed"}, "2132b4c6-ed46-4f94-a04f-d653467bb4fb": {"doc_hash": "596ff3a3b849a5e6b2ce817eb28ef1719deb6ffae58029903998091ef2b63ef2", "ref_doc_id": "824186cb-a303-46b4-b44b-aff2c4e05976"}, "4b2225aa-9426-4650-87a8-ae02e6fa24ac": {"doc_hash": "4d38d4ccf9ccaf85fb5fd251920d3a36c7f5416431711e11311e5d3f9e145d7f", "ref_doc_id": "824186cb-a303-46b4-b44b-aff2c4e05976"}, "46c8cbc6-b40e-40b0-ba93-31e8ed3a7153": {"doc_hash": "b8ff35be35b7a36884f8a10b0a74b4e7335685f99ce8baf25a25c40f912b0e2d", "ref_doc_id": "824186cb-a303-46b4-b44b-aff2c4e05976"}, "824186cb-a303-46b4-b44b-aff2c4e05976": {"doc_hash": "928cb325b0e366a60e0ea3fc35eff3ec5c7ea2d6b5fe7de48ab1769efdc8a650"}, "c60c1d2e-2cd0-4364-8584-fdb48fc08239": {"doc_hash": "7b9b7903d525c42c3f421e390176871d2646bfef5f1f120ecbf7bf0f3ca14e89", "ref_doc_id": "5968ec8e-fdf7-4303-9893-a11fcf769a1d"}, "415f5947-a43c-4c6d-a488-21de9d7b8b11": {"doc_hash": "e6f05d672615f0dec5dd9ee13b06b52acab875f006422c2cba26220f89e5e83b", "ref_doc_id": "5968ec8e-fdf7-4303-9893-a11fcf769a1d"}, "5968ec8e-fdf7-4303-9893-a11fcf769a1d": {"doc_hash": "9018f1decf1c68358b189b5a97251ea0c6148a15bd16e767e6fee7997108b67d"}, "78d4fb6d-a10d-4ebb-a3f1-eb836fe2f061": {"doc_hash": "2d4be6885cc243fb031744d4338c6dc0dc2691dfb4c53289b3080927dd0f558a", "ref_doc_id": "81b9bd71-f7de-42af-b524-7a932c901501"}, "0951632a-9f6d-4adc-bc0b-893acbdce41c": {"doc_hash": "72cc29d5ac2723e7c70d0456d610b9922bc9431825bb6e9746efb9aaa7904e42", "ref_doc_id": "81b9bd71-f7de-42af-b524-7a932c901501"}, "0c904fd0-7790-46f6-9dc0-0b7bdcecb61e": {"doc_hash": "8ee9c7593a4a5d9a9d38ca540e53c2916b059d24b74dec42c78b97d4e831e9d7", "ref_doc_id": "81b9bd71-f7de-42af-b524-7a932c901501"}, "e2d9ba94-8a2b-42e1-be77-0852437937b9": {"doc_hash": "eaabf85348ef0a3e83cc396f8130b1b619f044f6d5327271cea80dbf39ecfc16", "ref_doc_id": "81b9bd71-f7de-42af-b524-7a932c901501"}, "59503582-ad79-4685-a003-1c1b84ae1ac5": {"doc_hash": "75421c135fda8343691d6c447bed3729a15e39cb19a24539928be13f8ddb6cd3", "ref_doc_id": "81b9bd71-f7de-42af-b524-7a932c901501"}, "0959206a-ee7b-4d33-bf00-5337825e9cae": {"doc_hash": "a411009dcd267794d7366dc029f2aa30e3edc96b6ee453e25dcfdd2f019d7529", "ref_doc_id": "81b9bd71-f7de-42af-b524-7a932c901501"}, "7b566d6c-6e13-4b35-aebb-dcdf7504edb0": {"doc_hash": "774d4da71b5f25564f7b13b0df2d6a6fce84208c5e3799379f9aa49e608f180e", "ref_doc_id": "81b9bd71-f7de-42af-b524-7a932c901501"}, "525598fe-a218-4f11-bdf1-901fa523417c": {"doc_hash": "2c4e446d4ca62b693673abbac67abb271ee9ba762051cda0fc7a2acede562ca7", "ref_doc_id": "81b9bd71-f7de-42af-b524-7a932c901501"}, "81b9bd71-f7de-42af-b524-7a932c901501": {"doc_hash": "fe386147345284bf23631edcc7473fa35a7821d19263743c2b6068b0d6ced6bd"}, "9418fddb-01f5-4bec-893d-75a157c6819a": {"doc_hash": "9a155f95a7af860eaeb5769e0ff32a27cce3fd2da3e690800103d9442acd30bb", "ref_doc_id": "ea172faa-1bf1-48bf-8470-6e4ea7dde048"}, "d2e44479-2d65-43dd-ad9a-6e63d3182a0b": {"doc_hash": "5f549a7af1ea8ad6d8ccb15c6fc2f7db8b79f3e12250c75a9b2125a46aece620", "ref_doc_id": "ea172faa-1bf1-48bf-8470-6e4ea7dde048"}, "cc45f623-2cc3-4c80-9e99-5a571a8452e0": {"doc_hash": "495e706f7848f964f61ee1268061387364b54f94cd8a1e9752924dbd17dbe126", "ref_doc_id": "ea172faa-1bf1-48bf-8470-6e4ea7dde048"}, "710f7a71-1614-472f-ac06-d6bffc79e888": {"doc_hash": "811cbd4226446267fae43891aebebb67c02f2cf2c68f8f0d2972043419c1b7fa", "ref_doc_id": "ea172faa-1bf1-48bf-8470-6e4ea7dde048"}, "ea172faa-1bf1-48bf-8470-6e4ea7dde048": {"doc_hash": "475e739cce399c56f474e2d734410d5c99cd4576e1c5cc444ca2ae83e3d40e52"}, "4c1cee8b-7db8-4372-bb94-78cf12e59d4e": {"doc_hash": "570b47d720f4d77eaee52622909997df5b03e39e1dd1b186fff5e00815541264", "ref_doc_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2"}, "29b48206-89f1-45c4-97a3-4b36a0326e56": {"doc_hash": "9ae47ef45de205a9c802abcae6ba75fc4106c0e9cf4a45d7ad9d8c72af1dbcec", "ref_doc_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2"}, "f7ca81fd-a58f-40cf-9035-9aff931fdc00": {"doc_hash": "04197f3711eca704e4293b43d907cdde7a1997a605b6bb3d8d7a695bc48c423e", "ref_doc_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2"}, "47d8a91e-cafd-4ea2-a255-12d43902aa3d": {"doc_hash": "6e9ecf23a6154357e0ff594b2d22078ed8aa2418aba4eddefbd475a5de288361", "ref_doc_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2"}, "febc4676-b683-43a7-a209-fe09ef9c44b9": {"doc_hash": "419bd34c6e84728886e365e16db7ce4eaa93365ad3b0f5fa0edffb8b68fa5ec9", "ref_doc_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2"}, "f597d9da-7cc4-4d85-9c00-8ee8edad52df": {"doc_hash": "ff2ff66bdffe179052d10f127393dc47f32c40245535b9eed8a14ae660cd6ec4", "ref_doc_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2"}, "0f5201a0-675a-4249-98e3-4b7759737873": {"doc_hash": "7bee5f5825e2ce36d1d7dfcf99fbe807f097227bf1f72f17f3d44ba5bd3f7e8f", "ref_doc_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2"}, "1b42695f-4068-4da2-a2b3-de44867b9bb2": {"doc_hash": "5fdf2883968745ea1e71fb808c633a8184397098f25a49232e5fa8f685258252"}, "eeae53ca-af32-4d06-9d59-3fe6d83a7314": {"doc_hash": "0e7c05b4feab75d6a2f5764995ee12a3dc2df9d1393429c17a484700aac19c6a", "ref_doc_id": "7994668a-0e4f-4d5d-8d53-d7ba6586a3cc"}, "0e9b6eed-cafd-4bed-bef3-cb2deba8e295": {"doc_hash": "be068ceac1fa6f21cbd21b8497de9882e90eed1c149a4c1e4c4ce7401620ca66", "ref_doc_id": "7994668a-0e4f-4d5d-8d53-d7ba6586a3cc"}, "331c113b-be24-437e-b21c-fed3276897f7": {"doc_hash": "4969905a956a6272b067cd11967dba7b419c06483b0cde1703b565e96e58e7fd", "ref_doc_id": "7994668a-0e4f-4d5d-8d53-d7ba6586a3cc"}, "e67e2ce0-127b-4b8a-8f30-feaadd7db1b0": {"doc_hash": "fb58b24b2f50825977c63bccabd3e534bbc72b27b7e62b3deba6b3351c1283f6", "ref_doc_id": "7994668a-0e4f-4d5d-8d53-d7ba6586a3cc"}, "eb718235-0b11-4f04-a710-9767beed3cf7": {"doc_hash": "a54315f97d14a6db7ff0686fbc226576d541136eab710391721c9e242fbe9dea", "ref_doc_id": "7994668a-0e4f-4d5d-8d53-d7ba6586a3cc"}, "7994668a-0e4f-4d5d-8d53-d7ba6586a3cc": {"doc_hash": "c2620047d43692f96c7f1e805f4813c0549f41a398e51f88e93322d4541054f9"}, "fec9bd48-de84-4037-a5db-1a1c7f9123dd": {"doc_hash": "bde7eb6cbe6be85ecc34c2fafd4c4e5e155a8e21bd217b0f282b54bfbea5afea", "ref_doc_id": "b36b93ab-3813-4bdc-a4b0-bbfd558bccf5"}, "d8f8a750-e266-4933-9267-129ee24223df": {"doc_hash": "7089cda2f03ed23982a71f8dcaae5e01add6eb3513b2d443dc4aa87842ef416a", "ref_doc_id": "b36b93ab-3813-4bdc-a4b0-bbfd558bccf5"}, "19a53875-5b16-43fb-98c7-5918680dbf33": {"doc_hash": "667ec7130308e7b3f3b4a0951cfc303928974f1c0c5ff64a440ecdb2d66ee586", "ref_doc_id": "b36b93ab-3813-4bdc-a4b0-bbfd558bccf5"}, "b36b93ab-3813-4bdc-a4b0-bbfd558bccf5": {"doc_hash": "6bff6e4d034551063fe2890c001a8b22513b9862453eb84346bde7297af7c67c"}, "9c107ad4-d38a-4023-826e-e4edd65f7002": {"doc_hash": "96ad324cee7442872dcf5cd5b1ee6d46f4334aaf257c879b61aa4833d0b1c76d", "ref_doc_id": "ab07d92b-bcb6-405e-b831-d099703e2bdc"}, "bd04d83f-084a-431d-a00a-5ad7437ebc61": {"doc_hash": "64cc24fd1fbaead9f262d51e4a74c0cff62d71982cd139f1dada05b82ad7ff62", "ref_doc_id": "ab07d92b-bcb6-405e-b831-d099703e2bdc"}, "64768b67-abfe-4b47-90b7-7cca631471a5": {"doc_hash": "2e88144a1c29e39d66df31f30ff3d39489a9c44462c1834d2ae335cbeab2e47d", "ref_doc_id": "ab07d92b-bcb6-405e-b831-d099703e2bdc"}, "dbf64994-6b87-40dd-aa98-60fdbbc9fb92": {"doc_hash": "ba1d6bffcdf88a9a78b7aa49a464ba55a57c7043f0a08286982ff983fd0ac766", "ref_doc_id": "ab07d92b-bcb6-405e-b831-d099703e2bdc"}, "ab07d92b-bcb6-405e-b831-d099703e2bdc": {"doc_hash": "1aad39722e82eedc09957e217ba147ce4f8ed226dc1644436032ddb2e8e62467"}, "7d3ea0b0-1025-4fa4-9738-82d1d6c64ae7": {"doc_hash": "7956b37bb8a3b201a4e8eed6ca3f260c764285bf0fa4dd7850d153bbad366a96", "ref_doc_id": "df21632a-c358-487b-8023-0ec7e7add3dd"}, "3496f612-a1bc-4b22-96f7-9d314615e8af": {"doc_hash": "45939ea65a155a57fca710b7daddcf4055cbf12e1a7098915f2013a6e2d1a2a0", "ref_doc_id": "df21632a-c358-487b-8023-0ec7e7add3dd"}, "45577703-9eba-4055-a6d3-b8a99e089027": {"doc_hash": "48477cc52ec2bb82c9a52ed2965627345c5f1314d68d2758e89c155f7066518e", "ref_doc_id": "df21632a-c358-487b-8023-0ec7e7add3dd"}, "9a958742-7e86-474a-b13d-f0f00d11a708": {"doc_hash": "4bd60f75ae660296aa2d4d01a88f27621ce7b94b284093f0f67aa36c2dfdad30", "ref_doc_id": "df21632a-c358-487b-8023-0ec7e7add3dd"}, "df21632a-c358-487b-8023-0ec7e7add3dd": {"doc_hash": "2880d7b1eb94f88bc8ed5d017008b77cd93075f740121eebd5e9c8578fb1a17f"}, "02a8e608-a26f-47b3-a592-8c954e86ee29": {"doc_hash": "a5713984d4e4e099921733ea9058b9fde8c5b9d71f75e4b34bbe2283479bce2b", "ref_doc_id": "ba613405-9475-4938-ae68-036aa597a1c3"}, "ee1a19b4-8c93-4840-a75e-2312c9f57e69": {"doc_hash": "fd355ec8d7d2e9a23dbe5cd91d21308b37dc413a89d2c415e0ab28e5e27015e9", "ref_doc_id": "ba613405-9475-4938-ae68-036aa597a1c3"}, "27fe44aa-0e3b-47e7-bf07-0f04f9f0258e": {"doc_hash": "a0dbe4c3411dbad5f15e53230d07aec96def7371382c7b644ae506d41cbb5704", "ref_doc_id": "ba613405-9475-4938-ae68-036aa597a1c3"}, "2f75a080-1144-4618-b51b-cc74e0b550f1": {"doc_hash": "ad292e3cecae03a15e5adc9d5adc2e9240dfaac9c5c3740767015c37f936b8ba", "ref_doc_id": "ba613405-9475-4938-ae68-036aa597a1c3"}, "ba613405-9475-4938-ae68-036aa597a1c3": {"doc_hash": "cf1c561d172ba9d35cde96b27b8527a7eba5b471d2e85aa3542f3b6118fbfb6e"}, "10ad2bfa-9f83-44fe-a81d-e2353aa6ae38": {"doc_hash": "90dbfc2bdb14f2d2dfc218c117fb44d8253b9527151013c75ca555132229babb", "ref_doc_id": "847416df-5686-4eaf-8108-2a52e706903d"}, "cfb9428d-3e32-4477-99fe-ca200021d2b1": {"doc_hash": "259d2f83e349a69b3b63a3682cdb90d952280a437f21c2f6700a25bbc6a29895", "ref_doc_id": "847416df-5686-4eaf-8108-2a52e706903d"}, "29793935-f7ab-4e57-bf87-c7be91afdbb1": {"doc_hash": "c326676b1f724c38c49281103c2a6c9eda5965056fcad4ff83400a0ef0d5c1dd", "ref_doc_id": "847416df-5686-4eaf-8108-2a52e706903d"}, "a601ba43-885c-4927-b04f-68311704d0c4": {"doc_hash": "571798f4f04c2160578f3aa28f241e2b65f4294f1b9a1b7ecf584e107aae726f", "ref_doc_id": "847416df-5686-4eaf-8108-2a52e706903d"}, "847416df-5686-4eaf-8108-2a52e706903d": {"doc_hash": "c8948f66fa28f7d8fd216a059e30844417c050c1a998f2db93ee56c0f3fcab19"}, "91d648f3-73ff-4350-9ad8-6b56bc5e4913": {"doc_hash": "4552d96b0878af7854ab3f2d7b36b21cff8bc96f1fe0419f7e5712b7e70194b3", "ref_doc_id": "b1b1131c-be94-4192-bc82-af8e247fa58c"}, "65206850-f9ff-4992-af30-2965d3f30b20": {"doc_hash": "f84ab505430704937e235f8360c3947243dc299fe1c12c4c33c6e9c085d3b8cd", "ref_doc_id": "b1b1131c-be94-4192-bc82-af8e247fa58c"}, "b1b1131c-be94-4192-bc82-af8e247fa58c": {"doc_hash": "a5a4f55768f626086f44a0dc6476c3a6a802aa59faa5897eb871db98b512dc77"}, "6abb9c2b-add7-466a-b907-b02bba4487e3": {"doc_hash": "459b838ce3f15494a019eb5295e2fb71c925aa259242c90bd448e6b6daea9ced", "ref_doc_id": "1376bd02-1d92-4d41-b9d9-92483544b484"}, "636b11df-74cf-4d00-bbfb-25b4e7eac215": {"doc_hash": "5456baf5b654c9bfea09afc39988ecc111dbe3b55c64b5ea304681c573ba7211", "ref_doc_id": "1376bd02-1d92-4d41-b9d9-92483544b484"}, "b56a6812-1b91-4a46-9acd-2c3f7f90ff33": {"doc_hash": "9885e1f5b6de0b2805e77314660a6cd418d2f8ee66948733b2bc5b17268eba88", "ref_doc_id": "1376bd02-1d92-4d41-b9d9-92483544b484"}, "3cc89a71-8e1c-44fd-89a1-ce324ef9e448": {"doc_hash": "f31be2da83a2c0a75172f8a2a3536bde01d5f42c2e6cadf916ef50f3c9f22602", "ref_doc_id": "1376bd02-1d92-4d41-b9d9-92483544b484"}, "b85dfca6-5620-4e73-baa9-fbc07b6d888f": {"doc_hash": "e01a31c17e4228f21526591d0c3a52832dba3578d203e8eac81630380ce3e015", "ref_doc_id": "1376bd02-1d92-4d41-b9d9-92483544b484"}, "1376bd02-1d92-4d41-b9d9-92483544b484": {"doc_hash": "18d41d7cfb94709b55e99d34521889c09fdb6b68f847652a0caaebd57c0cca76"}, "8091657c-ac4d-4c16-818f-84580dab4ebd": {"doc_hash": "66851e67dbc28429b470358c10c3c19101404c6a9b3daeb1a6a56ca01b4bde0d", "ref_doc_id": "dcf19e6f-6bf6-4a72-b5f3-42ec9228d639"}, "67f3c380-8ea5-42bf-8248-8c0982dd0211": {"doc_hash": "4cef15ab16ea9d3d5295ee7f21a23daf6d1101afff004e250144567778d34131", "ref_doc_id": "dcf19e6f-6bf6-4a72-b5f3-42ec9228d639"}, "948d7d48-3fe7-4199-9a8d-386661131ac8": {"doc_hash": "abc0e850956dc1acfa78862fb7bd63dcecca6a8e88ab514f6b462e0f0d8cb782", "ref_doc_id": "dcf19e6f-6bf6-4a72-b5f3-42ec9228d639"}, "dcf19e6f-6bf6-4a72-b5f3-42ec9228d639": {"doc_hash": "e0f2153cf30e8d0c752b1c9b9a219b05894ab5e0e1a6e4dcfe1b19aac3356c3a"}, "ce941a0f-87c1-4433-9aba-839c5a227b4d": {"doc_hash": "7b450bbe1fb6790277bf06aa19320d5cfc56738f46ad2c3c62565048ace512cb", "ref_doc_id": "fea08087-d98d-44fa-9415-8d1580ba13a2"}, "c10ea66d-6baf-4b42-8175-c8f215105655": {"doc_hash": "80118a8e0a5a6d324f2c3f812b80fae3530d144816e67f85989db930a45c39e9", "ref_doc_id": "fea08087-d98d-44fa-9415-8d1580ba13a2"}, "a7bacbd6-3f66-4c29-9421-d29e69ec3d4d": {"doc_hash": "097b1461978ac70960bef00f2cd3247632ea37246f2d3d0d0f5b84b6be0c588e", "ref_doc_id": "fea08087-d98d-44fa-9415-8d1580ba13a2"}, "e6441edd-7f7b-4260-8b90-b76f45044ae0": {"doc_hash": "7d9f7d568317f9ce92e30cd3304bdedb6b83fb16415dae281988d6e721c4e589", "ref_doc_id": "fea08087-d98d-44fa-9415-8d1580ba13a2"}, "59c66e4b-2a26-4bd1-b138-879e17a2e4bc": {"doc_hash": "a44360c02e901a43a9e34c7af387e92c01f816c6394bef4cddf2fe95712a3c72", "ref_doc_id": "fea08087-d98d-44fa-9415-8d1580ba13a2"}, "b3e495b1-dec6-4d60-b7bf-ca0ddd27402d": {"doc_hash": "ed7c9573f9dac28f8e6d45346446d652e7b14db97df8e4bba2da43da07012e72", "ref_doc_id": "fea08087-d98d-44fa-9415-8d1580ba13a2"}, "fea08087-d98d-44fa-9415-8d1580ba13a2": {"doc_hash": "0c14475784166e47c92dad495628e819e1b464dffe70027487431c840c97d0d7"}, "9063cc09-9b45-497c-8fe9-e8845bbef073": {"doc_hash": "3afa354cec26cf6f5242bc87f9d1499d792dfab2d766703d2230d5bd45333a9a", "ref_doc_id": "08bf9903-4e29-4ce8-8e26-10d4e97702ef"}, "b150b699-b9b9-48ea-8904-98c3ef4576cf": {"doc_hash": "e8e3dd01ba2b06e164c069c20b948b505e9f306ee0136eba18d1f421b3dcd614", "ref_doc_id": "08bf9903-4e29-4ce8-8e26-10d4e97702ef"}, "08bf9903-4e29-4ce8-8e26-10d4e97702ef": {"doc_hash": "b3d7b301295827f5922523d3161d56f9f3dbfc08340b156cf72dd8f93ba0416b"}, "97feb81a-b5b5-48fb-a956-b72c28aaf1e6": {"doc_hash": "e63ebc14de1c1a08d17447eebb0b148c47a2f766f9bd6cc1cc46729218318565", "ref_doc_id": "99c39f0a-04f0-4911-a220-d028fee40787"}, "019eb502-40bb-444a-aabf-5f475b752494": {"doc_hash": "5dbe1a6e0601f49349fd6ed96710c34ec49884c6649ad8e1e49d4fb191964118", "ref_doc_id": "99c39f0a-04f0-4911-a220-d028fee40787"}, "fa21369e-e5e0-4542-b503-eb8db636dd87": {"doc_hash": "fc5a5db77febdbff3c465568e7c0ad32df411b2e43c221d9fad7765c25eb97ca", "ref_doc_id": "99c39f0a-04f0-4911-a220-d028fee40787"}, "1510a1c6-f7de-42b3-b93f-fe7bacca89cc": {"doc_hash": "1e0ab5ba183a02188be031bbc5285ef375a89d1f6a55f287233f611e44703382", "ref_doc_id": "99c39f0a-04f0-4911-a220-d028fee40787"}, "99c39f0a-04f0-4911-a220-d028fee40787": {"doc_hash": "2210aef74e0e50cc9d510cffb5a09fb0e540a33ed0558e62d477c05f357ad6a5"}, "80c5d81d-6c0e-4d6e-b25c-1fca89b5746a": {"doc_hash": "4c6ad37a76a5fdaf5982eee54deadeb7e71b964bbd7bc96d482d18af35122395", "ref_doc_id": "9e0ebf7f-d8c9-4bd4-8a15-d7df331ee735"}, "6570c955-9b41-419c-958c-95a57a21c90b": {"doc_hash": "0bcd1ad69a4eb7661bdd82ec95ba483e5467710ebc5ec2aaf0bdb9704ccc383e", "ref_doc_id": "9e0ebf7f-d8c9-4bd4-8a15-d7df331ee735"}, "896773ec-8662-4e21-b508-aa20884741f4": {"doc_hash": "ef76f9b2e57a42ad5752eb7684c6ae98beff5e88e63e3c65ea1727590ff114bc", "ref_doc_id": "9e0ebf7f-d8c9-4bd4-8a15-d7df331ee735"}, "fe0b072a-c451-415d-89ae-d4448683d338": {"doc_hash": "6491cace05de199e9b91dc73fd7ce4708eab90e7ce5f66d0c7a824d53cb16ec8", "ref_doc_id": "9e0ebf7f-d8c9-4bd4-8a15-d7df331ee735"}, "960e6665-7228-48fc-9bdf-0c68d3d3392e": {"doc_hash": "a50a1669d0a6c7a131f929a4cd4070e81fbe17131f9461f64283dae5d48d6ad2", "ref_doc_id": "9e0ebf7f-d8c9-4bd4-8a15-d7df331ee735"}, "9e0ebf7f-d8c9-4bd4-8a15-d7df331ee735": {"doc_hash": "ceabeba7ff66ad3338e338738e4928c00326686ab573b42b87ce8ca37da8082e"}, "4a9a3b61-d5d9-40d1-a4c4-7c384c5ac200": {"doc_hash": "75764640262638040c830ab428b9324e18ff9b7b0a139da38b7f1b64a75c719f", "ref_doc_id": "23870692-93e9-4897-88f1-43397233b337"}, "d177c318-1fe3-47de-a000-0defd7fba24b": {"doc_hash": "5e7a2f1cb1f221394e61cc00e2cf22292abb222fdcc2e68d650824c205c4aaa9", "ref_doc_id": "23870692-93e9-4897-88f1-43397233b337"}, "1da52d1b-d143-4d2b-90a3-a1d86f687181": {"doc_hash": "b8e981c9fe9d5fbbeabd10f4e655bfe649732c0fbc76ba7fab15e0e8c4277efa", "ref_doc_id": "23870692-93e9-4897-88f1-43397233b337"}, "1654acd9-de14-4d37-957b-4c5aae69b2cb": {"doc_hash": "3870f41bae4bce848b3b569097c5c6e71422a4dfb89b8d19a3ac7fb1863b662f", "ref_doc_id": "23870692-93e9-4897-88f1-43397233b337"}, "ca32de04-7f44-4fa7-8588-65fb97e84e95": {"doc_hash": "3ad43a8ed64cd5d22c785eb2201056a651e1fa0853d060d77061b35255dffe30", "ref_doc_id": "23870692-93e9-4897-88f1-43397233b337"}, "23870692-93e9-4897-88f1-43397233b337": {"doc_hash": "cb3970fdd3f6e5a6bafc1ee90a6b646ca81473c858810eac70238e830e0559f8"}, "6aae3b6c-3dc7-4cb8-9129-97840dfeb038": {"doc_hash": "0756eef8a5d1fae509f371935254627b6e015c25f2a4d40ffffa6df95385b3ac", "ref_doc_id": "5b60632f-2c61-4d3c-9c95-65eb8fb0d6b0"}, "a016e878-2178-44c7-aa02-30bc4ce2865a": {"doc_hash": "d36bccdf6f29932709d615ed354c0d7e985679d7683f061a369102d60f1eba1f", "ref_doc_id": "5b60632f-2c61-4d3c-9c95-65eb8fb0d6b0"}, "3ad27dcc-228e-44bc-8b42-e3fe0a0a7244": {"doc_hash": "c33775e793c9cee0836fa374a85cee42174bc57d4e359b98cd26a60aac046503", "ref_doc_id": "5b60632f-2c61-4d3c-9c95-65eb8fb0d6b0"}, "9447632b-a8ce-4682-add9-87728802e4cf": {"doc_hash": "e7862015f2d99bd19cf7a0837638bdaa050d86f34ffce01d0206bcc5bd371728", "ref_doc_id": "5b60632f-2c61-4d3c-9c95-65eb8fb0d6b0"}, "a2bc4904-34da-4966-95ef-cfd943627715": {"doc_hash": "b971c7b2906e055a410d03c4f6ce68e3b0f3f6091be3524c6217a3859dfdede9", "ref_doc_id": "5b60632f-2c61-4d3c-9c95-65eb8fb0d6b0"}, "5b60632f-2c61-4d3c-9c95-65eb8fb0d6b0": {"doc_hash": "2254ba4ee2e3b1ce33301f727e7b0d8130cace5d57d72f40c4786b59dee67414"}}, "docstore/data": {"766a02fc-7e51-4158-976f-d61779c530f0": {"__data__": {"id_": "766a02fc-7e51-4158-976f-d61779c530f0", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a73d618-a0fb-47fa-a08f-2e07785d5d8f", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "hash": "e10c1517837a7616e0dbf4579ed6076df6b40e5e7ae9138636159312d2198d74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3002568c-7732-4f1b-8307-bab5cdf97c06", "node_type": "1", "metadata": {}, "hash": "ecaca2eb3147cde1d62b5d7ff2f7fb9aafae316ead8b648b7cf3726ad123ecb6", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Welcome to our introductory course on machine learning here at UC Irvine.\n00:00:10 My name is Professor Alexander Eiler, and I created these lectures for our introductory courses for undergraduates CS178 and graduates CS273.\n00:00:20 Machine learning is generally considered to be a subfield of artificial intelligence.\n00:00:30 As a general field, artificial intelligence is focused on building so-called intelligent agents, which are often typified by various tasks, including games like chess, now a classic man versus machine kind of scenario, autonomous behavior, such as autonomously driving vehicles, and multi-agent dynamics, such as robotic soccer competitions.\n00:00:40 Machine learning is both more specific and more broadly applied than typical artificial intelligence.\n00:00:50 Machine learning is focused on making predictions or decisions\n00:01:00 and specifically on those getting better with experience.\n00:01:04 It is fundamentally a data analysis science and combines parts of computer science and computational thinking with traditional areas of mathematics like statistics and optimization.\n00:01:09 This is intended as an introductory class with an emphasis on the practical.\n00:01:13 Some theories included, of course, but mainly to understand the principles of what works and what doesn't and how to make it work better.\n00:01:18 Machine learning is often typified by problems whose solutions are hard to describe explicitly.\n00:01:23 For instance, consider face detection by a camera autofocus.\n00:01:27 What makes a face?\n00:01:32 How can we describe to a program that this patch of the image contains a face while this patch doesn't?\n00:01:36 It's hard to describe that in a set of rules.\n00:01:41 Similarly, Netflix predicting how much you'll like a movie.\n00:01:46 What kinds of movies do you like?\n00:01:50 What kind of movie is this?\n00:01:55 Again, very hard to write down in an explicit set of.\n00:01:59 Instead, we'll need the computer to learn through examples.\n00:02:06 Both these tasks are examples of so-called supervised learning, in which we're given training data with the correct answers already tagged.\n00:02:13 We can then design a program to try to reproduce these correct answers.\n00:02:19 In general, these problems come in two flavors.\n00:02:26 Classification problems, in which the thing we must predict is a discrete value, for instance, deciding that a square contains a face or does not, a binary decision, or whether an email is a spam email or not.\n00:02:33 In contrast, regression problems, we're predicting a real value number.\n00:02:39 For example, Netflix may guess that you will rate a movie 3.8 stars.\n00:02:46 Note that the discreteness is in the prediction.\n00:02:53 In Netflix, for example, you can only rate a movie 3 or 4 stars, but because Netflix can predict a real value number like 3.8, it would be called a regression.\n00:02:59 Another type of problem, unsupervised learning, refers to problems in which there's no specific signal to predict.\n00:03:08 Instead, we simply want to understand the data, the structure, notions of similarity, how they relate to one another.\n00:03:17 Often the term data mining refers to this kind of data exploration or data understanding problem.\n00:03:25 On the left is one such unsupervised framework, this time applied to the Netflix data again, where now the data have been used to understand the notion of similarity between movies.\n00:03:34 We can use the ratings to organize and group them by similarity, summarize them, or even improve some kind of prediction problem.\n00:03:42 On the right are images of a hand as it explores two degrees of freedom, opening and closing and rotating.\n00:03:51 The computer only observes image patches.\n00:04:00 consisting of hundreds of grayscale values.\n00:04:08 Using only these kinds of image patches, unsupervised learning techniques can organize the images automatically, such that, for example, closed hands are here on the bottom, open hands are organized onto the top, and left to right shows vertical to horizontal rotation.\n00:04:17 Note that in both cases, the computer doesn't so much understand the semantic meaning behind this, as it does understand the similarity between the data.", "start_char_idx": 0, "end_char_idx": 4247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3002568c-7732-4f1b-8307-bab5cdf97c06": {"__data__": {"id_": "3002568c-7732-4f1b-8307-bab5cdf97c06", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a73d618-a0fb-47fa-a08f-2e07785d5d8f", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "hash": "e10c1517837a7616e0dbf4579ed6076df6b40e5e7ae9138636159312d2198d74", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "766a02fc-7e51-4158-976f-d61779c530f0", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "hash": "ff277ceebdd15cbca6837d2fb2153cb733aedfc2477d8c785f869f2e30f9a303", "class_name": "RelatedNodeInfo"}}, "text": "00:03:34 We can use the ratings to organize and group them by similarity, summarize them, or even improve some kind of prediction problem.\n00:03:42 On the right are images of a hand as it explores two degrees of freedom, opening and closing and rotating.\n00:03:51 The computer only observes image patches.\n00:04:00 consisting of hundreds of grayscale values.\n00:04:08 Using only these kinds of image patches, unsupervised learning techniques can organize the images automatically, such that, for example, closed hands are here on the bottom, open hands are organized onto the top, and left to right shows vertical to horizontal rotation.\n00:04:17 Note that in both cases, the computer doesn't so much understand the semantic meaning behind this, as it does understand the similarity between the data.\n00:04:25 So this is an automatic organization, but the semantic meaning is then usually imposed by humans later.\n00:04:34 Many other variants of learning problems also exist.\n00:04:42 A few of note include semi-supervised learning, which is really a supervised learning problem in that there's a signal, a specific signal to predict, but not all of the examples have the correct answer already given.\n00:04:51 Typically, unsupervised-like methods can then be used to support and improve on the supervised decision.\n00:04:59 These kinds of problems are increasingly common since we now have a lot of data, but not very much time to hand label it.\n00:05:05 In medical data, for example, we might have a lot of patient information, but very few examples in which we know the actual outcome or the best action to take.\n00:05:11 On the web, on the other hand, we might have millions of photos, but we can't expect users to label or tag all of them.\n00:05:17 Users will only label a small subset.\n00:05:23 So we have a vast trove of unsupervised data to go along with a small collection of supervised data.\n00:05:29 Finally, reinforcement learning involves learning when there's only indirect feedback on quality.\n00:05:35 Rather than actually knowing the correct answer or what a human would have done, as in supervised learning, here we only get a relative quality score telling us whether we're doing better or worse.\n00:05:41 Moreover, this feedback might be delayed.\n00:05:47 It might reflect many actions or predictions in sequence.\n00:05:53 It might be noisy.\n00:05:59 just consider the game of poker.\n00:06:08 In poker, we take a sequence of actions, betting, raising, but at the end, we don't even find out the correct answer to those actions.\n00:06:16 We just find out whether we won or lost in the end.\n00:06:24 Much of robotics and other kinds of sequential action planning rely on reinforcement learning techniques for developing their behaviors.\n00:06:32 In summary, machine learning is a computational study of data, including many types of subproblems.\n00:06:40 In this course, we'll start with supervised learning, which are prediction problems in which we're given examples of what our function should output in a number of instances, taken in the form of training data with input features and labeled desired output values.", "start_char_idx": 3447, "end_char_idx": 6584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32c73033-a333-4db8-977f-b09051c4bdff": {"__data__": {"id_": "32c73033-a333-4db8-977f-b09051c4bdff", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c5e79d0-be54-4b95-b6b4-2da42b62a478", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "da4374e265f1fef0d256feb75845cd02a3190812087dc6dbfb7508dcf60f94a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a061ad5d-9101-4671-ba12-ae7fa7d15d7b", "node_type": "1", "metadata": {}, "hash": "6985bb40bbf0cabad4d14a887cd9b4575e4fe8c831fef7bed1ec2b49992f697d", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Next, we'll look at some simple data exploration and visualization techniques and use MATLAB software to do so.\n00:00:07 Since machine learning is fundamentally a data science, the first thing we're likely to want to do is to take a look at the data, understand it, get a feel for what might work.\n00:00:15 The type of data is often very important.\n00:00:22 Some data might be binary, for instance, labeling emails as spam or not, observing the gender of a user.\n00:00:30 Some data might be categorical, for instance, the state that the user resides in or age values binned into several groups.\n00:00:37 And some observed features are likely to be continuous or nearly so, like pixel intensities or prices.\n00:00:45 Another very important point is, are any of the data values missing?\n00:00:52 If we only have partial data, meaning some of the features that we're going to observe about these data items are...\n00:01:00 not observed in all cases, we'll likely have to do something to fill those missing entries in.\n00:01:05 We'll return to this point in later lectures.\n00:01:10 Finally, we'd often like to get an idea of the shape of the data.\n00:01:15 For multi-dimensional data, how are the dimensions, how are the features related?\n00:01:20 Are there data outliers?\n00:01:25 For instance, data points that look very different from the majority of observations.\n00:01:30 Are any of the features redundant or unnecessary?\n00:01:35 To do our visualization, we'll need some software.\n00:01:40 Here, we'll use MATLAB, a popular language for scientific computing.\n00:01:45 MATLAB was designed for linear algebra and operates directly on vectors and matrices.\n00:01:50 It's widely used in many areas of science, including machine learning, so there's lots of code available on the web, but it can be a bit inefficient since MATLAB is an interpreted language, not compiled.\n00:01:55 If you don't want to buy or use MATLAB,\n00:02:00 Octave is a free, nearly code compatible open source project mimicking MATLAB.\n00:02:06 Octave is very well established and stable, although the graphical user interface is a little bit rough.\n00:02:12 Another newer MATLAB replacement is called FreeMAT.\n00:02:18 I have less experience with this, but it's another possibility.\n00:02:24 There are a lot of other alternatives as well, of course.\n00:02:30 In statistics, the language R is very popular and free.\n00:02:36 Again, with lots of available statistics code.\n00:02:42 Python is growing in usage with a number of packages intended to replicate some of the most useful aspects of MATLAB and especially high adoption in computational biology and bioinformatics.\n00:02:48 And, of course, for fast performance, C++ is widely used.\n00:02:54 It's very hard to beat for efficiency, but it's often slow to write code and prototype in, so I don't really recommend it.\n00:03:00 it for classwork, only for production.\n00:03:04 Let's get into a little notation to get started.\n00:03:09 We'll assume that we have M data points, X, X1 through XM.\n00:03:13 I'll try to use parentheses superscripts to indicate data identification numbers.\n00:03:18 Each datum is then a vector of observation.\n00:03:23 So data point J consists of N dimensional measurements.\n00:03:27 These dimensions are often called features.\n00:03:32 It will be useful to stack these data points together so that data point 1 is the first row, data point 2 is the second row through data point N.\n00:03:36 All the features are aligned into something called the data matrix.\n00:03:41 Again, each row is a data point, each column is a feature.\n00:03:46 Note that data matrices are not standardized.\n00:03:50 Some text and code you'll find will use the opposite convention, the transpose of the matrix I'm using.\n00:03:55 So please...\n00:03:59 be careful.\n00:04:05 Here's some code just showing how to load, say, the Fisher-Iris dataset in MATLAB, renaming that matrix as X.", "start_char_idx": 0, "end_char_idx": 3916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a061ad5d-9101-4671-ba12-ae7fa7d15d7b": {"__data__": {"id_": "a061ad5d-9101-4671-ba12-ae7fa7d15d7b", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c5e79d0-be54-4b95-b6b4-2da42b62a478", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "da4374e265f1fef0d256feb75845cd02a3190812087dc6dbfb7508dcf60f94a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32c73033-a333-4db8-977f-b09051c4bdff", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "18585c990f801fb75fd1c3155c609ba1a080671b0bd9a03d86e447cc90149570", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25a3b692-f483-4101-83e4-1d758d859434", "node_type": "1", "metadata": {}, "hash": "4e573eae35e0f013178bb41d0007695c86920f55758f60f7b93ed36116022589", "class_name": "RelatedNodeInfo"}}, "text": "00:03:23 So data point J consists of N dimensional measurements.\n00:03:27 These dimensions are often called features.\n00:03:32 It will be useful to stack these data points together so that data point 1 is the first row, data point 2 is the second row through data point N.\n00:03:36 All the features are aligned into something called the data matrix.\n00:03:41 Again, each row is a data point, each column is a feature.\n00:03:46 Note that data matrices are not standardized.\n00:03:50 Some text and code you'll find will use the opposite convention, the transpose of the matrix I'm using.\n00:03:55 So please...\n00:03:59 be careful.\n00:04:05 Here's some code just showing how to load, say, the Fisher-Iris dataset in MATLAB, renaming that matrix as X.\n00:04:11 And if we look at the size of X, we'll see that there are 150 measurements, each of four dimensions.\n00:04:18 The very first thing we might want to do is look at some basic statistics of the data.\n00:04:24 The mean, for example, tells us the average value.\n00:04:30 Calling mean on the data matrix averages over the first dimension so that we get the average in each feature.\n00:04:36 Median similarly tells us the center of the data.\n00:04:42 Maximum and minimum will tell us the range of the data.\n00:04:48 And something like standard deviation will tell us how much variation, how much spread to expect in each feature.\n00:04:54 A histogram bins the data points into a collection of discrete locations and counts the data that fall into each bin.\n00:05:00 If there are k bins, it essentially summarizes the data with k counts, and the plot gives a picture and a sense of how the data are distributed, the shape and spread of the data, presence of any outliers, etc.\n00:05:08 It's a bit trivial, but useful to think about k and how it determines how much summarization actually occurs in the histogram.\n00:05:17 K can in some sense be thought of as a complexity of the histogram.\n00:05:25 For example, if k is extremely large and the data values are actually unique, then every data point will fall into its own bin, and the histogram would simply tell you the positions of all those data.\n00:05:34 It would memorize all the data positions, and it wouldn't really be of any use in summarizing or understanding the data.\n00:05:42 Similarly, if k is too small, the binning would be extremely coarse, and you wouldn't learn anything about the shape.\n00:05:51 The actually right value of k, the best value of k for a plot, depends on...\n00:05:59 a lot of things, including how much data you have, how those data are distributed, and what you're trying to learn about the shape of the distribution.\n00:06:08 A scatterplot is another easy way to visualize how two variables relate to each other in the data.\n00:06:17 Here, we plot each datum as a point, blue, with coordinates given by its first feature X1 and second feature X2.\n00:06:25 This can give you an idea of the spread of the data in those two features, the relationship between the two features, and also, again, the presence of any outliers in the data.\n00:06:34 Of course, in practice, we'll often have more than two features, and it's very difficult to plot such things, but one option is to just look at every possible pair and do a number of two-dimensional plots.\n00:06:42 This is a so-called pair plot, showing all possible feature pairs.\n00:06:51 So for example, this entry here...\n00:06:59 corresponds to plotting x1 versus x2.\n00:07:08 This entry is x1 versus x3, and so on.\n00:07:17 A collection of plots like this give you a quick idea of how features might be related to each other and what their distribution looks like.\n00:07:25 In supervised learning, our goal is to predict something, specifically the target variable.\n00:07:34 This might be a label for email, whether it's spam or not, a price to go along with something.", "start_char_idx": 3169, "end_char_idx": 7022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25a3b692-f483-4101-83e4-1d758d859434": {"__data__": {"id_": "25a3b692-f483-4101-83e4-1d758d859434", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c5e79d0-be54-4b95-b6b4-2da42b62a478", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "da4374e265f1fef0d256feb75845cd02a3190812087dc6dbfb7508dcf60f94a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a061ad5d-9101-4671-ba12-ae7fa7d15d7b", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "9b7400a3585928be0cbb40d301e8a94d800e1a39b168309a4f27cb0232501a38", "class_name": "RelatedNodeInfo"}}, "text": "00:06:34 Of course, in practice, we'll often have more than two features, and it's very difficult to plot such things, but one option is to just look at every possible pair and do a number of two-dimensional plots.\n00:06:42 This is a so-called pair plot, showing all possible feature pairs.\n00:06:51 So for example, this entry here...\n00:06:59 corresponds to plotting x1 versus x2.\n00:07:08 This entry is x1 versus x3, and so on.\n00:07:17 A collection of plots like this give you a quick idea of how features might be related to each other and what their distribution looks like.\n00:07:25 In supervised learning, our goal is to predict something, specifically the target variable.\n00:07:34 This might be a label for email, whether it's spam or not, a price to go along with something.\n00:07:42 In this iris data set that we loaded, it's the species of the iris flower that's having its properties measured, one of three different types.\n00:07:51 So while we could treat this target like any other feature that's being measured, since part of our goal is to understand how the other measurements are related to and can help us predict this label, so for a discrete labeling problem, a classification problem.\n00:07:59 we can use color to understand how the histograms differ between the different classes or where the data from each class are located on that scatter plot.\n00:08:16 This gives us a sense of how easy it's going to be to predict the species, the target variable, from the other features.\n00:08:32 This lecture introduced some basic concepts in data exploration, the representation in terms of the data matrix, computing some basic statistics of the data, and visualizing the data using histograms, scatter plots, pair plots for higher dimensions, and using color for classification and discrete targets.", "start_char_idx": 6238, "end_char_idx": 8055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7d4f842-9ceb-41ec-9eaa-3df2fc47b14e": {"__data__": {"id_": "b7d4f842-9ceb-41ec-9eaa-3df2fc47b14e", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "221b314e-7748-4d7f-a195-4bec511e846c", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "d869abb3377f9a983345fd5d58e332b7bc33676f4e3d6b98057ddda2713de17e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72c351e9-ce3b-450f-b1a2-97ebfc0c207b", "node_type": "1", "metadata": {}, "hash": "a369fc95dd4456f2b51220f5b205b028a3056207a5d27ffa9ad747fa153cf32c", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 We'll begin with supervised learning problems, including classification and regression problems.\n00:00:07 As I mentioned, machine learning may be unlike much of the programming that you might have done in the past.\n00:00:15 Here, we can't simply tell the computer how to solve the problem, give it a precise algorithm to follow.\n00:00:22 Instead, we'll tell it how to figure out how to solve the problem, a kind of metaprogramming task.\n00:00:30 To do so, we design a flexible program called the learner, whose behavior can be modified by changing aspects of its operation, called the parameters.\n00:00:37 The learner is a deterministic function that takes in the features of a new example and spits out a prediction for its target.\n00:00:45 This input-output behavior is a function of these internal parameters.\n00:00:52 We can then write a second program, the learning algorithm, which sets or modifies the parameters of the learner.\n00:01:00 until it's doing a good job at predicting the training points that we've seen in the past, as measured by some score or cost function.\n00:01:05 Some notation is useful.\n00:01:10 The input features X will constitute all the information we have to make our prediction.\n00:01:15 For example, if I want to predict whether you'll need to go to the hospital in the next year, I might use all the information I have about you, your age, your weight, height, the outcomes of recent medical tests, etc.\n00:01:20 These are all available to me beforehand.\n00:01:25 I input them into my learner, and it outputs a prediction, yes or no, whether you'll be hospitalized.\n00:01:30 Then later, I'll find out the answer.\n00:01:35 Did you actually go to the hospital?\n00:01:40 That's the true target value, Y.\n00:01:45 I call my prediction Y hat, the estimate of Y, and by comparing Y and Y hat, I can use it to determine my score.\n00:01:50 I pay some penalty or cost if I'm wrong.\n00:01:55 We'll denote the generic parameter\n00:02:00 of our learner by theta.\n00:02:06 Since data play a fundamental role in machine learning, we'll need some way of visualizing the training data along with the results of our learning algorithm.\n00:02:13 Our training data consists of pairs of features and their associated target value.\n00:02:20 So you can imagine plotting those data.\n00:02:26 Since we can never really plot more than two dimensions, when we plot, we'll pretend that we have only one real valued x, and then the purpose of our learner is to map the feature value x into this target value y.\n00:02:33 In other words, given a new feature, x new, we're supposed to predict what the y value associated with that feature would be.\n00:02:39 Hopefully something that's reasonably similar to the training data.\n00:02:46 Since it outputs a value for any x, our learner, whatever it is, defines a function from x to y.\n00:02:53 And by evaluating every possible x,\n00:02:59 We can trace out that function to see what our learner thinks is the actual relationship between X and Y.\n00:03:07 In some cases, the shape or functional form of this function may be explicitly stated by the model, or it might be implicitly defined by the prediction program.\n00:03:14 Let's see some simple examples of this.\n00:03:22 An extremely simple predictor that we'll discuss in more detail soon is the nearest neighbor predictor.\n00:03:29 Nearest neighbor is defined in a very simple rule.\n00:03:37 Store the data, the red points in the scatter plot, in a database, and when asked to predict a new point X new, just find the most similar point in the database, looking only at the feature values, and predict whatever that closest point's value Y is.\n00:03:44 So this is basically a memorize and regurgitate kind of procedure.\n00:03:52 If we follow that procedure for every possible X, we'll trace out a set of predictions.\n00:03:59 implicitly defines a function mapping X to Y.\n00:04:08 You can notice the properties of this function.", "start_char_idx": 0, "end_char_idx": 3942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72c351e9-ce3b-450f-b1a2-97ebfc0c207b": {"__data__": {"id_": "72c351e9-ce3b-450f-b1a2-97ebfc0c207b", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "221b314e-7748-4d7f-a195-4bec511e846c", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "d869abb3377f9a983345fd5d58e332b7bc33676f4e3d6b98057ddda2713de17e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7d4f842-9ceb-41ec-9eaa-3df2fc47b14e", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "03d5c35c7d9bb6d2a3db46edc0171ce3b4952d562d4a9c2c5ef3d6d00ab0a121", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd80c443-2757-4b89-8b1b-520870a1da44", "node_type": "1", "metadata": {}, "hash": "6c4a55eca7b39a88e474eb6d83efa13a501467f8e36034397081b17f50170a76", "class_name": "RelatedNodeInfo"}}, "text": "00:03:14 Let's see some simple examples of this.\n00:03:22 An extremely simple predictor that we'll discuss in more detail soon is the nearest neighbor predictor.\n00:03:29 Nearest neighbor is defined in a very simple rule.\n00:03:37 Store the data, the red points in the scatter plot, in a database, and when asked to predict a new point X new, just find the most similar point in the database, looking only at the feature values, and predict whatever that closest point's value Y is.\n00:03:44 So this is basically a memorize and regurgitate kind of procedure.\n00:03:52 If we follow that procedure for every possible X, we'll trace out a set of predictions.\n00:03:59 implicitly defines a function mapping X to Y.\n00:04:08 You can notice the properties of this function.\n00:04:17 It's flat, constant, near any training example, since that example is the nearest point, and it continues to predict that Y, and then it abruptly changes the moment that X becomes closer to the next training example, so halfway between the two points.\n00:04:25 In contrast, here's another predictor we'll spend more time on, the linear predictor.\n00:04:34 Here, the predictor evaluates a linear function of the feature, computing some value, say theta zero, and adding theta one times X, and then outputting that prediction.\n00:04:42 Tracing out the values of this prediction on various values of X exactly shows this equation.\n00:04:51 The functional form of f of X is explicitly defined within this procedure.\n00:04:59 It's then quite easy to see what the effect of changing the parameters theta is.\n00:05:07 It changes f of x within some parametric family of possible functions such as lines here.\n00:05:14 The goal of the learning algorithm is then to modify these parameters until it finds a good function f of x within that family.\n00:05:22 Obviously our predictions may not perfectly match the data points.\n00:05:29 For any data point i, we can measure the difference between the observed target value yi and our prediction y-hat-i, and we just measure this difference and we can call it the error residual.\n00:05:37 For an accurate predictor, these residuals should be small.\n00:05:44 A common way to measure the total amount of error is the mean squared error, which just averages the square of the error residuals on the data.\n00:05:52 So the squaring makes them all positive, averaging tells me overall\n00:05:59 how I'm doing.\n00:06:04 Again, since we're restricted to looking at 2D plots when we actually visualize things, we'll only be able to draw very simple examples for visualization.\n00:06:09 For regression, we're essentially forced to look at one feature, x, versus the real value target, y.\n00:06:14 Even though in practice, of course, we'll probably use many more than one feature.\n00:06:19 For classification, however, we're predicting a discrete-valued y, often, say, a binary one, y equals 0 versus y equals 1.\n00:06:24 Spam or not spam, say.\n00:06:29 We can similarly plot x and y, just like we did in regression.\n00:06:34 But since y is discrete, this makes for a pretty boring plot.\n00:06:39 Instead, we can just use colors or symbols on the data points to indicate their y value.\n00:06:44 And then we only need to plot x.\n00:06:49 We don't actually need to plot y at all.\n00:06:54 So this means we can get away with plotting 2D.\n00:06:59 features instead of just one.\n00:07:08 We can plot a scatter plot of feature one versus feature two, and we indicate the class, the value of y, using a symbol or a color, here again, blue or red.\n00:07:17 Again, any learner should learn a map, a way to map locations, x1, x2, into a prediction.\n00:07:25 Now a discrete class, say 0 or 1, if we do that for every possible x1, x2 place on the plane, we'll now get a three-dimensional function whose value we can indicate with color.", "start_char_idx": 3175, "end_char_idx": 6999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd80c443-2757-4b89-8b1b-520870a1da44": {"__data__": {"id_": "fd80c443-2757-4b89-8b1b-520870a1da44", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "221b314e-7748-4d7f-a195-4bec511e846c", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "d869abb3377f9a983345fd5d58e332b7bc33676f4e3d6b98057ddda2713de17e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72c351e9-ce3b-450f-b1a2-97ebfc0c207b", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "1dd413183b7b02f4f82388e18ccbcdcd4dce0532cfd24caed0ff5a3dab9c2bfa", "class_name": "RelatedNodeInfo"}}, "text": "00:06:44 And then we only need to plot x.\n00:06:49 We don't actually need to plot y at all.\n00:06:54 So this means we can get away with plotting 2D.\n00:06:59 features instead of just one.\n00:07:08 We can plot a scatter plot of feature one versus feature two, and we indicate the class, the value of y, using a symbol or a color, here again, blue or red.\n00:07:17 Again, any learner should learn a map, a way to map locations, x1, x2, into a prediction.\n00:07:25 Now a discrete class, say 0 or 1, if we do that for every possible x1, x2 place on the plane, we'll now get a three-dimensional function whose value we can indicate with color.\n00:07:34 Where now red shows all the points where we predict class 1, and blue, all those where we decide the other class, say minus 1.\n00:07:42 An important concept for classification is that the function transitions immediately from one value to the other.\n00:07:51 And this means that there's a set of points at which the function changes.\n00:07:59 abruptly from one value to the other.\n00:08:07 This is called the decision boundary.\n00:08:14 In some sense, the learner, the classifier, can be characterized completely by the decision boundaries it's able to represent.\n00:08:22 Note again that this is a function, y hat, of two variables, x1 and x2, whose value is visualized with color.\n00:08:29 Again, in classification, it may be that our predictions don't necessarily match all of the observed data.\n00:08:37 For classification problems, the most common method of assessing accuracy is called the error rate, which is the probability of making an incorrect prediction.\n00:08:44 The empirical error rate is just given by the fraction of data points, i, at which our predictor, y hat, makes an incorrect prediction, i.e., a prediction that differs from the observed value, y.\n00:08:52 So here, we have a blue point where we're predicting red, a red point where we're.\n00:08:59 we're predicting blue, so we're making two out of 10 errors.\n00:09:12 In summary, we started with supervised learning, which are prediction problems in which we're given examples of what our function should output in the form of training data with input features X and a labeled desired output value Y.\n00:09:25 We saw two examples of these types of problems, regression problems, where we predict real valued numbers, and we'll visualize these as a function mapping a single feature X to its predicted Y value.\n00:09:38 Classification problems, on the other hand, predict discrete valued targets, and so I'll usually visualize them using two input features, X1 and X2, and drawing the output Y values using colors or symbols, or the decision boundary transitioning from one prediction value to another.", "start_char_idx": 6361, "end_char_idx": 9086, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2902509e-8b06-48fe-beb9-1c5670be2ee8": {"__data__": {"id_": "2902509e-8b06-48fe-beb9-1c5670be2ee8", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe380da3-000a-4742-9443-b5eaa35e5042", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "hash": "e10c1517837a7616e0dbf4579ed6076df6b40e5e7ae9138636159312d2198d74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d42ebddf-5646-4be6-9f84-fbd85492d3cd", "node_type": "1", "metadata": {}, "hash": "ecaca2eb3147cde1d62b5d7ff2f7fb9aafae316ead8b648b7cf3726ad123ecb6", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Welcome to our introductory course on machine learning here at UC Irvine.\n00:00:10 My name is Professor Alexander Eiler, and I created these lectures for our introductory courses for undergraduates CS178 and graduates CS273.\n00:00:20 Machine learning is generally considered to be a subfield of artificial intelligence.\n00:00:30 As a general field, artificial intelligence is focused on building so-called intelligent agents, which are often typified by various tasks, including games like chess, now a classic man versus machine kind of scenario, autonomous behavior, such as autonomously driving vehicles, and multi-agent dynamics, such as robotic soccer competitions.\n00:00:40 Machine learning is both more specific and more broadly applied than typical artificial intelligence.\n00:00:50 Machine learning is focused on making predictions or decisions\n00:01:00 and specifically on those getting better with experience.\n00:01:04 It is fundamentally a data analysis science and combines parts of computer science and computational thinking with traditional areas of mathematics like statistics and optimization.\n00:01:09 This is intended as an introductory class with an emphasis on the practical.\n00:01:13 Some theories included, of course, but mainly to understand the principles of what works and what doesn't and how to make it work better.\n00:01:18 Machine learning is often typified by problems whose solutions are hard to describe explicitly.\n00:01:23 For instance, consider face detection by a camera autofocus.\n00:01:27 What makes a face?\n00:01:32 How can we describe to a program that this patch of the image contains a face while this patch doesn't?\n00:01:36 It's hard to describe that in a set of rules.\n00:01:41 Similarly, Netflix predicting how much you'll like a movie.\n00:01:46 What kinds of movies do you like?\n00:01:50 What kind of movie is this?\n00:01:55 Again, very hard to write down in an explicit set of.\n00:01:59 Instead, we'll need the computer to learn through examples.\n00:02:06 Both these tasks are examples of so-called supervised learning, in which we're given training data with the correct answers already tagged.\n00:02:13 We can then design a program to try to reproduce these correct answers.\n00:02:19 In general, these problems come in two flavors.\n00:02:26 Classification problems, in which the thing we must predict is a discrete value, for instance, deciding that a square contains a face or does not, a binary decision, or whether an email is a spam email or not.\n00:02:33 In contrast, regression problems, we're predicting a real value number.\n00:02:39 For example, Netflix may guess that you will rate a movie 3.8 stars.\n00:02:46 Note that the discreteness is in the prediction.\n00:02:53 In Netflix, for example, you can only rate a movie 3 or 4 stars, but because Netflix can predict a real value number like 3.8, it would be called a regression.\n00:02:59 Another type of problem, unsupervised learning, refers to problems in which there's no specific signal to predict.\n00:03:08 Instead, we simply want to understand the data, the structure, notions of similarity, how they relate to one another.\n00:03:17 Often the term data mining refers to this kind of data exploration or data understanding problem.\n00:03:25 On the left is one such unsupervised framework, this time applied to the Netflix data again, where now the data have been used to understand the notion of similarity between movies.\n00:03:34 We can use the ratings to organize and group them by similarity, summarize them, or even improve some kind of prediction problem.\n00:03:42 On the right are images of a hand as it explores two degrees of freedom, opening and closing and rotating.\n00:03:51 The computer only observes image patches.\n00:04:00 consisting of hundreds of grayscale values.\n00:04:08 Using only these kinds of image patches, unsupervised learning techniques can organize the images automatically, such that, for example, closed hands are here on the bottom, open hands are organized onto the top, and left to right shows vertical to horizontal rotation.\n00:04:17 Note that in both cases, the computer doesn't so much understand the semantic meaning behind this, as it does understand the similarity between the data.", "start_char_idx": 0, "end_char_idx": 4247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d42ebddf-5646-4be6-9f84-fbd85492d3cd": {"__data__": {"id_": "d42ebddf-5646-4be6-9f84-fbd85492d3cd", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe380da3-000a-4742-9443-b5eaa35e5042", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "hash": "e10c1517837a7616e0dbf4579ed6076df6b40e5e7ae9138636159312d2198d74", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2902509e-8b06-48fe-beb9-1c5670be2ee8", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "hash": "ff277ceebdd15cbca6837d2fb2153cb733aedfc2477d8c785f869f2e30f9a303", "class_name": "RelatedNodeInfo"}}, "text": "00:03:34 We can use the ratings to organize and group them by similarity, summarize them, or even improve some kind of prediction problem.\n00:03:42 On the right are images of a hand as it explores two degrees of freedom, opening and closing and rotating.\n00:03:51 The computer only observes image patches.\n00:04:00 consisting of hundreds of grayscale values.\n00:04:08 Using only these kinds of image patches, unsupervised learning techniques can organize the images automatically, such that, for example, closed hands are here on the bottom, open hands are organized onto the top, and left to right shows vertical to horizontal rotation.\n00:04:17 Note that in both cases, the computer doesn't so much understand the semantic meaning behind this, as it does understand the similarity between the data.\n00:04:25 So this is an automatic organization, but the semantic meaning is then usually imposed by humans later.\n00:04:34 Many other variants of learning problems also exist.\n00:04:42 A few of note include semi-supervised learning, which is really a supervised learning problem in that there's a signal, a specific signal to predict, but not all of the examples have the correct answer already given.\n00:04:51 Typically, unsupervised-like methods can then be used to support and improve on the supervised decision.\n00:04:59 These kinds of problems are increasingly common since we now have a lot of data, but not very much time to hand label it.\n00:05:05 In medical data, for example, we might have a lot of patient information, but very few examples in which we know the actual outcome or the best action to take.\n00:05:11 On the web, on the other hand, we might have millions of photos, but we can't expect users to label or tag all of them.\n00:05:17 Users will only label a small subset.\n00:05:23 So we have a vast trove of unsupervised data to go along with a small collection of supervised data.\n00:05:29 Finally, reinforcement learning involves learning when there's only indirect feedback on quality.\n00:05:35 Rather than actually knowing the correct answer or what a human would have done, as in supervised learning, here we only get a relative quality score telling us whether we're doing better or worse.\n00:05:41 Moreover, this feedback might be delayed.\n00:05:47 It might reflect many actions or predictions in sequence.\n00:05:53 It might be noisy.\n00:05:59 just consider the game of poker.\n00:06:08 In poker, we take a sequence of actions, betting, raising, but at the end, we don't even find out the correct answer to those actions.\n00:06:16 We just find out whether we won or lost in the end.\n00:06:24 Much of robotics and other kinds of sequential action planning rely on reinforcement learning techniques for developing their behaviors.\n00:06:32 In summary, machine learning is a computational study of data, including many types of subproblems.\n00:06:40 In this course, we'll start with supervised learning, which are prediction problems in which we're given examples of what our function should output in a number of instances, taken in the form of training data with input features and labeled desired output values.", "start_char_idx": 3447, "end_char_idx": 6584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d829baee-5699-4398-bb50-885e65afa561": {"__data__": {"id_": "d829baee-5699-4398-bb50-885e65afa561", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "503fe2e6-0597-4404-b5b3-103cb2632808", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "da4374e265f1fef0d256feb75845cd02a3190812087dc6dbfb7508dcf60f94a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93057f03-ade9-4a36-9ea6-7df0a4d1106d", "node_type": "1", "metadata": {}, "hash": "6985bb40bbf0cabad4d14a887cd9b4575e4fe8c831fef7bed1ec2b49992f697d", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Next, we'll look at some simple data exploration and visualization techniques and use MATLAB software to do so.\n00:00:07 Since machine learning is fundamentally a data science, the first thing we're likely to want to do is to take a look at the data, understand it, get a feel for what might work.\n00:00:15 The type of data is often very important.\n00:00:22 Some data might be binary, for instance, labeling emails as spam or not, observing the gender of a user.\n00:00:30 Some data might be categorical, for instance, the state that the user resides in or age values binned into several groups.\n00:00:37 And some observed features are likely to be continuous or nearly so, like pixel intensities or prices.\n00:00:45 Another very important point is, are any of the data values missing?\n00:00:52 If we only have partial data, meaning some of the features that we're going to observe about these data items are...\n00:01:00 not observed in all cases, we'll likely have to do something to fill those missing entries in.\n00:01:05 We'll return to this point in later lectures.\n00:01:10 Finally, we'd often like to get an idea of the shape of the data.\n00:01:15 For multi-dimensional data, how are the dimensions, how are the features related?\n00:01:20 Are there data outliers?\n00:01:25 For instance, data points that look very different from the majority of observations.\n00:01:30 Are any of the features redundant or unnecessary?\n00:01:35 To do our visualization, we'll need some software.\n00:01:40 Here, we'll use MATLAB, a popular language for scientific computing.\n00:01:45 MATLAB was designed for linear algebra and operates directly on vectors and matrices.\n00:01:50 It's widely used in many areas of science, including machine learning, so there's lots of code available on the web, but it can be a bit inefficient since MATLAB is an interpreted language, not compiled.\n00:01:55 If you don't want to buy or use MATLAB,\n00:02:00 Octave is a free, nearly code compatible open source project mimicking MATLAB.\n00:02:06 Octave is very well established and stable, although the graphical user interface is a little bit rough.\n00:02:12 Another newer MATLAB replacement is called FreeMAT.\n00:02:18 I have less experience with this, but it's another possibility.\n00:02:24 There are a lot of other alternatives as well, of course.\n00:02:30 In statistics, the language R is very popular and free.\n00:02:36 Again, with lots of available statistics code.\n00:02:42 Python is growing in usage with a number of packages intended to replicate some of the most useful aspects of MATLAB and especially high adoption in computational biology and bioinformatics.\n00:02:48 And, of course, for fast performance, C++ is widely used.\n00:02:54 It's very hard to beat for efficiency, but it's often slow to write code and prototype in, so I don't really recommend it.\n00:03:00 it for classwork, only for production.\n00:03:04 Let's get into a little notation to get started.\n00:03:09 We'll assume that we have M data points, X, X1 through XM.\n00:03:13 I'll try to use parentheses superscripts to indicate data identification numbers.\n00:03:18 Each datum is then a vector of observation.\n00:03:23 So data point J consists of N dimensional measurements.\n00:03:27 These dimensions are often called features.\n00:03:32 It will be useful to stack these data points together so that data point 1 is the first row, data point 2 is the second row through data point N.\n00:03:36 All the features are aligned into something called the data matrix.\n00:03:41 Again, each row is a data point, each column is a feature.\n00:03:46 Note that data matrices are not standardized.\n00:03:50 Some text and code you'll find will use the opposite convention, the transpose of the matrix I'm using.\n00:03:55 So please...\n00:03:59 be careful.\n00:04:05 Here's some code just showing how to load, say, the Fisher-Iris dataset in MATLAB, renaming that matrix as X.", "start_char_idx": 0, "end_char_idx": 3916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93057f03-ade9-4a36-9ea6-7df0a4d1106d": {"__data__": {"id_": "93057f03-ade9-4a36-9ea6-7df0a4d1106d", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "503fe2e6-0597-4404-b5b3-103cb2632808", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "da4374e265f1fef0d256feb75845cd02a3190812087dc6dbfb7508dcf60f94a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d829baee-5699-4398-bb50-885e65afa561", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "18585c990f801fb75fd1c3155c609ba1a080671b0bd9a03d86e447cc90149570", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10370ebc-6551-4687-b1a9-4b18cc2af6c8", "node_type": "1", "metadata": {}, "hash": "4e573eae35e0f013178bb41d0007695c86920f55758f60f7b93ed36116022589", "class_name": "RelatedNodeInfo"}}, "text": "00:03:23 So data point J consists of N dimensional measurements.\n00:03:27 These dimensions are often called features.\n00:03:32 It will be useful to stack these data points together so that data point 1 is the first row, data point 2 is the second row through data point N.\n00:03:36 All the features are aligned into something called the data matrix.\n00:03:41 Again, each row is a data point, each column is a feature.\n00:03:46 Note that data matrices are not standardized.\n00:03:50 Some text and code you'll find will use the opposite convention, the transpose of the matrix I'm using.\n00:03:55 So please...\n00:03:59 be careful.\n00:04:05 Here's some code just showing how to load, say, the Fisher-Iris dataset in MATLAB, renaming that matrix as X.\n00:04:11 And if we look at the size of X, we'll see that there are 150 measurements, each of four dimensions.\n00:04:18 The very first thing we might want to do is look at some basic statistics of the data.\n00:04:24 The mean, for example, tells us the average value.\n00:04:30 Calling mean on the data matrix averages over the first dimension so that we get the average in each feature.\n00:04:36 Median similarly tells us the center of the data.\n00:04:42 Maximum and minimum will tell us the range of the data.\n00:04:48 And something like standard deviation will tell us how much variation, how much spread to expect in each feature.\n00:04:54 A histogram bins the data points into a collection of discrete locations and counts the data that fall into each bin.\n00:05:00 If there are k bins, it essentially summarizes the data with k counts, and the plot gives a picture and a sense of how the data are distributed, the shape and spread of the data, presence of any outliers, etc.\n00:05:08 It's a bit trivial, but useful to think about k and how it determines how much summarization actually occurs in the histogram.\n00:05:17 K can in some sense be thought of as a complexity of the histogram.\n00:05:25 For example, if k is extremely large and the data values are actually unique, then every data point will fall into its own bin, and the histogram would simply tell you the positions of all those data.\n00:05:34 It would memorize all the data positions, and it wouldn't really be of any use in summarizing or understanding the data.\n00:05:42 Similarly, if k is too small, the binning would be extremely coarse, and you wouldn't learn anything about the shape.\n00:05:51 The actually right value of k, the best value of k for a plot, depends on...\n00:05:59 a lot of things, including how much data you have, how those data are distributed, and what you're trying to learn about the shape of the distribution.\n00:06:08 A scatterplot is another easy way to visualize how two variables relate to each other in the data.\n00:06:17 Here, we plot each datum as a point, blue, with coordinates given by its first feature X1 and second feature X2.\n00:06:25 This can give you an idea of the spread of the data in those two features, the relationship between the two features, and also, again, the presence of any outliers in the data.\n00:06:34 Of course, in practice, we'll often have more than two features, and it's very difficult to plot such things, but one option is to just look at every possible pair and do a number of two-dimensional plots.\n00:06:42 This is a so-called pair plot, showing all possible feature pairs.\n00:06:51 So for example, this entry here...\n00:06:59 corresponds to plotting x1 versus x2.\n00:07:08 This entry is x1 versus x3, and so on.\n00:07:17 A collection of plots like this give you a quick idea of how features might be related to each other and what their distribution looks like.\n00:07:25 In supervised learning, our goal is to predict something, specifically the target variable.\n00:07:34 This might be a label for email, whether it's spam or not, a price to go along with something.", "start_char_idx": 3169, "end_char_idx": 7022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10370ebc-6551-4687-b1a9-4b18cc2af6c8": {"__data__": {"id_": "10370ebc-6551-4687-b1a9-4b18cc2af6c8", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "503fe2e6-0597-4404-b5b3-103cb2632808", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "da4374e265f1fef0d256feb75845cd02a3190812087dc6dbfb7508dcf60f94a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93057f03-ade9-4a36-9ea6-7df0a4d1106d", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "9b7400a3585928be0cbb40d301e8a94d800e1a39b168309a4f27cb0232501a38", "class_name": "RelatedNodeInfo"}}, "text": "00:06:34 Of course, in practice, we'll often have more than two features, and it's very difficult to plot such things, but one option is to just look at every possible pair and do a number of two-dimensional plots.\n00:06:42 This is a so-called pair plot, showing all possible feature pairs.\n00:06:51 So for example, this entry here...\n00:06:59 corresponds to plotting x1 versus x2.\n00:07:08 This entry is x1 versus x3, and so on.\n00:07:17 A collection of plots like this give you a quick idea of how features might be related to each other and what their distribution looks like.\n00:07:25 In supervised learning, our goal is to predict something, specifically the target variable.\n00:07:34 This might be a label for email, whether it's spam or not, a price to go along with something.\n00:07:42 In this iris data set that we loaded, it's the species of the iris flower that's having its properties measured, one of three different types.\n00:07:51 So while we could treat this target like any other feature that's being measured, since part of our goal is to understand how the other measurements are related to and can help us predict this label, so for a discrete labeling problem, a classification problem.\n00:07:59 we can use color to understand how the histograms differ between the different classes or where the data from each class are located on that scatter plot.\n00:08:16 This gives us a sense of how easy it's going to be to predict the species, the target variable, from the other features.\n00:08:32 This lecture introduced some basic concepts in data exploration, the representation in terms of the data matrix, computing some basic statistics of the data, and visualizing the data using histograms, scatter plots, pair plots for higher dimensions, and using color for classification and discrete targets.", "start_char_idx": 6238, "end_char_idx": 8055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c517231c-9e19-46b5-8bf2-85e8ae8a05c9": {"__data__": {"id_": "c517231c-9e19-46b5-8bf2-85e8ae8a05c9", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88aa12d8-ab91-4cb0-8e27-c2c81c318ca4", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "d869abb3377f9a983345fd5d58e332b7bc33676f4e3d6b98057ddda2713de17e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b422bef4-3051-4a00-97a9-414a2d12bd9f", "node_type": "1", "metadata": {}, "hash": "a369fc95dd4456f2b51220f5b205b028a3056207a5d27ffa9ad747fa153cf32c", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 We'll begin with supervised learning problems, including classification and regression problems.\n00:00:07 As I mentioned, machine learning may be unlike much of the programming that you might have done in the past.\n00:00:15 Here, we can't simply tell the computer how to solve the problem, give it a precise algorithm to follow.\n00:00:22 Instead, we'll tell it how to figure out how to solve the problem, a kind of metaprogramming task.\n00:00:30 To do so, we design a flexible program called the learner, whose behavior can be modified by changing aspects of its operation, called the parameters.\n00:00:37 The learner is a deterministic function that takes in the features of a new example and spits out a prediction for its target.\n00:00:45 This input-output behavior is a function of these internal parameters.\n00:00:52 We can then write a second program, the learning algorithm, which sets or modifies the parameters of the learner.\n00:01:00 until it's doing a good job at predicting the training points that we've seen in the past, as measured by some score or cost function.\n00:01:05 Some notation is useful.\n00:01:10 The input features X will constitute all the information we have to make our prediction.\n00:01:15 For example, if I want to predict whether you'll need to go to the hospital in the next year, I might use all the information I have about you, your age, your weight, height, the outcomes of recent medical tests, etc.\n00:01:20 These are all available to me beforehand.\n00:01:25 I input them into my learner, and it outputs a prediction, yes or no, whether you'll be hospitalized.\n00:01:30 Then later, I'll find out the answer.\n00:01:35 Did you actually go to the hospital?\n00:01:40 That's the true target value, Y.\n00:01:45 I call my prediction Y hat, the estimate of Y, and by comparing Y and Y hat, I can use it to determine my score.\n00:01:50 I pay some penalty or cost if I'm wrong.\n00:01:55 We'll denote the generic parameter\n00:02:00 of our learner by theta.\n00:02:06 Since data play a fundamental role in machine learning, we'll need some way of visualizing the training data along with the results of our learning algorithm.\n00:02:13 Our training data consists of pairs of features and their associated target value.\n00:02:20 So you can imagine plotting those data.\n00:02:26 Since we can never really plot more than two dimensions, when we plot, we'll pretend that we have only one real valued x, and then the purpose of our learner is to map the feature value x into this target value y.\n00:02:33 In other words, given a new feature, x new, we're supposed to predict what the y value associated with that feature would be.\n00:02:39 Hopefully something that's reasonably similar to the training data.\n00:02:46 Since it outputs a value for any x, our learner, whatever it is, defines a function from x to y.\n00:02:53 And by evaluating every possible x,\n00:02:59 We can trace out that function to see what our learner thinks is the actual relationship between X and Y.\n00:03:07 In some cases, the shape or functional form of this function may be explicitly stated by the model, or it might be implicitly defined by the prediction program.\n00:03:14 Let's see some simple examples of this.\n00:03:22 An extremely simple predictor that we'll discuss in more detail soon is the nearest neighbor predictor.\n00:03:29 Nearest neighbor is defined in a very simple rule.\n00:03:37 Store the data, the red points in the scatter plot, in a database, and when asked to predict a new point X new, just find the most similar point in the database, looking only at the feature values, and predict whatever that closest point's value Y is.\n00:03:44 So this is basically a memorize and regurgitate kind of procedure.\n00:03:52 If we follow that procedure for every possible X, we'll trace out a set of predictions.\n00:03:59 implicitly defines a function mapping X to Y.\n00:04:08 You can notice the properties of this function.", "start_char_idx": 0, "end_char_idx": 3942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b422bef4-3051-4a00-97a9-414a2d12bd9f": {"__data__": {"id_": "b422bef4-3051-4a00-97a9-414a2d12bd9f", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88aa12d8-ab91-4cb0-8e27-c2c81c318ca4", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "d869abb3377f9a983345fd5d58e332b7bc33676f4e3d6b98057ddda2713de17e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c517231c-9e19-46b5-8bf2-85e8ae8a05c9", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "03d5c35c7d9bb6d2a3db46edc0171ce3b4952d562d4a9c2c5ef3d6d00ab0a121", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9ff7407-55fa-47c6-8a43-11b28d350374", "node_type": "1", "metadata": {}, "hash": "6c4a55eca7b39a88e474eb6d83efa13a501467f8e36034397081b17f50170a76", "class_name": "RelatedNodeInfo"}}, "text": "00:03:14 Let's see some simple examples of this.\n00:03:22 An extremely simple predictor that we'll discuss in more detail soon is the nearest neighbor predictor.\n00:03:29 Nearest neighbor is defined in a very simple rule.\n00:03:37 Store the data, the red points in the scatter plot, in a database, and when asked to predict a new point X new, just find the most similar point in the database, looking only at the feature values, and predict whatever that closest point's value Y is.\n00:03:44 So this is basically a memorize and regurgitate kind of procedure.\n00:03:52 If we follow that procedure for every possible X, we'll trace out a set of predictions.\n00:03:59 implicitly defines a function mapping X to Y.\n00:04:08 You can notice the properties of this function.\n00:04:17 It's flat, constant, near any training example, since that example is the nearest point, and it continues to predict that Y, and then it abruptly changes the moment that X becomes closer to the next training example, so halfway between the two points.\n00:04:25 In contrast, here's another predictor we'll spend more time on, the linear predictor.\n00:04:34 Here, the predictor evaluates a linear function of the feature, computing some value, say theta zero, and adding theta one times X, and then outputting that prediction.\n00:04:42 Tracing out the values of this prediction on various values of X exactly shows this equation.\n00:04:51 The functional form of f of X is explicitly defined within this procedure.\n00:04:59 It's then quite easy to see what the effect of changing the parameters theta is.\n00:05:07 It changes f of x within some parametric family of possible functions such as lines here.\n00:05:14 The goal of the learning algorithm is then to modify these parameters until it finds a good function f of x within that family.\n00:05:22 Obviously our predictions may not perfectly match the data points.\n00:05:29 For any data point i, we can measure the difference between the observed target value yi and our prediction y-hat-i, and we just measure this difference and we can call it the error residual.\n00:05:37 For an accurate predictor, these residuals should be small.\n00:05:44 A common way to measure the total amount of error is the mean squared error, which just averages the square of the error residuals on the data.\n00:05:52 So the squaring makes them all positive, averaging tells me overall\n00:05:59 how I'm doing.\n00:06:04 Again, since we're restricted to looking at 2D plots when we actually visualize things, we'll only be able to draw very simple examples for visualization.\n00:06:09 For regression, we're essentially forced to look at one feature, x, versus the real value target, y.\n00:06:14 Even though in practice, of course, we'll probably use many more than one feature.\n00:06:19 For classification, however, we're predicting a discrete-valued y, often, say, a binary one, y equals 0 versus y equals 1.\n00:06:24 Spam or not spam, say.\n00:06:29 We can similarly plot x and y, just like we did in regression.\n00:06:34 But since y is discrete, this makes for a pretty boring plot.\n00:06:39 Instead, we can just use colors or symbols on the data points to indicate their y value.\n00:06:44 And then we only need to plot x.\n00:06:49 We don't actually need to plot y at all.\n00:06:54 So this means we can get away with plotting 2D.\n00:06:59 features instead of just one.\n00:07:08 We can plot a scatter plot of feature one versus feature two, and we indicate the class, the value of y, using a symbol or a color, here again, blue or red.\n00:07:17 Again, any learner should learn a map, a way to map locations, x1, x2, into a prediction.\n00:07:25 Now a discrete class, say 0 or 1, if we do that for every possible x1, x2 place on the plane, we'll now get a three-dimensional function whose value we can indicate with color.", "start_char_idx": 3175, "end_char_idx": 6999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9ff7407-55fa-47c6-8a43-11b28d350374": {"__data__": {"id_": "d9ff7407-55fa-47c6-8a43-11b28d350374", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88aa12d8-ab91-4cb0-8e27-c2c81c318ca4", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "d869abb3377f9a983345fd5d58e332b7bc33676f4e3d6b98057ddda2713de17e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b422bef4-3051-4a00-97a9-414a2d12bd9f", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "1dd413183b7b02f4f82388e18ccbcdcd4dce0532cfd24caed0ff5a3dab9c2bfa", "class_name": "RelatedNodeInfo"}}, "text": "00:06:44 And then we only need to plot x.\n00:06:49 We don't actually need to plot y at all.\n00:06:54 So this means we can get away with plotting 2D.\n00:06:59 features instead of just one.\n00:07:08 We can plot a scatter plot of feature one versus feature two, and we indicate the class, the value of y, using a symbol or a color, here again, blue or red.\n00:07:17 Again, any learner should learn a map, a way to map locations, x1, x2, into a prediction.\n00:07:25 Now a discrete class, say 0 or 1, if we do that for every possible x1, x2 place on the plane, we'll now get a three-dimensional function whose value we can indicate with color.\n00:07:34 Where now red shows all the points where we predict class 1, and blue, all those where we decide the other class, say minus 1.\n00:07:42 An important concept for classification is that the function transitions immediately from one value to the other.\n00:07:51 And this means that there's a set of points at which the function changes.\n00:07:59 abruptly from one value to the other.\n00:08:07 This is called the decision boundary.\n00:08:14 In some sense, the learner, the classifier, can be characterized completely by the decision boundaries it's able to represent.\n00:08:22 Note again that this is a function, y hat, of two variables, x1 and x2, whose value is visualized with color.\n00:08:29 Again, in classification, it may be that our predictions don't necessarily match all of the observed data.\n00:08:37 For classification problems, the most common method of assessing accuracy is called the error rate, which is the probability of making an incorrect prediction.\n00:08:44 The empirical error rate is just given by the fraction of data points, i, at which our predictor, y hat, makes an incorrect prediction, i.e., a prediction that differs from the observed value, y.\n00:08:52 So here, we have a blue point where we're predicting red, a red point where we're.\n00:08:59 we're predicting blue, so we're making two out of 10 errors.\n00:09:12 In summary, we started with supervised learning, which are prediction problems in which we're given examples of what our function should output in the form of training data with input features X and a labeled desired output value Y.\n00:09:25 We saw two examples of these types of problems, regression problems, where we predict real valued numbers, and we'll visualize these as a function mapping a single feature X to its predicted Y value.\n00:09:38 Classification problems, on the other hand, predict discrete valued targets, and so I'll usually visualize them using two input features, X1 and X2, and drawing the output Y values using colors or symbols, or the decision boundary transitioning from one prediction value to another.", "start_char_idx": 6361, "end_char_idx": 9086, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a03953c5-d372-4086-a721-98efd52726e7": {"__data__": {"id_": "a03953c5-d372-4086-a721-98efd52726e7", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c770b46-ce8a-4e25-87ba-fc85fb1ec708", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "hash": "e10c1517837a7616e0dbf4579ed6076df6b40e5e7ae9138636159312d2198d74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47382864-30b1-4572-9790-abb13713f7a0", "node_type": "1", "metadata": {}, "hash": "ecaca2eb3147cde1d62b5d7ff2f7fb9aafae316ead8b648b7cf3726ad123ecb6", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Welcome to our introductory course on machine learning here at UC Irvine.\n00:00:10 My name is Professor Alexander Eiler, and I created these lectures for our introductory courses for undergraduates CS178 and graduates CS273.\n00:00:20 Machine learning is generally considered to be a subfield of artificial intelligence.\n00:00:30 As a general field, artificial intelligence is focused on building so-called intelligent agents, which are often typified by various tasks, including games like chess, now a classic man versus machine kind of scenario, autonomous behavior, such as autonomously driving vehicles, and multi-agent dynamics, such as robotic soccer competitions.\n00:00:40 Machine learning is both more specific and more broadly applied than typical artificial intelligence.\n00:00:50 Machine learning is focused on making predictions or decisions\n00:01:00 and specifically on those getting better with experience.\n00:01:04 It is fundamentally a data analysis science and combines parts of computer science and computational thinking with traditional areas of mathematics like statistics and optimization.\n00:01:09 This is intended as an introductory class with an emphasis on the practical.\n00:01:13 Some theories included, of course, but mainly to understand the principles of what works and what doesn't and how to make it work better.\n00:01:18 Machine learning is often typified by problems whose solutions are hard to describe explicitly.\n00:01:23 For instance, consider face detection by a camera autofocus.\n00:01:27 What makes a face?\n00:01:32 How can we describe to a program that this patch of the image contains a face while this patch doesn't?\n00:01:36 It's hard to describe that in a set of rules.\n00:01:41 Similarly, Netflix predicting how much you'll like a movie.\n00:01:46 What kinds of movies do you like?\n00:01:50 What kind of movie is this?\n00:01:55 Again, very hard to write down in an explicit set of.\n00:01:59 Instead, we'll need the computer to learn through examples.\n00:02:06 Both these tasks are examples of so-called supervised learning, in which we're given training data with the correct answers already tagged.\n00:02:13 We can then design a program to try to reproduce these correct answers.\n00:02:19 In general, these problems come in two flavors.\n00:02:26 Classification problems, in which the thing we must predict is a discrete value, for instance, deciding that a square contains a face or does not, a binary decision, or whether an email is a spam email or not.\n00:02:33 In contrast, regression problems, we're predicting a real value number.\n00:02:39 For example, Netflix may guess that you will rate a movie 3.8 stars.\n00:02:46 Note that the discreteness is in the prediction.\n00:02:53 In Netflix, for example, you can only rate a movie 3 or 4 stars, but because Netflix can predict a real value number like 3.8, it would be called a regression.\n00:02:59 Another type of problem, unsupervised learning, refers to problems in which there's no specific signal to predict.\n00:03:08 Instead, we simply want to understand the data, the structure, notions of similarity, how they relate to one another.\n00:03:17 Often the term data mining refers to this kind of data exploration or data understanding problem.\n00:03:25 On the left is one such unsupervised framework, this time applied to the Netflix data again, where now the data have been used to understand the notion of similarity between movies.\n00:03:34 We can use the ratings to organize and group them by similarity, summarize them, or even improve some kind of prediction problem.\n00:03:42 On the right are images of a hand as it explores two degrees of freedom, opening and closing and rotating.\n00:03:51 The computer only observes image patches.\n00:04:00 consisting of hundreds of grayscale values.\n00:04:08 Using only these kinds of image patches, unsupervised learning techniques can organize the images automatically, such that, for example, closed hands are here on the bottom, open hands are organized onto the top, and left to right shows vertical to horizontal rotation.\n00:04:17 Note that in both cases, the computer doesn't so much understand the semantic meaning behind this, as it does understand the similarity between the data.", "start_char_idx": 0, "end_char_idx": 4247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47382864-30b1-4572-9790-abb13713f7a0": {"__data__": {"id_": "47382864-30b1-4572-9790-abb13713f7a0", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c770b46-ce8a-4e25-87ba-fc85fb1ec708", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "hash": "e10c1517837a7616e0dbf4579ed6076df6b40e5e7ae9138636159312d2198d74", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a03953c5-d372-4086-a721-98efd52726e7", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}, "hash": "ff277ceebdd15cbca6837d2fb2153cb733aedfc2477d8c785f869f2e30f9a303", "class_name": "RelatedNodeInfo"}}, "text": "00:03:34 We can use the ratings to organize and group them by similarity, summarize them, or even improve some kind of prediction problem.\n00:03:42 On the right are images of a hand as it explores two degrees of freedom, opening and closing and rotating.\n00:03:51 The computer only observes image patches.\n00:04:00 consisting of hundreds of grayscale values.\n00:04:08 Using only these kinds of image patches, unsupervised learning techniques can organize the images automatically, such that, for example, closed hands are here on the bottom, open hands are organized onto the top, and left to right shows vertical to horizontal rotation.\n00:04:17 Note that in both cases, the computer doesn't so much understand the semantic meaning behind this, as it does understand the similarity between the data.\n00:04:25 So this is an automatic organization, but the semantic meaning is then usually imposed by humans later.\n00:04:34 Many other variants of learning problems also exist.\n00:04:42 A few of note include semi-supervised learning, which is really a supervised learning problem in that there's a signal, a specific signal to predict, but not all of the examples have the correct answer already given.\n00:04:51 Typically, unsupervised-like methods can then be used to support and improve on the supervised decision.\n00:04:59 These kinds of problems are increasingly common since we now have a lot of data, but not very much time to hand label it.\n00:05:05 In medical data, for example, we might have a lot of patient information, but very few examples in which we know the actual outcome or the best action to take.\n00:05:11 On the web, on the other hand, we might have millions of photos, but we can't expect users to label or tag all of them.\n00:05:17 Users will only label a small subset.\n00:05:23 So we have a vast trove of unsupervised data to go along with a small collection of supervised data.\n00:05:29 Finally, reinforcement learning involves learning when there's only indirect feedback on quality.\n00:05:35 Rather than actually knowing the correct answer or what a human would have done, as in supervised learning, here we only get a relative quality score telling us whether we're doing better or worse.\n00:05:41 Moreover, this feedback might be delayed.\n00:05:47 It might reflect many actions or predictions in sequence.\n00:05:53 It might be noisy.\n00:05:59 just consider the game of poker.\n00:06:08 In poker, we take a sequence of actions, betting, raising, but at the end, we don't even find out the correct answer to those actions.\n00:06:16 We just find out whether we won or lost in the end.\n00:06:24 Much of robotics and other kinds of sequential action planning rely on reinforcement learning techniques for developing their behaviors.\n00:06:32 In summary, machine learning is a computational study of data, including many types of subproblems.\n00:06:40 In this course, we'll start with supervised learning, which are prediction problems in which we're given examples of what our function should output in a number of instances, taken in the form of training data with input features and labeled desired output values.", "start_char_idx": 3447, "end_char_idx": 6584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bae5941d-5d4b-423c-9892-b48c472bd4c3": {"__data__": {"id_": "bae5941d-5d4b-423c-9892-b48c472bd4c3", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a29404b2-5ccb-4a7a-975c-a1efa702e8f5", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "da4374e265f1fef0d256feb75845cd02a3190812087dc6dbfb7508dcf60f94a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ae56c85-2dd6-48f6-8109-552daf5e7dd0", "node_type": "1", "metadata": {}, "hash": "6985bb40bbf0cabad4d14a887cd9b4575e4fe8c831fef7bed1ec2b49992f697d", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Next, we'll look at some simple data exploration and visualization techniques and use MATLAB software to do so.\n00:00:07 Since machine learning is fundamentally a data science, the first thing we're likely to want to do is to take a look at the data, understand it, get a feel for what might work.\n00:00:15 The type of data is often very important.\n00:00:22 Some data might be binary, for instance, labeling emails as spam or not, observing the gender of a user.\n00:00:30 Some data might be categorical, for instance, the state that the user resides in or age values binned into several groups.\n00:00:37 And some observed features are likely to be continuous or nearly so, like pixel intensities or prices.\n00:00:45 Another very important point is, are any of the data values missing?\n00:00:52 If we only have partial data, meaning some of the features that we're going to observe about these data items are...\n00:01:00 not observed in all cases, we'll likely have to do something to fill those missing entries in.\n00:01:05 We'll return to this point in later lectures.\n00:01:10 Finally, we'd often like to get an idea of the shape of the data.\n00:01:15 For multi-dimensional data, how are the dimensions, how are the features related?\n00:01:20 Are there data outliers?\n00:01:25 For instance, data points that look very different from the majority of observations.\n00:01:30 Are any of the features redundant or unnecessary?\n00:01:35 To do our visualization, we'll need some software.\n00:01:40 Here, we'll use MATLAB, a popular language for scientific computing.\n00:01:45 MATLAB was designed for linear algebra and operates directly on vectors and matrices.\n00:01:50 It's widely used in many areas of science, including machine learning, so there's lots of code available on the web, but it can be a bit inefficient since MATLAB is an interpreted language, not compiled.\n00:01:55 If you don't want to buy or use MATLAB,\n00:02:00 Octave is a free, nearly code compatible open source project mimicking MATLAB.\n00:02:06 Octave is very well established and stable, although the graphical user interface is a little bit rough.\n00:02:12 Another newer MATLAB replacement is called FreeMAT.\n00:02:18 I have less experience with this, but it's another possibility.\n00:02:24 There are a lot of other alternatives as well, of course.\n00:02:30 In statistics, the language R is very popular and free.\n00:02:36 Again, with lots of available statistics code.\n00:02:42 Python is growing in usage with a number of packages intended to replicate some of the most useful aspects of MATLAB and especially high adoption in computational biology and bioinformatics.\n00:02:48 And, of course, for fast performance, C++ is widely used.\n00:02:54 It's very hard to beat for efficiency, but it's often slow to write code and prototype in, so I don't really recommend it.\n00:03:00 it for classwork, only for production.\n00:03:04 Let's get into a little notation to get started.\n00:03:09 We'll assume that we have M data points, X, X1 through XM.\n00:03:13 I'll try to use parentheses superscripts to indicate data identification numbers.\n00:03:18 Each datum is then a vector of observation.\n00:03:23 So data point J consists of N dimensional measurements.\n00:03:27 These dimensions are often called features.\n00:03:32 It will be useful to stack these data points together so that data point 1 is the first row, data point 2 is the second row through data point N.\n00:03:36 All the features are aligned into something called the data matrix.\n00:03:41 Again, each row is a data point, each column is a feature.\n00:03:46 Note that data matrices are not standardized.\n00:03:50 Some text and code you'll find will use the opposite convention, the transpose of the matrix I'm using.\n00:03:55 So please...\n00:03:59 be careful.\n00:04:05 Here's some code just showing how to load, say, the Fisher-Iris dataset in MATLAB, renaming that matrix as X.", "start_char_idx": 0, "end_char_idx": 3916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ae56c85-2dd6-48f6-8109-552daf5e7dd0": {"__data__": {"id_": "4ae56c85-2dd6-48f6-8109-552daf5e7dd0", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a29404b2-5ccb-4a7a-975c-a1efa702e8f5", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "da4374e265f1fef0d256feb75845cd02a3190812087dc6dbfb7508dcf60f94a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bae5941d-5d4b-423c-9892-b48c472bd4c3", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "18585c990f801fb75fd1c3155c609ba1a080671b0bd9a03d86e447cc90149570", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "010cd150-1ff8-42c4-9717-d51f5140971b", "node_type": "1", "metadata": {}, "hash": "4e573eae35e0f013178bb41d0007695c86920f55758f60f7b93ed36116022589", "class_name": "RelatedNodeInfo"}}, "text": "00:03:23 So data point J consists of N dimensional measurements.\n00:03:27 These dimensions are often called features.\n00:03:32 It will be useful to stack these data points together so that data point 1 is the first row, data point 2 is the second row through data point N.\n00:03:36 All the features are aligned into something called the data matrix.\n00:03:41 Again, each row is a data point, each column is a feature.\n00:03:46 Note that data matrices are not standardized.\n00:03:50 Some text and code you'll find will use the opposite convention, the transpose of the matrix I'm using.\n00:03:55 So please...\n00:03:59 be careful.\n00:04:05 Here's some code just showing how to load, say, the Fisher-Iris dataset in MATLAB, renaming that matrix as X.\n00:04:11 And if we look at the size of X, we'll see that there are 150 measurements, each of four dimensions.\n00:04:18 The very first thing we might want to do is look at some basic statistics of the data.\n00:04:24 The mean, for example, tells us the average value.\n00:04:30 Calling mean on the data matrix averages over the first dimension so that we get the average in each feature.\n00:04:36 Median similarly tells us the center of the data.\n00:04:42 Maximum and minimum will tell us the range of the data.\n00:04:48 And something like standard deviation will tell us how much variation, how much spread to expect in each feature.\n00:04:54 A histogram bins the data points into a collection of discrete locations and counts the data that fall into each bin.\n00:05:00 If there are k bins, it essentially summarizes the data with k counts, and the plot gives a picture and a sense of how the data are distributed, the shape and spread of the data, presence of any outliers, etc.\n00:05:08 It's a bit trivial, but useful to think about k and how it determines how much summarization actually occurs in the histogram.\n00:05:17 K can in some sense be thought of as a complexity of the histogram.\n00:05:25 For example, if k is extremely large and the data values are actually unique, then every data point will fall into its own bin, and the histogram would simply tell you the positions of all those data.\n00:05:34 It would memorize all the data positions, and it wouldn't really be of any use in summarizing or understanding the data.\n00:05:42 Similarly, if k is too small, the binning would be extremely coarse, and you wouldn't learn anything about the shape.\n00:05:51 The actually right value of k, the best value of k for a plot, depends on...\n00:05:59 a lot of things, including how much data you have, how those data are distributed, and what you're trying to learn about the shape of the distribution.\n00:06:08 A scatterplot is another easy way to visualize how two variables relate to each other in the data.\n00:06:17 Here, we plot each datum as a point, blue, with coordinates given by its first feature X1 and second feature X2.\n00:06:25 This can give you an idea of the spread of the data in those two features, the relationship between the two features, and also, again, the presence of any outliers in the data.\n00:06:34 Of course, in practice, we'll often have more than two features, and it's very difficult to plot such things, but one option is to just look at every possible pair and do a number of two-dimensional plots.\n00:06:42 This is a so-called pair plot, showing all possible feature pairs.\n00:06:51 So for example, this entry here...\n00:06:59 corresponds to plotting x1 versus x2.\n00:07:08 This entry is x1 versus x3, and so on.\n00:07:17 A collection of plots like this give you a quick idea of how features might be related to each other and what their distribution looks like.\n00:07:25 In supervised learning, our goal is to predict something, specifically the target variable.\n00:07:34 This might be a label for email, whether it's spam or not, a price to go along with something.", "start_char_idx": 3169, "end_char_idx": 7022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "010cd150-1ff8-42c4-9717-d51f5140971b": {"__data__": {"id_": "010cd150-1ff8-42c4-9717-d51f5140971b", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a29404b2-5ccb-4a7a-975c-a1efa702e8f5", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "da4374e265f1fef0d256feb75845cd02a3190812087dc6dbfb7508dcf60f94a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ae56c85-2dd6-48f6-8109-552daf5e7dd0", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}, "hash": "9b7400a3585928be0cbb40d301e8a94d800e1a39b168309a4f27cb0232501a38", "class_name": "RelatedNodeInfo"}}, "text": "00:06:34 Of course, in practice, we'll often have more than two features, and it's very difficult to plot such things, but one option is to just look at every possible pair and do a number of two-dimensional plots.\n00:06:42 This is a so-called pair plot, showing all possible feature pairs.\n00:06:51 So for example, this entry here...\n00:06:59 corresponds to plotting x1 versus x2.\n00:07:08 This entry is x1 versus x3, and so on.\n00:07:17 A collection of plots like this give you a quick idea of how features might be related to each other and what their distribution looks like.\n00:07:25 In supervised learning, our goal is to predict something, specifically the target variable.\n00:07:34 This might be a label for email, whether it's spam or not, a price to go along with something.\n00:07:42 In this iris data set that we loaded, it's the species of the iris flower that's having its properties measured, one of three different types.\n00:07:51 So while we could treat this target like any other feature that's being measured, since part of our goal is to understand how the other measurements are related to and can help us predict this label, so for a discrete labeling problem, a classification problem.\n00:07:59 we can use color to understand how the histograms differ between the different classes or where the data from each class are located on that scatter plot.\n00:08:16 This gives us a sense of how easy it's going to be to predict the species, the target variable, from the other features.\n00:08:32 This lecture introduced some basic concepts in data exploration, the representation in terms of the data matrix, computing some basic statistics of the data, and visualizing the data using histograms, scatter plots, pair plots for higher dimensions, and using color for classification and discrete targets.", "start_char_idx": 6238, "end_char_idx": 8055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dad14003-d5da-40a7-b80b-46d1abc32ff4": {"__data__": {"id_": "dad14003-d5da-40a7-b80b-46d1abc32ff4", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f92fa923-f522-44ef-b8c6-dd3f4169548d", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "d869abb3377f9a983345fd5d58e332b7bc33676f4e3d6b98057ddda2713de17e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7845ea4a-c84a-460e-852e-098c51ebb80e", "node_type": "1", "metadata": {}, "hash": "a369fc95dd4456f2b51220f5b205b028a3056207a5d27ffa9ad747fa153cf32c", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 We'll begin with supervised learning problems, including classification and regression problems.\n00:00:07 As I mentioned, machine learning may be unlike much of the programming that you might have done in the past.\n00:00:15 Here, we can't simply tell the computer how to solve the problem, give it a precise algorithm to follow.\n00:00:22 Instead, we'll tell it how to figure out how to solve the problem, a kind of metaprogramming task.\n00:00:30 To do so, we design a flexible program called the learner, whose behavior can be modified by changing aspects of its operation, called the parameters.\n00:00:37 The learner is a deterministic function that takes in the features of a new example and spits out a prediction for its target.\n00:00:45 This input-output behavior is a function of these internal parameters.\n00:00:52 We can then write a second program, the learning algorithm, which sets or modifies the parameters of the learner.\n00:01:00 until it's doing a good job at predicting the training points that we've seen in the past, as measured by some score or cost function.\n00:01:05 Some notation is useful.\n00:01:10 The input features X will constitute all the information we have to make our prediction.\n00:01:15 For example, if I want to predict whether you'll need to go to the hospital in the next year, I might use all the information I have about you, your age, your weight, height, the outcomes of recent medical tests, etc.\n00:01:20 These are all available to me beforehand.\n00:01:25 I input them into my learner, and it outputs a prediction, yes or no, whether you'll be hospitalized.\n00:01:30 Then later, I'll find out the answer.\n00:01:35 Did you actually go to the hospital?\n00:01:40 That's the true target value, Y.\n00:01:45 I call my prediction Y hat, the estimate of Y, and by comparing Y and Y hat, I can use it to determine my score.\n00:01:50 I pay some penalty or cost if I'm wrong.\n00:01:55 We'll denote the generic parameter\n00:02:00 of our learner by theta.\n00:02:06 Since data play a fundamental role in machine learning, we'll need some way of visualizing the training data along with the results of our learning algorithm.\n00:02:13 Our training data consists of pairs of features and their associated target value.\n00:02:20 So you can imagine plotting those data.\n00:02:26 Since we can never really plot more than two dimensions, when we plot, we'll pretend that we have only one real valued x, and then the purpose of our learner is to map the feature value x into this target value y.\n00:02:33 In other words, given a new feature, x new, we're supposed to predict what the y value associated with that feature would be.\n00:02:39 Hopefully something that's reasonably similar to the training data.\n00:02:46 Since it outputs a value for any x, our learner, whatever it is, defines a function from x to y.\n00:02:53 And by evaluating every possible x,\n00:02:59 We can trace out that function to see what our learner thinks is the actual relationship between X and Y.\n00:03:07 In some cases, the shape or functional form of this function may be explicitly stated by the model, or it might be implicitly defined by the prediction program.\n00:03:14 Let's see some simple examples of this.\n00:03:22 An extremely simple predictor that we'll discuss in more detail soon is the nearest neighbor predictor.\n00:03:29 Nearest neighbor is defined in a very simple rule.\n00:03:37 Store the data, the red points in the scatter plot, in a database, and when asked to predict a new point X new, just find the most similar point in the database, looking only at the feature values, and predict whatever that closest point's value Y is.\n00:03:44 So this is basically a memorize and regurgitate kind of procedure.\n00:03:52 If we follow that procedure for every possible X, we'll trace out a set of predictions.\n00:03:59 implicitly defines a function mapping X to Y.\n00:04:08 You can notice the properties of this function.", "start_char_idx": 0, "end_char_idx": 3942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7845ea4a-c84a-460e-852e-098c51ebb80e": {"__data__": {"id_": "7845ea4a-c84a-460e-852e-098c51ebb80e", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f92fa923-f522-44ef-b8c6-dd3f4169548d", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "d869abb3377f9a983345fd5d58e332b7bc33676f4e3d6b98057ddda2713de17e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dad14003-d5da-40a7-b80b-46d1abc32ff4", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "03d5c35c7d9bb6d2a3db46edc0171ce3b4952d562d4a9c2c5ef3d6d00ab0a121", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d53e701-d53a-4475-a745-ea08ec4c58a9", "node_type": "1", "metadata": {}, "hash": "6c4a55eca7b39a88e474eb6d83efa13a501467f8e36034397081b17f50170a76", "class_name": "RelatedNodeInfo"}}, "text": "00:03:14 Let's see some simple examples of this.\n00:03:22 An extremely simple predictor that we'll discuss in more detail soon is the nearest neighbor predictor.\n00:03:29 Nearest neighbor is defined in a very simple rule.\n00:03:37 Store the data, the red points in the scatter plot, in a database, and when asked to predict a new point X new, just find the most similar point in the database, looking only at the feature values, and predict whatever that closest point's value Y is.\n00:03:44 So this is basically a memorize and regurgitate kind of procedure.\n00:03:52 If we follow that procedure for every possible X, we'll trace out a set of predictions.\n00:03:59 implicitly defines a function mapping X to Y.\n00:04:08 You can notice the properties of this function.\n00:04:17 It's flat, constant, near any training example, since that example is the nearest point, and it continues to predict that Y, and then it abruptly changes the moment that X becomes closer to the next training example, so halfway between the two points.\n00:04:25 In contrast, here's another predictor we'll spend more time on, the linear predictor.\n00:04:34 Here, the predictor evaluates a linear function of the feature, computing some value, say theta zero, and adding theta one times X, and then outputting that prediction.\n00:04:42 Tracing out the values of this prediction on various values of X exactly shows this equation.\n00:04:51 The functional form of f of X is explicitly defined within this procedure.\n00:04:59 It's then quite easy to see what the effect of changing the parameters theta is.\n00:05:07 It changes f of x within some parametric family of possible functions such as lines here.\n00:05:14 The goal of the learning algorithm is then to modify these parameters until it finds a good function f of x within that family.\n00:05:22 Obviously our predictions may not perfectly match the data points.\n00:05:29 For any data point i, we can measure the difference between the observed target value yi and our prediction y-hat-i, and we just measure this difference and we can call it the error residual.\n00:05:37 For an accurate predictor, these residuals should be small.\n00:05:44 A common way to measure the total amount of error is the mean squared error, which just averages the square of the error residuals on the data.\n00:05:52 So the squaring makes them all positive, averaging tells me overall\n00:05:59 how I'm doing.\n00:06:04 Again, since we're restricted to looking at 2D plots when we actually visualize things, we'll only be able to draw very simple examples for visualization.\n00:06:09 For regression, we're essentially forced to look at one feature, x, versus the real value target, y.\n00:06:14 Even though in practice, of course, we'll probably use many more than one feature.\n00:06:19 For classification, however, we're predicting a discrete-valued y, often, say, a binary one, y equals 0 versus y equals 1.\n00:06:24 Spam or not spam, say.\n00:06:29 We can similarly plot x and y, just like we did in regression.\n00:06:34 But since y is discrete, this makes for a pretty boring plot.\n00:06:39 Instead, we can just use colors or symbols on the data points to indicate their y value.\n00:06:44 And then we only need to plot x.\n00:06:49 We don't actually need to plot y at all.\n00:06:54 So this means we can get away with plotting 2D.\n00:06:59 features instead of just one.\n00:07:08 We can plot a scatter plot of feature one versus feature two, and we indicate the class, the value of y, using a symbol or a color, here again, blue or red.\n00:07:17 Again, any learner should learn a map, a way to map locations, x1, x2, into a prediction.\n00:07:25 Now a discrete class, say 0 or 1, if we do that for every possible x1, x2 place on the plane, we'll now get a three-dimensional function whose value we can indicate with color.", "start_char_idx": 3175, "end_char_idx": 6999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d53e701-d53a-4475-a745-ea08ec4c58a9": {"__data__": {"id_": "9d53e701-d53a-4475-a745-ea08ec4c58a9", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f92fa923-f522-44ef-b8c6-dd3f4169548d", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "d869abb3377f9a983345fd5d58e332b7bc33676f4e3d6b98057ddda2713de17e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7845ea4a-c84a-460e-852e-098c51ebb80e", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}, "hash": "1dd413183b7b02f4f82388e18ccbcdcd4dce0532cfd24caed0ff5a3dab9c2bfa", "class_name": "RelatedNodeInfo"}}, "text": "00:06:44 And then we only need to plot x.\n00:06:49 We don't actually need to plot y at all.\n00:06:54 So this means we can get away with plotting 2D.\n00:06:59 features instead of just one.\n00:07:08 We can plot a scatter plot of feature one versus feature two, and we indicate the class, the value of y, using a symbol or a color, here again, blue or red.\n00:07:17 Again, any learner should learn a map, a way to map locations, x1, x2, into a prediction.\n00:07:25 Now a discrete class, say 0 or 1, if we do that for every possible x1, x2 place on the plane, we'll now get a three-dimensional function whose value we can indicate with color.\n00:07:34 Where now red shows all the points where we predict class 1, and blue, all those where we decide the other class, say minus 1.\n00:07:42 An important concept for classification is that the function transitions immediately from one value to the other.\n00:07:51 And this means that there's a set of points at which the function changes.\n00:07:59 abruptly from one value to the other.\n00:08:07 This is called the decision boundary.\n00:08:14 In some sense, the learner, the classifier, can be characterized completely by the decision boundaries it's able to represent.\n00:08:22 Note again that this is a function, y hat, of two variables, x1 and x2, whose value is visualized with color.\n00:08:29 Again, in classification, it may be that our predictions don't necessarily match all of the observed data.\n00:08:37 For classification problems, the most common method of assessing accuracy is called the error rate, which is the probability of making an incorrect prediction.\n00:08:44 The empirical error rate is just given by the fraction of data points, i, at which our predictor, y hat, makes an incorrect prediction, i.e., a prediction that differs from the observed value, y.\n00:08:52 So here, we have a blue point where we're predicting red, a red point where we're.\n00:08:59 we're predicting blue, so we're making two out of 10 errors.\n00:09:12 In summary, we started with supervised learning, which are prediction problems in which we're given examples of what our function should output in the form of training data with input features X and a labeled desired output value Y.\n00:09:25 We saw two examples of these types of problems, regression problems, where we predict real valued numbers, and we'll visualize these as a function mapping a single feature X to its predicted Y value.\n00:09:38 Classification problems, on the other hand, predict discrete valued targets, and so I'll usually visualize them using two input features, X1 and X2, and drawing the output Y values using colors or symbols, or the decision boundary transitioning from one prediction value to another.", "start_char_idx": 6361, "end_char_idx": 9086, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cf60006-b177-4589-83fb-9339f1c67a43": {"__data__": {"id_": "9cf60006-b177-4589-83fb-9339f1c67a43", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (4) Complexity and Overfitting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=VZuKBKd4ck4", "Link": "https://www.youtube.com/watch?v=VZuKBKd4ck4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11814e8b-07cf-4a83-9007-407e89485cc9", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (4) Complexity and Overfitting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=VZuKBKd4ck4", "Link": "https://www.youtube.com/watch?v=VZuKBKd4ck4"}, "hash": "9ec17d15923037f8db947a19eaa3995737c05dfd059eb95bf752775de06cc993", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35ea3d05-4902-42cd-8839-4438234fd328", "node_type": "1", "metadata": {}, "hash": "5e1ecfb29a43c4520bda71c349c997a47d8d6513516969b1124b0f47dea2230c", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 A major theme that we'll revisit again and again during supervised learning is the concept of overfitting and the complexity of a predictor.\n00:00:07 Suppose we have a simple regression problem with one feature X and a real-valued target Y.\n00:00:15 And here are our training examples from which we'd like to learn the relationship between X and Y.\n00:00:22 Here's one possible, plausible model for the relationship between X and Y.\n00:00:30 It says that Y is a linear function of X, but then there's an addition of a small amount of noise so that these data points are not perfectly predictable given the relationship.\n00:00:37 So we're explaining each data point by a linear relationship plus some uncertainty due to things that we can't actually measure.\n00:00:45 On the other hand, here's a very different explanation of the same points.\n00:00:52 Here we're supposing that the relationship between X and Y is a linear function of X.\n00:01:00 y is some very complicated function, and because this function actually touches all of the data points exactly, we don't need to explain any uncertainty or misses in our predictions using noise.\n00:01:07 So this would be a noise-free or nearly noise-free model.\n00:01:15 Intuitively speaking, very few people believe that this is the real relationship between x and y, but there's no particular reason not to think so.\n00:01:22 In fact, both of these models explain the data perfectly well, just using different phenomena.\n00:01:30 However, the real test of a predictor is not how well it predicted the data that it saw when it trained, but how well it does in the future once you deploy it on a new system.\n00:01:37 Suppose we gather a new collection of examples of the relationship between x and y, consisting of these green points.\n00:01:45 So these are points that the model never saw when it was trying to learn itself.\n00:01:52 These can just be surrogates for...\n00:02:00 what would happen if we then tried to use this model in the future.\n00:02:06 When we compare the linear model, relating x and y through a line, its explanation still holds up.\n00:02:13 The line is fairly reasonable, and there's a small amount of noise that's not being explained by that relationship.\n00:02:20 On the other hand, the complex example completely breaks down.\n00:02:26 Many of these points are near misses, but many of them are completely poorly explained by this line.\n00:02:33 So although the complex curve manages to explain all the red points that we're seen during training fairly well, it has quite poor predictive properties on data that it hasn't seen.\n00:02:39 This general phenomenon is known as overfitting.\n00:02:46 And what typically happens is we can see how our performance on the training data evolves as we allow our predictions to get more complicated.\n00:02:53 As we allow ourselves more and more complex models, we'll be better and better able to fit.\n00:02:59 the training data so that we come closer and closer to them, and we need to explain less and less of their values using noise.\n00:03:06 However, the same property doesn't quite hold for test data.\n00:03:13 If we were to get new data that we've never seen before, we would find a different, more U-shaped curve.\n00:03:19 In this case, it's possible for our model to be too simple.\n00:03:26 As we allow ourselves more and more complex models, we're better able to model the actual relationship between X and Y.\n00:03:33 However, at some point, that bottoms out, and we start to get worse and worse.\n00:03:39 And in this section, we're actually overfitting to the data.\n00:03:46 We're using a very complex function to try to explain changes in the training data that are really just due to noise.\n00:03:53 And so, we're going to see as we try to fit the training data better.\n00:03:59 our performance on new data is actually going to degrade.\n00:04:09 So over here, where we've got a model that's too simple, and making it more complex would help us model the actual phenomenon, is a phenomenon called underfitting.", "start_char_idx": 0, "end_char_idx": 4040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35ea3d05-4902-42cd-8839-4438234fd328": {"__data__": {"id_": "35ea3d05-4902-42cd-8839-4438234fd328", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Introduction (4) Complexity and Overfitting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=VZuKBKd4ck4", "Link": "https://www.youtube.com/watch?v=VZuKBKd4ck4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11814e8b-07cf-4a83-9007-407e89485cc9", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Introduction (4) Complexity and Overfitting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=VZuKBKd4ck4", "Link": "https://www.youtube.com/watch?v=VZuKBKd4ck4"}, "hash": "9ec17d15923037f8db947a19eaa3995737c05dfd059eb95bf752775de06cc993", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9cf60006-b177-4589-83fb-9339f1c67a43", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Introduction (4) Complexity and Overfitting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=VZuKBKd4ck4", "Link": "https://www.youtube.com/watch?v=VZuKBKd4ck4"}, "hash": "effb1f12aacf446092a22520d6213ad176424a28cb207790f4f83ef5292efb71", "class_name": "RelatedNodeInfo"}}, "text": "00:03:26 As we allow ourselves more and more complex models, we're better able to model the actual relationship between X and Y.\n00:03:33 However, at some point, that bottoms out, and we start to get worse and worse.\n00:03:39 And in this section, we're actually overfitting to the data.\n00:03:46 We're using a very complex function to try to explain changes in the training data that are really just due to noise.\n00:03:53 And so, we're going to see as we try to fit the training data better.\n00:03:59 our performance on new data is actually going to degrade.\n00:04:09 So over here, where we've got a model that's too simple, and making it more complex would help us model the actual phenomenon, is a phenomenon called underfitting.\n00:04:19 And over here, where we've chosen a model that's too complex, and it's done a great job of fitting the training data, but is doing poorly on the test data, is called overfitting.\n00:04:29 Ideally, we'd like to be in this middle range, where we get the best possible performance on new data that we've never seen.\n00:04:39 Typically, the underfitting regime is characterized by fairly close values between the training error and the test error, while overfitting regime is characterized by diverging values of the training data as error heads towards zero and the test data as error increases.\n00:04:49 In the overfitting regime, our estimated error, using only the training data,\n00:04:59 underestimates the actual error we'll experience in practice.\n00:05:08 Thus, it's always important to have data that your model has never seen in order to try to assess what its performance will be in the future.\n00:05:17 Almost all competitions use this form, providing a certain amount of training data that you can use to build your models, and another set of different data called the validation data that they'll use to assess and select among the models.\n00:05:25 Usually, this is used for validation and for a leader board to tell you how your methods are improving.\n00:05:34 And finally, there'll be even more data that you'll never be able to see, and that's used to estimate the real performance and do the final scoring.\n00:05:42 So since you're not able to see this at any point during the competition, you're not able to train your model to do well on it in particular, and so it's going to estimate the actual performance in practice.\n00:05:51 This kind of splitting often happens in multiple levels.\n00:05:59 So for instance, you yourself might take your training data and split it into multiple tiers so that you have your own validation data with which to do the same kind of processing.\n00:06:10 Over- and underfitting phenomena are critical to machine learning and we'll see them arise again and again in numerous supervised learning tasks.", "start_char_idx": 3308, "end_char_idx": 6098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5839e6a5-eaa0-431c-830d-d14b4a8e8bbc": {"__data__": {"id_": "5839e6a5-eaa0-431c-830d-d14b4a8e8bbc", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Nearest Neighbor (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=FMCZXFRqZvg", "Link": "https://www.youtube.com/watch?v=FMCZXFRqZvg"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f7f690f-5604-4e6a-b4f4-2e1b47e7fc3d", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Nearest Neighbor (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=FMCZXFRqZvg", "Link": "https://www.youtube.com/watch?v=FMCZXFRqZvg"}, "hash": "4f199ab66a0cbf459668116002f3499129d6ceba9f43c88eaa82becb2fe7b4ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bb0498f-0b64-450b-b834-436966d126b5", "node_type": "1", "metadata": {}, "hash": "3d1a68e93424d7c86201e89d7081f2baad50752a785f4231da440ea2a388c30e", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 The first machine learning technique we'll look at are nearest neighbor methods.\n00:00:08 While at first they may not appear to do much learning per se, in practice they can be quite useful and will illustrate some of the issues we'll be dealing with.\n00:00:17 Recall that these are supervised learning problems, so we'll be trying to predict a target Y using some collection or vector of features X.\n00:00:25 Our learner will take in the features and will spit out a prediction Y hat, and our learner will be parameterized in some way so that observing data will change its input-output behavior.\n00:00:34 Let's start with a regression problem.\n00:00:42 In regression, for the purposes of plotting, we'll assume that we just have a single scalar feature X and the target Y, and therefore we can plot each of our observed data points as a point with an XY coordinate.\n00:00:51 And when we look at this scatter plot...\n00:00:59 that suggests some relationship between x and y that we can use so that given a new x point that we're asked to predict at, this relationship will suggest what value we should predict the y value for that feature x at.\n00:01:12 The nearest neighbor predictor follows a very simple formula.\n00:01:24 We simply save all of the training data points, the red dots in the scatter plot, and our predictor just follows a very simple procedure, which is that given a new feature x, it looks in that data set, finds the data point with the closest value of x, and predicts the y value that was associated with that training point.\n00:01:36 So given x new, we find the xi training data point that's closest and predict its associated yi.\n00:01:48 While defined as a procedure, this\n00:02:00 implicitly defines a function of x, since every x has a prediction associated with it.\n00:02:10 If we evaluate that procedure at each possible feature x, we get a function that looks like this.\n00:02:20 So, whenever we're closest to this data point, we predict its y value.\n00:02:30 The moment that we become halfway between that data point and the next data point, we transition abruptly to the new nearest data point's y value.\n00:02:40 Notice that as a regression, this function is locally constant, since as we move very slightly, we continue to have the same nearest neighbor, so we continue to predict exactly the same y value, followed by an abrupt transition whenever we change our nearest data point.\n00:02:50 In classification, a supervised learning problem in which the target values y are discrete-valued, instead of plotting them as we did in the\n00:03:00 a scatterplot in regression, we'll simply indicate them using a symbol or a color.\n00:03:05 So here, 1 and 0.\n00:03:10 And that allows us to pretend that we have two features.\n00:03:16 So for our purposes of illustration, we'll imagine that we have a feature x1 and another feature x2.\n00:03:21 And in this case, the problem is the same.\n00:03:27 Given a new feature vector x, we'd like to predict whether that associated y will be a 0 or a 1.\n00:03:32 The nearest neighbor rule here simply looks and chooses the data point that's closest in terms of distance and predicts the value associated with it.\n00:03:38 So here, we need a distance on a vector space.\n00:03:43 So we'll typically choose the Euclidean distance, which is just the sum of squared distances over the features.\n00:03:49 Again, we can evaluate this procedure at every possible point x.\n00:03:54 And that will define a function.\n00:04:00 points x where we're closest to a 1, we'll predict 1.\n00:04:06 At all points x where we're closest to a 0, we'll predict 0.\n00:04:13 And between them, there'll be an abrupt change called the decision boundary.\n00:04:20 The decision boundary indicates the set of points at which on one side we're predicting 1 and on the other side we're predicting 0.\n00:04:26 Geometrically, it's clear that just like in the regression case, there'll be a set of points where each data point is in fact the closest point.", "start_char_idx": 0, "end_char_idx": 3995, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bb0498f-0b64-450b-b834-436966d126b5": {"__data__": {"id_": "5bb0498f-0b64-450b-b834-436966d126b5", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Nearest Neighbor (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=FMCZXFRqZvg", "Link": "https://www.youtube.com/watch?v=FMCZXFRqZvg"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f7f690f-5604-4e6a-b4f4-2e1b47e7fc3d", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Nearest Neighbor (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=FMCZXFRqZvg", "Link": "https://www.youtube.com/watch?v=FMCZXFRqZvg"}, "hash": "4f199ab66a0cbf459668116002f3499129d6ceba9f43c88eaa82becb2fe7b4ec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5839e6a5-eaa0-431c-830d-d14b4a8e8bbc", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Nearest Neighbor (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=FMCZXFRqZvg", "Link": "https://www.youtube.com/watch?v=FMCZXFRqZvg"}, "hash": "d579df9bea198129bdf0fe20fa027730427f4c4fabeb1ffdf61bfe2ad5b02fa7", "class_name": "RelatedNodeInfo"}}, "text": "00:03:43 So we'll typically choose the Euclidean distance, which is just the sum of squared distances over the features.\n00:03:49 Again, we can evaluate this procedure at every possible point x.\n00:03:54 And that will define a function.\n00:04:00 points x where we're closest to a 1, we'll predict 1.\n00:04:06 At all points x where we're closest to a 0, we'll predict 0.\n00:04:13 And between them, there'll be an abrupt change called the decision boundary.\n00:04:20 The decision boundary indicates the set of points at which on one side we're predicting 1 and on the other side we're predicting 0.\n00:04:26 Geometrically, it's clear that just like in the regression case, there'll be a set of points where each data point is in fact the closest point.\n00:04:33 So in the neighborhood of this data point, I'll be using it for prediction, and so whenever I'm closest to it, I'll be predicting its value.\n00:04:40 Similarly, in the neighborhood of this data point, it will be the closest point and I will be predicting its value.\n00:04:46 If we think about what this turns into, we see that each data point can be thought of breaking up the space into a Voronoi tessellation.\n00:04:53 The set of points that are closest to a 1 is called a Voronoi tessellation, and the set of points that are closest to a 0 is called a Voronoi tessellation.\n00:05:00 to that data point than any other data point.\n00:05:08 So, for example, all points in this section of the space are closer to this data point than to any other example data point.\n00:05:17 Similarly, points in this set of space are closer to this data point than to any of the other data.\n00:05:25 The decision boundary, then, is simply the set of these boundaries that border one value on one side and a different value on the other side.\n00:05:34 That will be the set of points that are transitioning from predicting a 1 to predicting a 0.\n00:05:42 It's easy to see that this decision boundary must be piecewise linear, since if we look at any pair of data points, we can think about what the set of points that are equidistant between them, which will be the point at which we transition from being closer to this 0 to that 1.\n00:05:51 If we draw a straight line between those two points,\n00:06:00 the points that are equidistant between them must lie on the perpendicular to that line.\n00:06:10 So, locally, this point is halfway between the two data points, and if we deviate slightly perpendicular to the line between them, we'll stay equidistant between those two points.\n00:06:20 Then, if we simply follow that line a little bit further, eventually we will become closer to this data point than we are to one of those two other points.\n00:06:30 At that point, our new boundary will be equidistant between a new pair of data points and will be perpendicular to the line that connects them.\n00:06:40 Continuing on, we see more boundary emerging as we draw the perpendiculars between more pairs of points until the entire boundary has been drawn.\n00:06:50 Again, the decision boundary will then...\n00:07:00 be the set of these lines that form a transition between a region where we would be predicting 1 and a region where we would be predicting 0.\n00:07:09 If we have more data points, this procedure stays essentially the same.\n00:07:18 But the decision boundary itself may get more complex because every pair of points produces a possible decision boundary being the line between them.\n00:07:27 So the more points we have, the more possible segments there are to form our decision boundary.\n00:07:36 Of course, this also depends on the layout of the points, but in general, for nearest neighbor methods, more data points can lead to a more complex decision boundary.", "start_char_idx": 3245, "end_char_idx": 6965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fbe23d1-72ad-4516-9c9b-c65f3b22feb8": {"__data__": {"id_": "1fbe23d1-72ad-4516-9c9b-c65f3b22feb8", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Nearest neighbor (2) k-nearest neighbor.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ZWygMcenuWM", "Link": "https://www.youtube.com/watch?v=ZWygMcenuWM"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e26737a4-a6e0-44fb-a472-ccbc0760a1e6", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Nearest neighbor (2) k-nearest neighbor.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ZWygMcenuWM", "Link": "https://www.youtube.com/watch?v=ZWygMcenuWM"}, "hash": "3c1c613859a8491a5e02627b811aefa87b16f2f38e1ae84a6e7b22e77943947e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9058178c-17c3-404e-b039-0bcf2cd0b162", "node_type": "1", "metadata": {}, "hash": "d3a84b6028f64c05a553602fe7ad78cd4892336fec285908e7e0d68425ca1043", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Here we'll look at a simple variant on nearest neighbor methods called k-nearest neighbors.\n00:00:10 For the k-nearest neighbor methods, instead of finding the single training point that's closest to our point of interest x, we instead find the k-nearest data points.\n00:00:20 In other words, we rank all of the training data points according to their distance from the new point x, and we just take the k that are closest.\n00:00:30 Then, using these k-closest points, we can use them to make a prediction.\n00:00:40 For regression, we usually just average the y-values, i.e., the target values associated with those k-closest examples.\n00:00:50 For classification, the y-values are discrete, so the ranking gives the k-closest and a set of k class labels associated with them, and we usually use a majority vote method, so we pick the class label that's most common from that set.\n00:01:00 For problems with only two classes, like binary classification problems, if we choose k to be odd, then there will never be any ties.\n00:01:10 Otherwise, we can choose a somewhat arbitrary tie-breaking technique.\n00:01:20 Note that training for k-nearest neighbor methods is trivial.\n00:01:30 There's no training procedure at all.\n00:01:40 We just store all of the training data in a database, and whenever a test point arrives, we search it to find the nearest points.\n00:01:50 Like the one-nearest neighbor classifier, the decision boundary of a k-nearest neighbor classifier is piecewise linear, since, again, if we're anywhere in this feature space, our decision can change only when the set of nearest training examples also changes, which must be at the midpoint between some member of the old set of closest points and the new point that we'll be joining.\n00:02:00 As we increase k, it tends to simplify the decision function and simplify the decision boundary in some sense.\n00:02:08 You can see that for k equals 1, we'll carve out, for example, little regions to surround each training example.\n00:02:17 As we go to higher values of k, those regions become less and less noticeable.\n00:02:25 As k increases further, those regions become smaller and smaller, and the decision boundary, while technically not any more simple to describe, takes on a simpler-looking shape.\n00:02:34 So by k equals 7, this region of blue here has completely disappeared, and the decision boundary here is starting to take on a smoother, simpler form.\n00:02:42 By k equals 25, the decision boundary is quite simple indeed.\n00:02:51 Another thing that's visible from these pictures...\n00:03:00 is the change in the training error rate as we increase the value of k.\n00:03:08 When we start off with a small k, say k equals 1, we can see that each one of the data points has a little region carved around it that predicts its color.\n00:03:17 By the time we get to a high k, like k equals 25, that's no longer true.\n00:03:25 Several of the red points have blue being predicted, and several of the blue points have red being predicted.\n00:03:34 So what's happening is, as a function of k, as we increase k, the error on our training data is starting to increase.\n00:03:42 In fact, it's easy to see that at k equals 1, all of the training data will be exactly predicted, since if we pick any training data to evaluate at, its closest point in the training set will be itself, and will predict its y value.\n00:03:51 So at this point, in this point of the curve as k increases, we've actually memorized the data completely.\n00:04:00 regurgitating any particular data point and its correct label.\n00:04:06 However, of course, this isn't a complete story.\n00:04:13 What we're really interested in is how well these predictors work on new data that they haven't seen before.\n00:04:20 If we look at that, we see a pretty different looking curve.\n00:04:26 The error on test data follows a different shape entirely.\n00:04:33 We see that at k equals 1, it might be high, but it tends to decrease with k until some point at which it begins to increase again.", "start_char_idx": 0, "end_char_idx": 4029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9058178c-17c3-404e-b039-0bcf2cd0b162": {"__data__": {"id_": "9058178c-17c3-404e-b039-0bcf2cd0b162", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Nearest neighbor (2) k-nearest neighbor.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ZWygMcenuWM", "Link": "https://www.youtube.com/watch?v=ZWygMcenuWM"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e26737a4-a6e0-44fb-a472-ccbc0760a1e6", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Nearest neighbor (2) k-nearest neighbor.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ZWygMcenuWM", "Link": "https://www.youtube.com/watch?v=ZWygMcenuWM"}, "hash": "3c1c613859a8491a5e02627b811aefa87b16f2f38e1ae84a6e7b22e77943947e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fbe23d1-72ad-4516-9c9b-c65f3b22feb8", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Nearest neighbor (2) k-nearest neighbor.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ZWygMcenuWM", "Link": "https://www.youtube.com/watch?v=ZWygMcenuWM"}, "hash": "16d97b351ffa443a0dcc607ba33dbcbf235283bce27bcb93eed6c033d94cb861", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3689ead-3f6f-4393-8383-1ac59c5fbe8e", "node_type": "1", "metadata": {}, "hash": "a92e63830e9db293256743aac3fb1bb21dccfbfce2e35624c996876a32e256d4", "class_name": "RelatedNodeInfo"}}, "text": "00:03:51 So at this point, in this point of the curve as k increases, we've actually memorized the data completely.\n00:04:00 regurgitating any particular data point and its correct label.\n00:04:06 However, of course, this isn't a complete story.\n00:04:13 What we're really interested in is how well these predictors work on new data that they haven't seen before.\n00:04:20 If we look at that, we see a pretty different looking curve.\n00:04:26 The error on test data follows a different shape entirely.\n00:04:33 We see that at k equals 1, it might be high, but it tends to decrease with k until some point at which it begins to increase again.\n00:04:40 So if we want to select the best value of k for the purposes of future data, we should pick the minimum of this green curve, the point at which the test data's error is lowest.\n00:04:46 This is an example of the complexity and overfitting tradeoff that we discussed in the introduction.\n00:04:53 In particular, a very complex model might be able to predict all of the training points well, but may not generalize very well to new data.\n00:05:00 data points.\n00:05:07 Here we see that effect, where at k equals 1, we can perfectly memorize all of the training examples, and our decision function is very complex.\n00:05:15 It carves out a region of a particular color around every single data point.\n00:05:22 If we increase k, that decision function becomes less complex looking, and in fact another extreme point is by the time we pick k equals m, the full size of the data set, our majority vote rule will always predict whatever class is in the majority in the training set, meaning that every single point in all of space will be predicted with the same value.\n00:05:30 So this is about as simple of a function as you could possibly get.\n00:05:37 So for the purposes of k nearest neighbor, k equals 1 is as complex of a function as you can choose, and k equals m simplifies.\n00:05:45 So increasing k simplifies up until k equals m.\n00:05:52 The way to choose k is...\n00:06:00 to use some sort of validation or test data so that you can evaluate this green curve and find out what value of k is going to generalize best to the future.\n00:06:08 A few theoretical considerations about the k-nearest neighbor methods.\n00:06:17 So as I just mentioned, as k increases, the effect is that every prediction at any particular point is averaging over a larger set of neighbors.\n00:06:25 And that makes the decision boundary more smooth and simple looking.\n00:06:34 Similarly, as n increases, our ability to have complex functions also increases, since the complexity of our boundary is a function of the number of data points.\n00:06:42 So typically, as n increases, the best value of k to choose tends to increase as well, but usually more slowly than n, usually something like log n.\n00:06:51 Another nice theoretical result about nearest neighbor methods is for the...\n00:07:00 for the simplest nearest neighbor method, the k equals one nearest neighbor method, for a sufficiently large number of data points, which may be not achievable in practice.\n00:07:10 But if you have data approaching infinity, it turns out that the error of the k nearest neighbor or the nearest neighbor classifier in this case is at most two times the best possible error of any predictor.\n00:07:20 So if you have enough data, the k nearest neighbor or rather the single nearest neighbor method might not do so badly.\n00:07:30 K nearest neighbor methods are very highly studied, have been around for a long time, and so there are many variations and extensions.\n00:07:40 A very simple one that I'll mention here are weighted distance measures, where perhaps some of the features might be more important or less important for the prediction process.\n00:07:50 So if features are irrelevant, we might want to give them no weight in the distance calculation.\n00:08:00 important and small differences in that feature are very important in determining how similar two things is, we might want to give them higher weight.\n00:08:08 So, a simple definition might be to use a weighted Euclidean distance.", "start_char_idx": 3387, "end_char_idx": 7505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3689ead-3f6f-4393-8383-1ac59c5fbe8e": {"__data__": {"id_": "e3689ead-3f6f-4393-8383-1ac59c5fbe8e", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Nearest neighbor (2) k-nearest neighbor.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ZWygMcenuWM", "Link": "https://www.youtube.com/watch?v=ZWygMcenuWM"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e26737a4-a6e0-44fb-a472-ccbc0760a1e6", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Nearest neighbor (2) k-nearest neighbor.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ZWygMcenuWM", "Link": "https://www.youtube.com/watch?v=ZWygMcenuWM"}, "hash": "3c1c613859a8491a5e02627b811aefa87b16f2f38e1ae84a6e7b22e77943947e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9058178c-17c3-404e-b039-0bcf2cd0b162", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Nearest neighbor (2) k-nearest neighbor.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ZWygMcenuWM", "Link": "https://www.youtube.com/watch?v=ZWygMcenuWM"}, "hash": "58f16b26e992dbd158edb11160e23a9e40085992b67bccc1ec273239a16aa734", "class_name": "RelatedNodeInfo"}}, "text": "00:07:20 So if you have enough data, the k nearest neighbor or rather the single nearest neighbor method might not do so badly.\n00:07:30 K nearest neighbor methods are very highly studied, have been around for a long time, and so there are many variations and extensions.\n00:07:40 A very simple one that I'll mention here are weighted distance measures, where perhaps some of the features might be more important or less important for the prediction process.\n00:07:50 So if features are irrelevant, we might want to give them no weight in the distance calculation.\n00:08:00 important and small differences in that feature are very important in determining how similar two things is, we might want to give them higher weight.\n00:08:08 So, a simple definition might be to use a weighted Euclidean distance.\n00:08:17 There are also a number of works on using fast methods so that the data size for nearest neighbor methods can be extremely large.\n00:08:25 Since the amount of time that you spend searching for the nearest neighbor has to be done for every prediction and depends on the size of the training set.\n00:08:34 If you want to use a very large training set, you need to use some algorithmic tricks to try to speed this up.\n00:08:42 So, there are things like approximate hashing methods and spatially oriented data structures to help you find the k-nearest neighbors quickly.\n00:08:51 So, in summary, k-nearest neighbor methods use the k-nearest examples to make predictions.\n00:09:00 Typically, in classification, we use a majority vote out of those k-nearest neighbors, and in regression, we use an average or possibly a weighted average to combine them.\n00:09:13 For classifiers, what we find is that the decision boundary that's induced by k-nearest neighbors is piecewise linear, and I showed a method of calculating it.\n00:09:26 And we saw how k affects the process of overfitting, so that as k increases, the decision function becomes simpler, so that increasing k can be used to control overfitting.\n00:09:40 In order to find the best k, we need to use some kind of data that the trainer hasn't seen, so we need to use validation data that we've split off beforehand or a test dataset to try to estimate this test error and use that to select the value of k that'll perform best in the future.", "start_char_idx": 6701, "end_char_idx": 9007, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65ec63f8-fd43-4e05-8f2d-82873c9eb735": {"__data__": {"id_": "65ec63f8-fd43-4e05-8f2d-82873c9eb735", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44025363-bc17-4899-ab4b-c6b259f41354", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "a4f250dd6dbfb9d844c1fa60744f885e1a9b121af2059bcaf55d74567ec97919", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3434e657-16e4-435a-a482-0d46aaf64678", "node_type": "1", "metadata": {}, "hash": "b28aa69c7efe8ed76bac54952644aeda4aafc885b66a8b0c6c72ec5c43588bf9", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 In these slides, we'll discuss some background on random variables and probability.\n00:00:07 In computer science, it's often tempting to think of everything as very predictable, with binary outcomes in every condition either true or false.\n00:00:15 But in machine learning, we're interfacing computers into data and the real world.\n00:00:22 And in that case, these things may become much more complicated.\n00:00:30 In fact, real world systems contain quite a bit of uncertainty.\n00:00:37 This uncertainty may be due to true inherent randomness, such as games of chance, or it might be due to overwhelming complexity in the system, or even just a lack of knowledge about how that complexity operates.\n00:00:45 Think of understanding traffic patterns in a large urban region.\n00:00:52 It might be that if we had perfect knowledge of everyone in the system, where they live, where they work, when they leave for work, how fast and how aggressively they drive, it might be that the traffic patterns would be no surprise at all and not be...\n00:01:00 random at all, but in fact that kind of information is impossible, and so it's far easier to sweep that kind of thing under the rug and simply call it random.\n00:01:08 When making predictions and decisions we simply can't get by without representing and reasoning about uncertainty.\n00:01:17 For example, consider trying to decide when to leave for the airport.\n00:01:25 There's a high cost to being late and a low cost to being early, so it's no good to just know the average amount of time it takes to get there.\n00:01:34 Instead we need to know the distribution of times so that we can select something that will accurately balance our need to not be late with our desire to not be too early.\n00:01:42 Systems tend to be like this.\n00:01:51 Unless we reason and represent and communicate uncertainty from one subsystem to the next, it'll be very easy for a later subsystem to make a bad decision because it's operating without complete knowledge of the\n00:01:59 uncertainty in the estimates of some other part.\n00:02:10 So given that we need to represent uncertainty in some way, why should we use probability?\n00:02:20 It turns out that there's a result from 1946 that under fairly mild conditions, any system that expresses a degree of belief can be mapped into probability space.\n00:02:30 In fact, the Bayesian viewpoint of probability explicitly takes this notion of degree of belief.\n00:02:40 Moreover, another classic result from De Finetti tells us that if we try to make bets with probabilities that do not follow the laws of probability, there's always a way to construct bets such that one is always beaten.\n00:02:50 So in particular, probability will give us a natural way to try to describe our assumptions about the system in concrete mathematical terms and give us rules for how to combine information and make predictions.\n00:03:00 in a concise and clear way.\n00:03:05 In probability, we'll think about an event A and some event space S.\n00:03:10 And the event A can be anything which might or might not happen.\n00:03:15 So for example, the statement, I'll have a hangover tomorrow, might or might not be true.\n00:03:20 That's an event.\n00:03:25 The statement, I have a hangover, might or might not be true, at least as far as you know.\n00:03:30 The statement, I have hantavirus, might or might not be true.\n00:03:35 These are all events and they live in some space of possibilities.\n00:03:40 The probability of some event, PR of A, essentially counts what the chance that A is true is.\n00:03:45 So you can think of this as perhaps being something like the number of universes or the number of worlds in which A would happen.\n00:03:50 This is essentially a measure, kind of like area.\n00:03:55 If you think of perhaps listing out every possible outcome that could possibly happen.\n00:04:00 this would be counting the number of such outcomes.\n00:04:08 You can sort of think of it in that term, and I'll draw things over here that indicate that kind of intuition.\n00:04:17 So the space S of everything that can possibly happen, and the space in which the outcome A is true is some subset of that space.", "start_char_idx": 0, "end_char_idx": 4173, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3434e657-16e4-435a-a482-0d46aaf64678": {"__data__": {"id_": "3434e657-16e4-435a-a482-0d46aaf64678", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44025363-bc17-4899-ab4b-c6b259f41354", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "a4f250dd6dbfb9d844c1fa60744f885e1a9b121af2059bcaf55d74567ec97919", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65ec63f8-fd43-4e05-8f2d-82873c9eb735", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "50ea15f74a664a713c12caa3f4db9653719a46fe2e39ffb35396f9766dac644a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72b188f8-cdff-43b0-9b9c-77022445092c", "node_type": "1", "metadata": {}, "hash": "c3d5b25f18c03dc4318aeb1c9039945ef8a5e528f58dfda5a596597cb4f98104", "class_name": "RelatedNodeInfo"}}, "text": "00:03:35 These are all events and they live in some space of possibilities.\n00:03:40 The probability of some event, PR of A, essentially counts what the chance that A is true is.\n00:03:45 So you can think of this as perhaps being something like the number of universes or the number of worlds in which A would happen.\n00:03:50 This is essentially a measure, kind of like area.\n00:03:55 If you think of perhaps listing out every possible outcome that could possibly happen.\n00:04:00 this would be counting the number of such outcomes.\n00:04:08 You can sort of think of it in that term, and I'll draw things over here that indicate that kind of intuition.\n00:04:17 So the space S of everything that can possibly happen, and the space in which the outcome A is true is some subset of that space.\n00:04:25 Probability is defined by a relatively few number of axioms.\n00:04:34 The first axiom tells us that the probability of any event A must be greater than zero, greater than or equal to zero, and that just tells us that the size of this set in which A is true cannot get any smaller than size zero, in which case there are no worlds or universes in which A is true.\n00:04:42 Similarly, the probability of an event A cannot get any larger than one.\n00:04:51 We define the size of the entire space.\n00:04:59 S to be 1.\n00:05:07 And the set in which A is true can't get any larger than the set of all possible worlds.\n00:05:14 And so 100% of those might have A true, in which case the probability of A would be at most 1.\n00:05:22 The last axiom tells us how to compute the probability of overlaps or unions of regions.\n00:05:29 Particular, it says that the probability of the union of two events, A union B, is the sum of their individual probabilities minus the sum of their intersection.\n00:05:37 Again, an intuition resorting to area will help us out here.\n00:05:44 Considering the set A here of worlds in which event A is true, another set B of worlds in which B is true, then the area of A union B is simply the area of A plus the area of B minus the area of their intersection.\n00:05:52 For the most part, we'll be focusing on random variables.\n00:05:59 which are a particularly useful representation of events.\n00:06:08 A random variable X you can think of as taking on a finite set of values, say S, partitioning into A1 through AD.\n00:06:17 The outcomes of X partition the space S into a disjoint and exhaustive set.\n00:06:25 What this means is that X will take on one and exactly one of these possible outcomes.\n00:06:34 So over here I have a diagram of our entire event space S and it's been partitioned into the set in which X takes on value 1, value 2, and value 3.\n00:06:42 We can then define a probability mass function or PMF that defines a measure on this subset of S and in particular we can do that simply by defining the probability that random variable X takes on each value for each of these values.\n00:06:51 And then because these sets are disjoint, the probability...\n00:06:59 probability that X takes on a value in any subset will simply be the sum of the probabilities of its individual possible values.\n00:07:06 So this tells us that we can define the probability mass function on a variable X simply by defining the probabilities associated with each one of its possible outcomes.\n00:07:13 And these things have a few constraints.\n00:07:19 So the probability of any given outcome must be greater than or equal to zero for the same reason as before.\n00:07:26 They must be less than or equal to one.\n00:07:33 And the sum over all possible outcomes must be equal to one.\n00:07:39 So this tells us the probability of all outcomes totally must be one and all of them must be positive.\n00:07:46 It will also be useful to think about more than one variable at a time.\n00:07:53 So first of all, we'll typically abbreviate the probability that random variable to X takes on value little x.\n00:08:00 as just P of X.", "start_char_idx": 3381, "end_char_idx": 7311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72b188f8-cdff-43b0-9b9c-77022445092c": {"__data__": {"id_": "72b188f8-cdff-43b0-9b9c-77022445092c", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44025363-bc17-4899-ab4b-c6b259f41354", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "a4f250dd6dbfb9d844c1fa60744f885e1a9b121af2059bcaf55d74567ec97919", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3434e657-16e4-435a-a482-0d46aaf64678", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "5e6650246ee6c6d2d383b618573eb9e2d06614f167785197ccf9ac6f5db1a0b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "875d49c4-5294-4efa-a5c9-d540ddbb1170", "node_type": "1", "metadata": {}, "hash": "3504c719f11ba4fcef464421ae40bb8f42d0bcb0eb2badc8b8899a827437fb1e", "class_name": "RelatedNodeInfo"}}, "text": "00:07:06 So this tells us that we can define the probability mass function on a variable X simply by defining the probabilities associated with each one of its possible outcomes.\n00:07:13 And these things have a few constraints.\n00:07:19 So the probability of any given outcome must be greater than or equal to zero for the same reason as before.\n00:07:26 They must be less than or equal to one.\n00:07:33 And the sum over all possible outcomes must be equal to one.\n00:07:39 So this tells us the probability of all outcomes totally must be one and all of them must be positive.\n00:07:46 It will also be useful to think about more than one variable at a time.\n00:07:53 So first of all, we'll typically abbreviate the probability that random variable to X takes on value little x.\n00:08:00 as just P of X.\n00:08:07 This is a slight abusive notation, but it should be fairly clear from the notation.\n00:08:15 So each variable represents a partitioning of the entire event space.\n00:08:22 So random variable X partitions the space into the outcomes where it takes on value one, value two, or value three.\n00:08:30 A different random variable Y might partition the space into areas where it takes on values B1, B2, and B3.\n00:08:37 And so then the joint distribution is a probability distribution over X and Y together.\n00:08:45 And by P of XY, we mean the probability that random variable X takes on value little x and random variable Y takes on value little y.\n00:08:52 And since each of these are partitionings of the space S, X equals X and Y equals Y together is some finer partitioning of S in which some.\n00:09:00 region of the graph has X1 equal to A and Y1 equal to B.\n00:09:07 Another region of the graph might have X1 equal to A and Y equal to B, too, and so forth.\n00:09:15 So the joint configuration of several variables can itself be thought of as a partitioning of S.\n00:09:22 Similarly, we can write a probability mass function for X and Y together.\n00:09:30 In this case, we call it the joint distribution over X and Y.\n00:09:37 We can express it as a table of joint probability values where each entry of that table corresponds to the probability that X takes on value X and Y takes on value Y.\n00:09:45 Similarly to before, each of these entries will be positive, and the sum over the entire table must be 1.\n00:09:52 The law of total probability tells us how to relate\n00:10:00 the probability of just one variable P of X to this joint probability distribution table.\n00:10:12 In particular, the probability that X takes on value little x will be the sum over all possible values of Y of the joint probability.\n00:10:24 The intuition here is that some value of Y must occur, and if we don't know which one, we can simply sum over them since their events are disjoint.\n00:10:36 So the probability of this outcome, the probability associated with set little x, will simply be the probability that X and Y equals 0 happen, or that X and Y equals 1 happen, or so on and so forth.\n00:10:48 So in particular, if some point in this table is the joint probability of X and Y, then the entire column here summed together will be P of X, and the entire row summed together will be P of Y.\n00:11:00 A useful concept is that two variables might be independent.\n00:11:12 Independent variables have this table P of x, y is in fact equal to a product of a function only of x and a function only of y.\n00:11:24 Similarly, we can define a conditional probability via a chain rule that the joint probability of x and y together is the marginal probability of x times the conditional probability of y given x.\n00:11:36 So this says that we can decompose the probability of x and y both occurring into the probability that x occurs and any y times the probability that y occurs given that x.\n00:11:48 More generally, the chain rule can be expressed in a repeated fashion, so the probability of say three variables together, x, y, z, will be the probability of one, say x, times the probability of y.", "start_char_idx": 6508, "end_char_idx": 10502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "875d49c4-5294-4efa-a5c9-d540ddbb1170": {"__data__": {"id_": "875d49c4-5294-4efa-a5c9-d540ddbb1170", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44025363-bc17-4899-ab4b-c6b259f41354", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "a4f250dd6dbfb9d844c1fa60744f885e1a9b121af2059bcaf55d74567ec97919", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72b188f8-cdff-43b0-9b9c-77022445092c", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "fb5efdd4c7fcad7fdbb34c2a613fc2e92ec1480958c6e96dc566a460190e5f8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "957ee664-a093-4a05-9dde-66c96ebded45", "node_type": "1", "metadata": {}, "hash": "c304192e6c986ca76dd73801a253041637a536799aa75de02c814bd52db0c5c1", "class_name": "RelatedNodeInfo"}}, "text": "00:11:00 A useful concept is that two variables might be independent.\n00:11:12 Independent variables have this table P of x, y is in fact equal to a product of a function only of x and a function only of y.\n00:11:24 Similarly, we can define a conditional probability via a chain rule that the joint probability of x and y together is the marginal probability of x times the conditional probability of y given x.\n00:11:36 So this says that we can decompose the probability of x and y both occurring into the probability that x occurs and any y times the probability that y occurs given that x.\n00:11:48 More generally, the chain rule can be expressed in a repeated fashion, so the probability of say three variables together, x, y, z, will be the probability of one, say x, times the probability of y.\n00:12:00 of y given x times the probability of z given everything up to that point, so x and y.\n00:12:07 Or the probability of four variables could be expressed in a similar expansive way.\n00:12:15 P of x, P of y given x, P of z given x and y, P of w given x, y, and z.\n00:12:22 So each term involves conditioning on everything that's come before.\n00:12:30 So think of this intuition here, that the probability that x occurs times the probability that y occurs given what we've accepted so far times the probability that z occurs given that we've accepted so far.\n00:12:37 A few simple examples of classic probability mass functions.\n00:12:45 A Bernoulli random variable is just a binary outcome like a coin toss.\n00:12:52 So a random variable x might take on values say 0 and 1 or heads and tails.\n00:13:00 case, since the probability mass function has only two possible values and they sum to one, it's parameterized by just one number.\n00:13:07 So we usually write that the probability that X takes on value one is, say, P, and then the probability that X takes on its other value, zero, is just one minus P.\n00:13:15 So we'll assure this that they're both positive and sum to one.\n00:13:22 Another common example is the binomial distribution.\n00:13:30 So binomial distribution corresponds to a collection of Bernoulli random variables summed together.\n00:13:37 So, for example, if we were to flip the coin multiple times, we could ask, say, how many outcomes came up heads, rather than whether a single outcome did.\n00:13:45 An extension of Bernoulli to multivariate outcomes, for example, dice, is the discrete or categorical distribution with, say, d outcomes.\n00:13:52 And here, instead of\n00:14:00 If x being only 0 and 1, it can be 0 through, say, d minus 1, so that there are d possible outcomes.\n00:14:07 And in that case, instead of having one value, we typically write it down as a vector of probabilities, p0 through pd minus 1, with the constraint that those numbers all sum to 1.\n00:14:15 There's similarly a generalization of binomial called the multinomial, which is like rolling a die multiple times and counting how many times it came up with each value.\n00:14:22 Of course, in any machine learning class, we're primarily interested in estimating quantities from data.\n00:14:30 So what do we do if we get a couple of observations?\n00:14:37 How do we estimate these probabilities?\n00:14:45 Typical way is to use something called the likelihood function and to find a maximum likelihood estimator.\n00:14:52 So the likelihood of written p of x parameterized by theta, some parameter.\n00:15:00 It's very much like a probability distribution, except that the data, the outcome x, is fixed, and the parameter theta is what's varied.\n00:15:10 So consider, for example, the Bernoulli outcome.\n00:15:20 If we saw, if we had a single coin flip, and we observed that it was equal to zero, then the probability of that outcome is simply one minus theta, where theta is the probability of it getting, coming up heads.", "start_char_idx": 9702, "end_char_idx": 13527, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "957ee664-a093-4a05-9dde-66c96ebded45": {"__data__": {"id_": "957ee664-a093-4a05-9dde-66c96ebded45", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44025363-bc17-4899-ab4b-c6b259f41354", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "a4f250dd6dbfb9d844c1fa60744f885e1a9b121af2059bcaf55d74567ec97919", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "875d49c4-5294-4efa-a5c9-d540ddbb1170", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "8fc826ae53a4b112226dc04b7a87424a7a64e52adc9d5e18a0c0ed9b5995b21f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c9b09bb-01bf-49d0-a246-c0da6c9d834e", "node_type": "1", "metadata": {}, "hash": "34a5c074c749d50679ed3e13f0b0c39c72ae0c2d9b40c3ead64c11d2ac389205", "class_name": "RelatedNodeInfo"}}, "text": "00:14:30 So what do we do if we get a couple of observations?\n00:14:37 How do we estimate these probabilities?\n00:14:45 Typical way is to use something called the likelihood function and to find a maximum likelihood estimator.\n00:14:52 So the likelihood of written p of x parameterized by theta, some parameter.\n00:15:00 It's very much like a probability distribution, except that the data, the outcome x, is fixed, and the parameter theta is what's varied.\n00:15:10 So consider, for example, the Bernoulli outcome.\n00:15:20 If we saw, if we had a single coin flip, and we observed that it was equal to zero, then the probability of that outcome is simply one minus theta, where theta is the probability of it getting, coming up heads.\n00:15:30 So rather than regarding this as a function of x, where x could be zero and one, and they sum to one, we now regard it explicitly as a function of theta, and we fix the outcome that the first, that the coin flip came up tails.\n00:15:40 If we plot this as a function of theta, it's just the one minus theta line, so it's just this red descending line.\n00:15:50 Now imagine that we got, say, two observations.\n00:16:00 So, our first coin flip came up tails, and our second coin flip came up heads.\n00:16:10 And suppose these are independent coin flips, then the probability of these two outcomes is 1 minus theta for the tails times theta for the heads.\n00:16:20 If we plot this function as a function of theta, we get this blue curve here.\n00:16:30 Similarly, if we observed another heads, we would now have 2, the product of 2 theta terms times 1, 1 minus theta term, and so we get theta squared times 1 minus theta, and that would be this green curve here as you trace out theta between 0 and 1.\n00:16:40 What you notice is that all of these functions have their maximal value at the empirical estimate of P.\n00:16:50 So, for instance, the maximum of this function is simply at 0, and the empirical estimate is that we've gotten 1 tails for our only shot.\n00:17:00 here is at a half, and we've seen exactly half of the outcomes be heads.\n00:17:06 The maximum of this function, the green one, is at two-thirds, and we've seen exactly two out of three of the outcomes be heads.\n00:17:13 So in fact, the maximum likelihood estimator for a Bernoulli random variable is simply the empirical estimate.\n00:17:20 A histogram, which is a nice way of visualizing data, is also, in some sense, an empirical estimate of probabilities.\n00:17:26 Histograms estimate a probability mass function of a variable by binning the outcomes up, so it estimates the probability of the value falling within some bin region, so a particular interval.\n00:17:33 And you could think of this, again, as a maximum likelihood estimate of a probability.\n00:17:40 This is the fraction of data that fell within that interval.\n00:17:46 Histograms are very easy to do in MATLAB.\n00:17:53 You can simply define the bin position.\n00:18:00 and call the function HIST in order to construct it.\n00:18:08 Continuous random variables are a bit more subtle.\n00:18:17 Again, consider a disjoint and exhaustive partitioning of S, but now create one that's of some increasingly fine-detailed delta.\n00:18:25 So each bin is going to be of size delta, and there will be some large number of bins out of all the possible outcomes.\n00:18:34 So we know that since we've constructed this disjoint and exhaustive partitioning, if we want the probability of any outcome of a set, say some interval of this variable, then what we should do is sum together all of the bins that make up that interval and add together their probabilities.\n00:18:42 But we have this effect that as the bin size starts to go to zero, the probability of falling within any particular bin also decreases to zero.\n00:18:51 So bins that are becoming infinitesimally thin have incredible...\n00:19:00 probability of ending up in that bin.", "start_char_idx": 12792, "end_char_idx": 16692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c9b09bb-01bf-49d0-a246-c0da6c9d834e": {"__data__": {"id_": "8c9b09bb-01bf-49d0-a246-c0da6c9d834e", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44025363-bc17-4899-ab4b-c6b259f41354", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "a4f250dd6dbfb9d844c1fa60744f885e1a9b121af2059bcaf55d74567ec97919", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "957ee664-a093-4a05-9dde-66c96ebded45", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "22cd064be9367df8c669f425322d3ac08eda9dcb2f6dc64eb657e6a738a009c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7e0e680-382b-4c10-99b3-1cddbe2f63a5", "node_type": "1", "metadata": {}, "hash": "eb174c55c5f7282a5b25c01703423901f9fac10768cb659f1733113bb68a1b9c", "class_name": "RelatedNodeInfo"}}, "text": "00:18:17 Again, consider a disjoint and exhaustive partitioning of S, but now create one that's of some increasingly fine-detailed delta.\n00:18:25 So each bin is going to be of size delta, and there will be some large number of bins out of all the possible outcomes.\n00:18:34 So we know that since we've constructed this disjoint and exhaustive partitioning, if we want the probability of any outcome of a set, say some interval of this variable, then what we should do is sum together all of the bins that make up that interval and add together their probabilities.\n00:18:42 But we have this effect that as the bin size starts to go to zero, the probability of falling within any particular bin also decreases to zero.\n00:18:51 So bins that are becoming infinitesimally thin have incredible...\n00:19:00 probability of ending up in that bin.\n00:19:06 So instead of computing something like the probability mass function, which would list out the probabilities of each outcome.\n00:19:13 We instead define a density function p of x.\n00:19:20 And this is just the ratio of the probability of falling in bin i of size delta, divided by the size of delta.\n00:19:26 And as delta goes to zero, this ratio will not go to zero.\n00:19:33 It will converge to a relative scale value, which is the probability mass function, excuse me, probability density function.\n00:19:40 Then we can define the probabilities of outcomes or events A, which are sets of values, the same as before.\n00:19:46 So if A is some interval or set of intervals of x, the probability of falling within region A is simply the integral of the probability density function over that region.\n00:19:53 A subtle\n00:20:00 point here is that the density function P of X is not restricted in quite the same ways that the probability mass function is.\n00:20:07 In particular, it can have values that are greater than 1.\n00:20:15 Still all be non-negative, of course.\n00:20:22 What happens is P of X can be greater than 1 on a region with a very small area, so that the integral of P of X over that region will still be less than 1.\n00:20:30 The restriction of the axioms of probability was on the probability of an outcome, and so we just need for this integral of the mass function to be always less than 1.\n00:20:37 So for example, a Gaussian distribution with a very thin variance can rise to a high value.\n00:20:45 So here the peak value is something like 4, but if I actually integrated this function, I would find that its area was still 1.\n00:20:52 The Gaussian distribution is, of course, the most classic form of a continuous...\n00:21:00 random variable and probability density function.\n00:21:10 The Gaussian, or sometimes the normal distribution in one dimension, so over a single scalar random variable X, is parameterized by two numbers, the mean and the standard deviation, sigma, or sometimes the variance, sigma squared.\n00:21:20 And it's just written in this form.\n00:21:30 So it's an exponential e to the minus x minus the mean squared divided by sigma squared.\n00:21:40 The mean tells us the center point of this symmetric bell-shaped curve and the standard deviation, sigma, tells us something about the width of that curve.\n00:21:50 Perhaps not surprisingly, if you write down the likelihood function as a function of mu and sigma and find its maximum given some observations, x1 through xn, you'll find that the maximum likelihood estimate of the mean is simply the empirical mean.\n00:22:00 of the data.\n00:22:06 So if you want the maximum likelihood estimate, just take the empirical mean.\n00:22:12 And the maximum likelihood estimate of the variance is the empirical variance.\n00:22:18 So take each data point, subtract the empirical mean, and square it, and then average over those data points.\n00:22:24 The Gaussian distribution can also be extended to multivariate distributions fairly easily.\n00:22:30 So in this case, we define a distribution over a vector X, and it also is parameterized by mean, and now by a covariance.\n00:22:36 The mean is a vector of the same size as X.", "start_char_idx": 15851, "end_char_idx": 19907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7e0e680-382b-4c10-99b3-1cddbe2f63a5": {"__data__": {"id_": "f7e0e680-382b-4c10-99b3-1cddbe2f63a5", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44025363-bc17-4899-ab4b-c6b259f41354", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "a4f250dd6dbfb9d844c1fa60744f885e1a9b121af2059bcaf55d74567ec97919", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c9b09bb-01bf-49d0-a246-c0da6c9d834e", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}, "hash": "200bf788e8a6ae268e6d61d3f8a6d4cfa85fc08280bd8c8f119809620e0c9645", "class_name": "RelatedNodeInfo"}}, "text": "00:21:50 Perhaps not surprisingly, if you write down the likelihood function as a function of mu and sigma and find its maximum given some observations, x1 through xn, you'll find that the maximum likelihood estimate of the mean is simply the empirical mean.\n00:22:00 of the data.\n00:22:06 So if you want the maximum likelihood estimate, just take the empirical mean.\n00:22:12 And the maximum likelihood estimate of the variance is the empirical variance.\n00:22:18 So take each data point, subtract the empirical mean, and square it, and then average over those data points.\n00:22:24 The Gaussian distribution can also be extended to multivariate distributions fairly easily.\n00:22:30 So in this case, we define a distribution over a vector X, and it also is parameterized by mean, and now by a covariance.\n00:22:36 The mean is a vector of the same size as X.\n00:22:42 The covariance is a symmetric matrix that's square, and of the same size as X on each side.\n00:22:48 And it's defined quite similar.\n00:22:54 It's e to the minus something, where instead of X minus mu squared over sigma squared, it now becomes the vector X minus mu, times the inverse covariance\n00:23:00 matrix times the vector x minus mu.\n00:23:07 So this is a dot product defined by matrix inverse sigma squared.\n00:23:15 Just like in 1D, where it has a familiar bell-shaped curve, the Gaussian is also still unimodal and has a nice bell-shaped curve, but in this case, it will be a 2D function rather than a 1D function.\n00:23:22 In summary, the axioms of probability give us a concrete way to represent uncertainty and reason about it as we observe information, update our beliefs.\n00:23:30 Random variables define variables whose outcome is potentially known.\n00:23:37 For variables with discrete numbers of outcomes, we can use probability mass functions to represent our probability distributions.\n00:23:45 These will be vectors of numbers whose values will be positive and whose total will be 1, with one value for each possible outcome of the variable.\n00:23:52 Examples.\n00:24:00 include the Bernoulli random variables, which are coin tosses, and discrete or categorical random variables, which are things like rolling dice.\n00:24:12 We also saw the concepts of joint distributions over more than one variable, say x and y.\n00:24:24 The law of total probability taught us how to calculate the probability of just one outcome, say p of x, by summing over all possible values of y in the joint probability p of x and y together.\n00:24:36 We also saw the chain rule, which defined conditional probability, by telling us how to calculate it in terms of the joint distribution p of x and y, and the marginal distribution p of x.\n00:24:48 For continuous variables, similar concepts apply, but in this case they form probability density functions, which tell us the limiting ratio of the probability to the size of the area, the most common example of which is the Gaussian.\n00:25:00 distribution, a simple bell-shaped curve.", "start_char_idx": 19048, "end_char_idx": 22046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e035af73-c916-403a-b52f-9683e76d0f68": {"__data__": {"id_": "e035af73-c916-403a-b52f-9683e76d0f68", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ivBSZZyaRHY", "Link": "https://www.youtube.com/watch?v=ivBSZZyaRHY"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea3fb7fe-a2b7-4d16-a969-fc97e37328ee", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ivBSZZyaRHY", "Link": "https://www.youtube.com/watch?v=ivBSZZyaRHY"}, "hash": "7d359be2d6e5b53f900454cd4f12ffc532eb287d686bf14818b2a102f3577ac1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c180bb5-173e-47f4-96ff-f5d0479a1d51", "node_type": "1", "metadata": {}, "hash": "2aeb27305e0ada28475b57986cee5e5e316fdd5680db6336cd356a6ff4f27833", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Next, we'll look at a technique for classification with a more functional form and with motivation rooted in probability theory called Bayes classifiers.\n00:00:06 Let's start by considering a basic classifier, where we'd like to predict a discrete-valued Y, let's say binary, using some feature X.\n00:00:13 And here we'll assume that the feature X is a discrete variable.\n00:00:20 We have a collection of training data consisting of joint observations of that feature and the outcome Y that we'd like to predict.\n00:00:26 And the classifier is just a mapping from observed values of X into predicted values of Y.\n00:00:33 So, in this case, we can think of this function, our classifier, as being a contingency table.\n00:00:40 For every value of X, we're going to have some prediction Y.\n00:00:46 So, here's an example, maybe.\n00:00:53 Next, we'll have a credit rating problem, where we'd like to predict the credit quality or the risk of...\n00:00:59 some user as being either bad or good, so 0 or 1.\n00:01:10 And we'll just get a simple discrete measurement X that's a discretized version of their income into just categories low, medium, or high.\n00:01:20 So we might have a collection of data which we can simply assemble in a table right here where we have 42 observations of people in the low income whose credit turned bad, meaning they defaulted, and 15 people in that group who did not default, so they should have good credit.\n00:01:30 If we just list all of our data points, we get entries in that table for every value of X and the outcome of Y that's associated with each data point.\n00:01:40 So this is the number of entries in our data set that matched condition income low and poor risk.\n00:01:50 So how can we use this to make the most\n00:02:00 number of correct predictions.\n00:02:08 It's pretty clear that if we were to just choose the outcome that has more data associated with it, that would minimize the number of incorrect predictions we would have on the training data.\n00:02:17 So, for instance, for X equals zero, low income, we had 42 examples where we'd like to predict not to make a loan, and 15 examples where we would have liked to predict to make a loan.\n00:02:25 So if we want to minimize the number of errors we make, we should predict bad, because we'll get 42 of our training data correct and 15 of our training data incorrect.\n00:02:34 Similarly, for each line of this table, X equals one, we could choose the one with the most examples, most training examples, and X equals two, we could choose the outcome with the most training examples.\n00:02:42 Instead of considering the number of training examples, it's often useful to think of this in terms of probability.\n00:02:51 So if we\n00:03:00 normalize each row, we have then the probability that Y equals bad given that we observe X equals 0 empirically in the data.\n00:03:07 So within the data 73.7 percent of the time when we observe X equals 0, we have that Y equals bad.\n00:03:15 And 26 percent of the time when we observe X equals 1, we have Y equals good.\n00:03:22 So this is a table of entries whose probability is the probability of that outcome Y given our observation X.\n00:03:30 And again our decision, our actual classifier, is just choosing the most likely of these Y's.\n00:03:37 So we'll use this to generalize beyond discrete features in a form called the Bayes classifier.\n00:03:45 To start let me describe Bayes rule, which is a rule from probability theory that tells us how to combine the probabilities of different events to calculate conditional probabilities.\n00:03:52 So suppose we have two events were\n00:04:00 One is that I wake up with a headache, and the other is that I have the flu.\n00:04:05 And we have probabilities associated with this.\n00:04:10 So we know that about a tenth of the time, I have a headache.\n00:04:15 More rarely, about one time in 40, I actually have the flu.", "start_char_idx": 0, "end_char_idx": 3903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c180bb5-173e-47f4-96ff-f5d0479a1d51": {"__data__": {"id_": "8c180bb5-173e-47f4-96ff-f5d0479a1d51", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ivBSZZyaRHY", "Link": "https://www.youtube.com/watch?v=ivBSZZyaRHY"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea3fb7fe-a2b7-4d16-a969-fc97e37328ee", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ivBSZZyaRHY", "Link": "https://www.youtube.com/watch?v=ivBSZZyaRHY"}, "hash": "7d359be2d6e5b53f900454cd4f12ffc532eb287d686bf14818b2a102f3577ac1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e035af73-c916-403a-b52f-9683e76d0f68", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ivBSZZyaRHY", "Link": "https://www.youtube.com/watch?v=ivBSZZyaRHY"}, "hash": "7f9c6d465a6fd21885d62269cc79efe8fb8d6814ed08ce11cea7f4173c848bca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b3d0846-c6bd-4b63-812d-ff0d33d2d739", "node_type": "1", "metadata": {}, "hash": "91aefb84e245030cbe326355768169ae1b036877dd8a4c1723fb224ea509c291", "class_name": "RelatedNodeInfo"}}, "text": "00:03:30 And again our decision, our actual classifier, is just choosing the most likely of these Y's.\n00:03:37 So we'll use this to generalize beyond discrete features in a form called the Bayes classifier.\n00:03:45 To start let me describe Bayes rule, which is a rule from probability theory that tells us how to combine the probabilities of different events to calculate conditional probabilities.\n00:03:52 So suppose we have two events were\n00:04:00 One is that I wake up with a headache, and the other is that I have the flu.\n00:04:05 And we have probabilities associated with this.\n00:04:10 So we know that about a tenth of the time, I have a headache.\n00:04:15 More rarely, about one time in 40, I actually have the flu.\n00:04:20 But we know that when I have the flu, I often have a headache.\n00:04:25 So the probability of having a headache given that I am sick with the flu is a half.\n00:04:30 What we're really interested in is computing the inverse probability.\n00:04:35 So given that I observe a syndrome, like I have a headache, what's the probability of my underlying cause, that I have the flu?\n00:04:40 We can calculate this in two steps.\n00:04:45 First, we'll calculate the probability of having both a headache and the flu.\n00:04:50 And then we'll use that to calculate the probability of having the flu, given that we observe that I have a headache.\n00:04:55 By the chain rule of probability, the probability of the outcome, headache and flu.\n00:05:00 It's just the probability that I have a flu times the probability that I have a headache given the flu.\n00:05:08 And both of these numbers were given in the problem set up, so the probability of having the flu is 1 in 40, and the probability of a headache given a flu is a half, so I can compute this joint probability as 1 in 80.\n00:05:17 Then, I can use a similar representation to calculate the probability of having the flu given the headache.\n00:05:25 So, that is the probability that I have both, normalized by the probability of having a headache.\n00:05:34 So, we just plug in the probability of a headache and flu, and divide by probability of headache, and we find that it's 1 eighth.\n00:05:42 In other words, while I relatively rarely have the flu, 1 in 40th, once I observe my symptom of a headache, it becomes more likely, up to 1 eighth.\n00:05:51 In other words, given a model of how flus cause headaches, and some background probabilities, I can...\n00:06:00 compute, given an observation like a headache, what's the probability of some underlying unobservable cause that I have the flu?\n00:06:06 So how do we use this for classification?\n00:06:13 Well, we can simply assume that the class is the underlying cause of interest.\n00:06:20 So we can learn a probability of each possible class outcome given no observations at all.\n00:06:26 So this is, for instance, the fraction of the applicants that have good or bad credit.\n00:06:33 And then, for each of these classes, for good credit and for bad credit, we can learn a model for our observations X.\n00:06:40 So how likely are we to see this particular feature outcome in users with good credit, and similarly, how likely are we in users with bad credit?\n00:06:46 Then, just using the chain rule of probability, we can expand the joint probability of X and Y in two different ways.\n00:06:53 And since we're interested...\n00:07:00 estimating the probability of our class given our observation, we can move P of X over to this side, and this is precisely Bayes' rule.\n00:07:10 So the probability of our class given our observation can be expressed as the probability of the observation given the class times the probability overall of each class outcome divided by the probability of that measurement.\n00:07:20 And P of X, it's often convenient to expand using the law of complete probability.", "start_char_idx": 3176, "end_char_idx": 7001, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b3d0846-c6bd-4b63-812d-ff0d33d2d739": {"__data__": {"id_": "7b3d0846-c6bd-4b63-812d-ff0d33d2d739", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ivBSZZyaRHY", "Link": "https://www.youtube.com/watch?v=ivBSZZyaRHY"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea3fb7fe-a2b7-4d16-a969-fc97e37328ee", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ivBSZZyaRHY", "Link": "https://www.youtube.com/watch?v=ivBSZZyaRHY"}, "hash": "7d359be2d6e5b53f900454cd4f12ffc532eb287d686bf14818b2a102f3577ac1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c180bb5-173e-47f4-96ff-f5d0479a1d51", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ivBSZZyaRHY", "Link": "https://www.youtube.com/watch?v=ivBSZZyaRHY"}, "hash": "8f5cef81ff683733b41d3be8cd813d36259dcb8b73caf0bcda3d14818e8c5cce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4213762f-d15f-480f-b4ce-c48c0fa7a006", "node_type": "1", "metadata": {}, "hash": "7cc1fad907a67e8e99b5127124dc39006fe92c2a248f9fa5e9b354d25cd9c5c8", "class_name": "RelatedNodeInfo"}}, "text": "00:06:40 So how likely are we to see this particular feature outcome in users with good credit, and similarly, how likely are we in users with bad credit?\n00:06:46 Then, just using the chain rule of probability, we can expand the joint probability of X and Y in two different ways.\n00:06:53 And since we're interested...\n00:07:00 estimating the probability of our class given our observation, we can move P of X over to this side, and this is precisely Bayes' rule.\n00:07:10 So the probability of our class given our observation can be expressed as the probability of the observation given the class times the probability overall of each class outcome divided by the probability of that measurement.\n00:07:20 And P of X, it's often convenient to expand using the law of complete probability.\n00:07:30 So P of X is simply the probability of X and Y jointly having summed over all possible outcomes Y, and the probability of P of X and Y jointly we can write in terms of P of X given Y and P of Y again.\n00:07:40 So we'll get the denominator will be a sum over all possible outcomes Y of the product of those two terms.\n00:07:50 So the standard way of learning a Bayes classifier is to...\n00:08:00 do these steps.\n00:08:08 So, we first learn a model of P of Y, the probability of each class outcome, and then within each class, we estimate a separate probability model, P of X given Y for that particular class.\n00:08:17 And we can do that just by splitting up the training data into all the data in which that class occurs, and then learning the model of the feature.\n00:08:25 So, if we do this for this example training set, we just look over all the examples where we saw bad and good credit.\n00:08:34 We see that P of Y is the fraction of those outcomes with bad credit or the fraction of good credit.\n00:08:42 And we can also calculate for each of those conditioned on outcome Y equals 0 or outcome Y equals 1, what's the fraction of times we saw each outcome X.\n00:08:51 And then following Bayes' rule, we get exactly the same table we saw before where we've now computed the probability of each class given that observation.\n00:09:00 So for discrete measurements x, this is essentially producing the same table we saw before.\n00:09:06 The advantage now is that we can use this on non-discrete variables and in other cases as well using generalizations of the model.\n00:09:12 So we don't need to restrict ourselves to tabular models of p of x given y.\n00:09:18 We can learn any kind of model we'd like.\n00:09:24 So if our x is, say, continuous, we can use any density estimate we'd like for p of x given y.\n00:09:30 For instance, we might learn a histogram model.\n00:09:36 So given the blue class, we might learn some histogram distribution of x's.\n00:09:42 Given the red class, we might learn a different histogram model.\n00:09:48 We might also learn Gaussian models for each class simply by estimating the parameters of that Gaussian using the training data.\n00:09:54 So for instance, here we might estimate the probability of each class by using the fraction of...\n00:10:00 data in which we saw a class one divided by the total number of data.\n00:10:08 And for each class, we would then estimate the parameters of, say, a Gaussian model.\n00:10:17 So we take all the data with class zero, here the red class, and use it to estimate the mean and variance of those data.\n00:10:25 And then separately, we would take all the data where class one occurred.\n00:10:34 We would use those to estimate the mean and variance of the class one data.\n00:10:42 And this would give us two models for P of X given class zero, P of X given class one, and we can apply Bayes' rule to calculate P of each class given X.\n00:10:51 For multivariate continuous features X, we can use a similar Gaussian approach where we, instead of learning a single scalar Gaussian distribution, we learn a multivariate distribution for X, which will then be parameterized by a vector mean and a.\n00:11:00 matrix.", "start_char_idx": 6210, "end_char_idx": 10190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4213762f-d15f-480f-b4ce-c48c0fa7a006": {"__data__": {"id_": "4213762f-d15f-480f-b4ce-c48c0fa7a006", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ivBSZZyaRHY", "Link": "https://www.youtube.com/watch?v=ivBSZZyaRHY"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea3fb7fe-a2b7-4d16-a969-fc97e37328ee", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ivBSZZyaRHY", "Link": "https://www.youtube.com/watch?v=ivBSZZyaRHY"}, "hash": "7d359be2d6e5b53f900454cd4f12ffc532eb287d686bf14818b2a102f3577ac1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b3d0846-c6bd-4b63-812d-ff0d33d2d739", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ivBSZZyaRHY", "Link": "https://www.youtube.com/watch?v=ivBSZZyaRHY"}, "hash": "596d062489e5b1ce16c1e46f71644b03912f493dd93fb01593091df25a2b650e", "class_name": "RelatedNodeInfo"}}, "text": "00:10:17 So we take all the data with class zero, here the red class, and use it to estimate the mean and variance of those data.\n00:10:25 And then separately, we would take all the data where class one occurred.\n00:10:34 We would use those to estimate the mean and variance of the class one data.\n00:10:42 And this would give us two models for P of X given class zero, P of X given class one, and we can apply Bayes' rule to calculate P of each class given X.\n00:10:51 For multivariate continuous features X, we can use a similar Gaussian approach where we, instead of learning a single scalar Gaussian distribution, we learn a multivariate distribution for X, which will then be parameterized by a vector mean and a.\n00:11:00 matrix.\n00:11:10 So, the mean will be of the same size as the number of features that we have, and the covariance matrix sigma will be n by n, where n is the number of features.\n00:11:20 Again, we'll tend to estimate these parameters just using a standard maximum likelihood estimate, which in this case is just the empirical estimate.\n00:11:30 So, we can take all the data associated with that class and compute its empirical mean vector that will be the center of our multivariate Gaussian, and we can compute its empirical covariance matrix, and that will tell us the spread and shape of the Gaussian in those dimensions.\n00:11:40 For more information on Gaussian and multivariate Gaussian models, please see my background slides on Gaussian models.", "start_char_idx": 9455, "end_char_idx": 10935, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54936631-28a6-416c-84b9-8e7dae428d9c": {"__data__": {"id_": "54936631-28a6-416c-84b9-8e7dae428d9c", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd947843-9be4-4dc4-84df-ca245ad4e9a6", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "hash": "ae76dfe7bf5142afd9bcede947d2d1664bb22bd8cb91a069c276247d142f8620", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ed96378-5b61-47b0-99ef-cc6a0c4f1172", "node_type": "1", "metadata": {}, "hash": "81368d36489f3421fa4bd43e981ccc7b2d80fa81cee9d971bb67b3b09778c892", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 A very common variant of Bayes classifiers for systems with many features is called Naive Bayes.\n00:00:07 Recall that for Bayes classifiers, we would estimate several quantities.\n00:00:15 The first was the probability of each class overall, and the second was a collection of class conditional distributions.\n00:00:22 So for every possible class outcome of Y, we would estimate a model of the features X.\n00:00:30 And then we could take these and use them to calculate the probability of each class, given an observation of that feature X using Bayes rule.\n00:00:37 Given these probabilities, we would simply choose the most likely class C.\n00:00:45 We saw that for a simple discrete X, we could represent this as a contingency table.\n00:00:52 So we could compute the probabilities of each outcome, Y given X, and then choose the maximum class, the class that maximized that probability, as our prediction.\n00:01:00 However, what if we have more features, discrete features X, perhaps many more?\n00:01:06 Well, one straightforward approach is to just use a joint distribution over all the outcomes of X.\n00:01:12 So, for instance, say X is made up of three features A, B, C.\n00:01:18 We can represent a truth table over all the possible outcomes.\n00:01:24 And each of these entries represents one possible outcome for the entire feature vector X.\n00:01:30 Then, for each possible combination of those values, we can associate a probability.\n00:01:36 So P of X is then a probability associated with the outcome of that vector A, B, C.\n00:01:42 Since this is a probability distribution, the total amount of probability here must sum to one.\n00:01:48 And so if we have, say, three binary variables, this is a table over eight possible entries.\n00:01:54 Those entries sum to one, so we essentially have freedom to specify seven of those entries.\n00:02:00 such that they sum to less than 1.\n00:02:08 The problem with this approach comes in when we start to bring in the fact that we'll be estimating these using a finite data set.\n00:02:17 So we have a collection of data, and we'll estimate the probabilities, let's say using the empirical probabilities that we saw in our data set.\n00:02:25 So we have a collection of outcomes, for instance, ABC might be 0, 0, 0, and we see that in our data set, say, 4 out of 10 times.\n00:02:34 Similarly, another outcome, 0, 0, 1, might occur 1 out of 10 times, 0, 1, 0 might have never occurred out of the 10 examples in this class.\n00:02:42 The problem is that we have only a fixed amount of data, say, m data points, and the size of this table is growing exponentially with the number of discrete features.\n00:02:51 So if we have n features, then we have 2 to the n possible entries in this table to fill in, and this is increasingly important because\n00:03:00 if we have very small amounts of data, many of these entries will have no data associated with it, which means that we're estimating that the probability of seeing that outcome in this particular class is zero.\n00:03:07 So if we ever do see that outcome, our empirical estimate of probability will say that that should never have occurred in this class, and we'll rule out this class entirely.\n00:03:15 So this is essentially an overfitting effect.\n00:03:22 We have 2 to the n possible parameters to associate with each class and only m data points to associate.\n00:03:30 So the more features we add, the more complex our contingency table representation of our classifier is getting.\n00:03:37 One simple option to try to improve things is to smooth or regularize our empirical estimates away from zero.\n00:03:45 So instead of choosing the fraction of data points, say 4 out of 10, we would add some small smoothing parameter alpha.\n00:03:52 So we would take 4 plus alpha.\n00:04:00 for that entry, and then we'd normalize by the sum of all of them, so 10 plus 8 alpha.", "start_char_idx": 0, "end_char_idx": 3874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ed96378-5b61-47b0-99ef-cc6a0c4f1172": {"__data__": {"id_": "7ed96378-5b61-47b0-99ef-cc6a0c4f1172", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd947843-9be4-4dc4-84df-ca245ad4e9a6", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "hash": "ae76dfe7bf5142afd9bcede947d2d1664bb22bd8cb91a069c276247d142f8620", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54936631-28a6-416c-84b9-8e7dae428d9c", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "hash": "3c2a05f0babf7494ee9830f3db586dce5c6e07f2ebc7ad9a3c360b98cf9026e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "485e98b9-2f0e-47cd-b50a-93425a1710b7", "node_type": "1", "metadata": {}, "hash": "34bc6d68fbcce02c007eaff362eadc25fdfb5aedb08edfb480de49e1e99b5763", "class_name": "RelatedNodeInfo"}}, "text": "00:03:15 So this is essentially an overfitting effect.\n00:03:22 We have 2 to the n possible parameters to associate with each class and only m data points to associate.\n00:03:30 So the more features we add, the more complex our contingency table representation of our classifier is getting.\n00:03:37 One simple option to try to improve things is to smooth or regularize our empirical estimates away from zero.\n00:03:45 So instead of choosing the fraction of data points, say 4 out of 10, we would add some small smoothing parameter alpha.\n00:03:52 So we would take 4 plus alpha.\n00:04:00 for that entry, and then we'd normalize by the sum of all of them, so 10 plus 8 alpha.\n00:04:08 This ensures that we never estimate any probability as exactly 0, so it's sometimes called smoothing.\n00:04:17 It's better to think of this as regularization, which we'll cover in later lectures.\n00:04:25 However, another powerful approach is to try to simplify the representation of this table of possible outcomes.\n00:04:34 So, one possibility is to just compute this probability under a very simple model of the features.\n00:04:42 So, for instance, independent variables have the property that the joint probability of two variables A and B is just the product of a probability of A and a probability associated with outcome B, meaning that events A and B are independent of one another.\n00:04:51 Observing A does not affect B, and vice versa.\n00:05:00 For more background on probability, please see my background probability slides.\n00:05:08 Taking this to a large number of features, say x1 through xn, discrete features, we might assume that within this class, feature 1 and 2 and 3 and so on are all independent of each other.\n00:05:17 So that means the probabilities in this table are simply the product of a probability associated with the outcome of x1, a probability associated with the outcome of x2, and so on.\n00:05:25 And that means we only need to estimate these probabilities over single discrete features at a time.\n00:05:34 So, for instance, we might estimate the probability of A in our data, and A might say 40% of the time come out 0 and 60% of the time come out 1.\n00:05:42 Separately from that, we would estimate the probability of B being 0 or 1 or C being 0 and 1.\n00:05:51 And then the entries of this exponentially large table can be computed very simply as the product of the probability\n00:05:59 associated with the outcomes of A, B, and C.\n00:06:05 Clearly, this drastically reduces the number of parameters being estimated.\n00:06:11 For instance, here we have one parameter for A, one for B, and one for C, so three total as compared to seven to specify the joint probability table.\n00:06:17 Often this is used in systems with very many features.\n00:06:23 So, for example, suppose we want to predict a discrete binary Y, say whether or not a person will be in an auto accident within the next year.\n00:06:29 For each person, we have a large number of possible co-observed measurements.\n00:06:35 We might observe their age, their income, their education level, perhaps the zip code they live in, the number of years driving, things like that.\n00:06:41 Lots of possible discrete variables in this case.\n00:06:47 So, we want to learn a model that takes all of those possible features into account when it predicts Y.\n00:06:53 If we were to try to learn this table directly,\n00:06:59 as a contingency over every possibility of X1 through Xm, it would take d to the m plus 1 possible values to specify that probability distribution.\n00:07:08 So, exponential in the number of features that we observe, which is many.\n00:07:17 The Naive Bayes model, on the other hand, uses Bayes' rule combined to a prediction of the possible outcomes of Y, so that's quite simple.\n00:07:25 There are only two outcomes of Y, and a model of P of X given Y that factors into a probability of each feature given Y.", "start_char_idx": 3200, "end_char_idx": 7104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "485e98b9-2f0e-47cd-b50a-93425a1710b7": {"__data__": {"id_": "485e98b9-2f0e-47cd-b50a-93425a1710b7", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd947843-9be4-4dc4-84df-ca245ad4e9a6", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "hash": "ae76dfe7bf5142afd9bcede947d2d1664bb22bd8cb91a069c276247d142f8620", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ed96378-5b61-47b0-99ef-cc6a0c4f1172", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "hash": "4b608bcf619cc1c651e388d9823c5566fc432734436eaf9c11cd1daeedd40cf6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c828cb87-65fd-41f7-b117-64e0e8cc6c6d", "node_type": "1", "metadata": {}, "hash": "fbf74ecf8a1ffed4c04314b0c57501142bf6f8d05cb9b9dc89e7d3f269d5a3d6", "class_name": "RelatedNodeInfo"}}, "text": "00:06:41 Lots of possible discrete variables in this case.\n00:06:47 So, we want to learn a model that takes all of those possible features into account when it predicts Y.\n00:06:53 If we were to try to learn this table directly,\n00:06:59 as a contingency over every possibility of X1 through Xm, it would take d to the m plus 1 possible values to specify that probability distribution.\n00:07:08 So, exponential in the number of features that we observe, which is many.\n00:07:17 The Naive Bayes model, on the other hand, uses Bayes' rule combined to a prediction of the possible outcomes of Y, so that's quite simple.\n00:07:25 There are only two outcomes of Y, and a model of P of X given Y that factors into a probability of each feature given Y.\n00:07:34 So, each feature only has a small number of outcomes, so estimating this probability over just the outcomes of that one feature is much easier.\n00:07:42 So, this assumes that given each underlying cause, each class, the covariate features are actually independent.\n00:07:51 Note that that might not be a good model of the actual data.\n00:07:59 So, for instance, we think it's unlikely that age and income or income and education are actually independent of one another, even given whether you will or will not have an accident in the future.\n00:08:08 But the point of this is not to be a good model of the features X, but to capture the dependencies that might lead to predicting Y.\n00:08:17 So, in practice, this can be very good at predicting Y, even if it's not a very good model of the features X itself.\n00:08:25 So, here's a classic model of using Naive Bayes models, which is spam filtering in email.\n00:08:34 So, we have a binary class to predict whether the email is spam or not spam.\n00:08:42 And our features will be the context, the content of the email, right, the words that appear in the email.\n00:08:51 So, for example, we might just list out every possible word in the English dictionary.\n00:08:59 10,000 of those words commonly used, and then we give ourselves a 1 in the entry if that particular word appears.\n00:09:09 So we'll have one feature that's whether the word the appears, another feature that's whether the word probabilistic appears anywhere in the email, another that's whether the word lottery appears anywhere in the email.\n00:09:19 So this will be about 10,000 binary features of each email.\n00:09:29 So if we have several thousand words, if we were to estimate the joint probability of X given Y or Y given every possible outcome X, we would need 2 to the 10,000 or so parameters, which is a big problem because it's much, much larger than the number of atoms even in the universe.\n00:09:39 So instead what we'll do is we'll model the words that appear in the email as independent given the email type.\n00:09:49 So what this is going to capture is that some words are more likely to appear in...\n00:09:59 the spam category.\n00:10:06 Like for me, spam will be more likely to have lottery than real email would.\n00:10:13 And some words are going to be indicative of real email.\n00:10:19 Like I rarely get spam with the word probabilistic in it, but I regularly get email from students with that word.\n00:10:26 And so what that's going to mean is each possible word, appearing or not appearing, gets its own distribution in the spam and not spam categories.\n00:10:33 And so that's going to mean that instead of 2 to the 10,000 parameters, we're only going to have about 10,000 parameters for spam and 10,000 parameters for not spam.\n00:10:39 So only on the order of 10,000 or 20,000 parameters total.\n00:10:46 Given only that many parameters, a reasonably sized data set should be able to estimate those probabilities with fairly good quality.\n00:10:53 It's useful to think about what the naive Bayes assumption is from a Gaussian modeling...\n00:10:59 perspective as well.", "start_char_idx": 6358, "end_char_idx": 10218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c828cb87-65fd-41f7-b117-64e0e8cc6c6d": {"__data__": {"id_": "c828cb87-65fd-41f7-b117-64e0e8cc6c6d", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd947843-9be4-4dc4-84df-ca245ad4e9a6", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "hash": "ae76dfe7bf5142afd9bcede947d2d1664bb22bd8cb91a069c276247d142f8620", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "485e98b9-2f0e-47cd-b50a-93425a1710b7", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "hash": "3263fa286144a96d8971a712d006d663350a946d3a0d5317ac39a5f890fcfd74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33fed07a-e4f5-4204-a3e1-616c099f2f56", "node_type": "1", "metadata": {}, "hash": "5ca0b13fac94ed60ada54bc808df124ef803a2876899e67c50463f60dfff2c95", "class_name": "RelatedNodeInfo"}}, "text": "00:10:19 Like I rarely get spam with the word probabilistic in it, but I regularly get email from students with that word.\n00:10:26 And so what that's going to mean is each possible word, appearing or not appearing, gets its own distribution in the spam and not spam categories.\n00:10:33 And so that's going to mean that instead of 2 to the 10,000 parameters, we're only going to have about 10,000 parameters for spam and 10,000 parameters for not spam.\n00:10:39 So only on the order of 10,000 or 20,000 parameters total.\n00:10:46 Given only that many parameters, a reasonably sized data set should be able to estimate those probabilities with fairly good quality.\n00:10:53 It's useful to think about what the naive Bayes assumption is from a Gaussian modeling...\n00:10:59 perspective as well.\n00:11:11 So, if we give a Gaussian model to feature 1, so feature 1 is a Gaussian over a univariate feature x1, and feature 2 is a Gaussian over the univariate feature x2, the Naive Bayes assumes that for each class, P of x1 and x2, the joint probability is just the product of these two individual probabilities.\n00:11:23 If we write that out and try to fit it into the form of a complete joint probability, what we find is that the x1 and x2 components decompose.\n00:11:35 And the way to fit that into this joint probability with a arbitrary covariance matrix is that you find that the mean parameters is just the mean of the first feature and the mean of the second feature formed as a vector, but the covariance matrix is simplified.\n00:11:47 The covariance matrix is a diagonal matrix whose 1, 1 entry is the variance of feature 1 and whose 2, 2 entry is the variance of feature 2.\n00:11:59 entry is the variance of feature 2.\n00:12:11 Again, this has the interpretation of having fewer parameters to estimate, since an arbitrary covariance matrix over two features would have not only those diagonal entries, but would also estimate an off-diagonal term relating to their correlation.\n00:12:23 So, if I have, say, m features, an arbitrary covariance matrix will estimate approximately m squared over 2 of those parameters, whereas a Naive Bayes model will only estimate the diagonal entries, so it's going to be on the order of m features.\n00:12:35 So, again, we have the notion that the Naive Bayes model has many fewer parameters to a model over all the features.\n00:12:47 In terms of Gaussians, this diagonal form means that our covariance matrices are going to form axis-aligned independent Gaussians, so they'll tend to look like ovals that follow axes.\n00:12:59 aligned shapes, so there'll be no correlation to the different features X1 and X2 within that class.\n00:13:11 In summary, to use Bayes classifiers and Naive Bayes classifiers, you should know Bayes rule, which expresses the probability of our class given the features in terms of a class conditional model P of X given Y, and a class distribution P of Y.\n00:13:23 So to estimate these from data, we learn these two components, the probability of each class individually, and a model of the features for each possible class outcome, which we can learn from the data only associated with that class.\n00:13:35 Naive Bayes models simplify P of X given Y by assuming that for each class Y, P of X is an independent distribution.\n00:13:47 So P of X given Y equals some outcome C is just a product over models for each feature individually given that class.\n00:13:59 And typically, for all these probabilities, we'll be estimating their values using just the empirical estimators, which in this case also correspond to the maximum likelihood estimators.\n00:14:08 So, for discrete variables, you should be familiar with the empirical estimator, the number of counts divided by the total number of data.\n00:14:17 For Gaussian variables, we'll use the empirical mean and empirical covariance or empirical variances.\n00:14:25 And most importantly, there's an effect as this model, P of X given Y, becomes increasingly complex as it requires increasingly many parameters to express it.\n00:14:34 We run into danger of overfitting.", "start_char_idx": 9425, "end_char_idx": 13510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33fed07a-e4f5-4204-a3e1-616c099f2f56": {"__data__": {"id_": "33fed07a-e4f5-4204-a3e1-616c099f2f56", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd947843-9be4-4dc4-84df-ca245ad4e9a6", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "hash": "ae76dfe7bf5142afd9bcede947d2d1664bb22bd8cb91a069c276247d142f8620", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c828cb87-65fd-41f7-b117-64e0e8cc6c6d", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}, "hash": "7f2715ba9a085ad7edb846f934febd6dc52ebe4ab0b553513500175379d08ea4", "class_name": "RelatedNodeInfo"}}, "text": "00:13:47 So P of X given Y equals some outcome C is just a product over models for each feature individually given that class.\n00:13:59 And typically, for all these probabilities, we'll be estimating their values using just the empirical estimators, which in this case also correspond to the maximum likelihood estimators.\n00:14:08 So, for discrete variables, you should be familiar with the empirical estimator, the number of counts divided by the total number of data.\n00:14:17 For Gaussian variables, we'll use the empirical mean and empirical covariance or empirical variances.\n00:14:25 And most importantly, there's an effect as this model, P of X given Y, becomes increasingly complex as it requires increasingly many parameters to express it.\n00:14:34 We run into danger of overfitting.\n00:14:42 If we have exponentially many parameters for this model and only a small number of data, we'll never be able to estimate it very well.\n00:14:51 So, models like Naive Bayes use simplifying assumptions on the form of this distribution, like a product of individual ones, to reduce the number of parameters and thus simplify the\n00:14:59 overall model of P of X given Y.", "start_char_idx": 12717, "end_char_idx": 13887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "292e19a9-c5ed-444c-867c-2bac8cfbc822": {"__data__": {"id_": "292e19a9-c5ed-444c-867c-2bac8cfbc822", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Multivariate Gaussian distributions.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=eho8xH3E6mE", "Link": "https://www.youtube.com/watch?v=eho8xH3E6mE"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11490156-0a15-4f83-b095-50e657d53446", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Multivariate Gaussian distributions.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=eho8xH3E6mE", "Link": "https://www.youtube.com/watch?v=eho8xH3E6mE"}, "hash": "949742e25526889534eedb79f94dbdaf74f1196fc3069aaefa6fb7c9c3f923bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3552e3f4-2f63-4406-935c-99c458285615", "node_type": "1", "metadata": {}, "hash": "234be7ad12c94220a9cf8d4e0f145a5ca7788e4defabc067b95fe0c89453e9f3", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 We'll examine the multivariate Gaussian distribution and some of its properties and geometry.\n00:00:07 The Gaussian distribution in one dimension is a classic distribution over a single scalar random variable X and parameterized by a mean mu and a standard deviation sigma, or equivalently its variance, sigma squared.\n00:00:15 The distribution is just given by an exponentiated quadratic, so there's a normalization constant that assures it integrates to 1, and then e to the minus 1 half times a quadratic form, X minus mu squared divided by the variance sigma squared.\n00:00:22 This distribution has a classic bell-shaped curve.\n00:00:30 It's symmetric.\n00:00:37 Its mean is the center point, also the optimum of the maximum of the function.\n00:00:45 And the variance tells us about the spread.\n00:00:52 So the plus...\n00:01:00 or minus 2 sigma region will give us a 95% confidence interval.\n00:01:06 So the larger sigma is, the wider this bell-shaped curve is.\n00:01:13 Also useful, the classic maximum likelihood estimators.\n00:01:20 Given some data, if we want to estimate the distribution, we can estimate the mean as being the empirical mean of the data points by taking the average of the data and estimate the variance as being the empirical average of the centered data points squared.\n00:01:26 So we take x, subtract the empirical mean, square those points, and average them.\n00:01:33 That'll give us the variance.\n00:01:40 A multivariate Gaussian model is simply an extension of this model to vector-valued random variables.\n00:01:46 So now x will be a vector with d values, and the distribution will be parameterized by a mean mu, which will also be a length d vector.\n00:01:53 And a covariance matrix sigma, which will be a d by d matrix.\n00:02:00 Again, it has the same kind of exponentiated quadratic form.\n00:02:05 So here's a constant to ensure that the whole thing integrates to 1.\n00:02:10 And then it's e to the minus 1 by 2.\n00:02:15 And then instead of the x minus mu squared over sigma squared, we have x minus mu.\n00:02:20 This is a row vector.\n00:02:25 Sigma inverse, a d by d matrix, times x minus mu transpose.\n00:02:30 So this is a column vector.\n00:02:35 So a row times a square matrix times a column vector will give us a scalar number which evaluates to the probability of the value x.\n00:02:40 This function, if plotted in the multiple dimensional features, say we have feature x1 and feature x2, this function will be, again, a bell-shaped curve, but now in two dimensions.\n00:02:45 So here we have the same kind of symmetric bell-shaped mode.\n00:02:50 If we take a single cut of this at some probability, we can plot it as a contour plot.\n00:02:55 So here.\n00:03:00 might be x1, here might be x2.\n00:03:04 And the plot of constant probability will be an ellipse for a Gaussian.\n00:03:08 So the mean vector mu will be a two-dimensional point in the center.\n00:03:12 And the covariance will, again, define the spread and now the shape of this ellipsoidal function.\n00:03:17 The maximum likelihood estimators are quite similar to the univariate case.\n00:03:21 So the mean vector mu will now be the empirical average of the data vectors x.\n00:03:25 So remember, mu is the same size as x.\n00:03:30 So we just average the vectors.\n00:03:34 The covariance matrix, sigma, will also be an empirical average.\n00:03:38 We take the data and center them by removing the empirical mean.\n00:03:42 And then what we average is the vector-vector product, so the outer product, x minus mu transpose times x minus mu.\n00:03:47 The x minus mu transpose is a column vector.\n00:03:51 x minus mu is a row vector.\n00:03:55 And so their outer product.\n00:03:59 product is a d by d matrix.\n00:04:06 If we average those over all the data, we'll get the empirical covariance matrix sigma.", "start_char_idx": 0, "end_char_idx": 3804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3552e3f4-2f63-4406-935c-99c458285615": {"__data__": {"id_": "3552e3f4-2f63-4406-935c-99c458285615", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Multivariate Gaussian distributions.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=eho8xH3E6mE", "Link": "https://www.youtube.com/watch?v=eho8xH3E6mE"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11490156-0a15-4f83-b095-50e657d53446", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Multivariate Gaussian distributions.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=eho8xH3E6mE", "Link": "https://www.youtube.com/watch?v=eho8xH3E6mE"}, "hash": "949742e25526889534eedb79f94dbdaf74f1196fc3069aaefa6fb7c9c3f923bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "292e19a9-c5ed-444c-867c-2bac8cfbc822", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Multivariate Gaussian distributions.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=eho8xH3E6mE", "Link": "https://www.youtube.com/watch?v=eho8xH3E6mE"}, "hash": "1369744837ce6efa1c4885babbc386ee8b1516f3223f14aaa7ed0c21b44cf094", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7826277c-9417-4413-9b9a-8e6bc35f1f42", "node_type": "1", "metadata": {}, "hash": "eb79c5179752df233e7ec3a3453d58dc89255d708f106e92d2b4dfeae9570782", "class_name": "RelatedNodeInfo"}}, "text": "00:03:21 So the mean vector mu will now be the empirical average of the data vectors x.\n00:03:25 So remember, mu is the same size as x.\n00:03:30 So we just average the vectors.\n00:03:34 The covariance matrix, sigma, will also be an empirical average.\n00:03:38 We take the data and center them by removing the empirical mean.\n00:03:42 And then what we average is the vector-vector product, so the outer product, x minus mu transpose times x minus mu.\n00:03:47 The x minus mu transpose is a column vector.\n00:03:51 x minus mu is a row vector.\n00:03:55 And so their outer product.\n00:03:59 product is a d by d matrix.\n00:04:06 If we average those over all the data, we'll get the empirical covariance matrix sigma.\n00:04:13 It's useful to see an example of this for independent variables x1 and x2.\n00:04:19 If we have a variable x1 that's Gaussian, it'll have a normalization constant and then a Gaussian distribution with mean mu1 and variance sigma1 squared.\n00:04:26 x2 being Gaussian means it has the same form, but say, with mean 2 and variance sigma2 squared.\n00:04:33 If we now create a new vector variable, x, just by concatenating x1 and x2, we can ask what the distribution of x is.\n00:04:40 We'll assume that x1 and x2 are independent, so that means their joint distribution is the product of their individual distributions.\n00:04:46 And what we see is we get the product of their normalization constants times e to the minus 1 by 2.\n00:04:53 And now, there'll be one term.\n00:05:00 x1-mu1 over sigma1 squared, and another term, x2-mu2 squared over sigma2 squared, added together, since the product of these two things will add in the exponent.\n00:05:20 If we just vectorize that, we can describe that as a x-mu, sigma inverse x-mu, where now x1-mu1 and x2-mu2 is given by the vector difference x-mu, and the scaling of the sigmas is accomplished by creating a diagonal covariance matrix.\n00:05:40 So, sigma1 squared in the upper left, sigma2 squared in the lower right, and this inverse will simply be the same thing point-wise inverse, and when we multiply that by x-mu, transpose x-mu, we will get this term.\n00:06:00 x1 minus mu1 over sigma 1 squared, and this term, x2 minus mu2 over sigma 2 squared, added together.\n00:06:06 If we plot this on two features, we will find that this covariance matrix has an axis-aligned ellipsoidal shape, with the spread in the axis being proportional to the size of sigma.\n00:06:13 So here I've drawn sigma 1, 1 being larger than sigma 2, 2.\n00:06:20 So the spread in the x1 direction is larger than the spread in the x2 direction.\n00:06:26 We'll see more examples of this later.\n00:06:33 More generally, we can understand the geometry of the Gaussian through the eigendecomposition of the matrix sigma.\n00:06:40 Consider a curve of constant probability.\n00:06:46 The constant probability curve in a Gaussian means that the exponent will be constant, since the other terms are all just scalar normalization constants.\n00:06:53 So this curve, this red curve.\n00:07:00 is given by delta squared equal a constant, where delta squared is this exponentiated term.\n00:07:10 So, x minus mu, sigma inverse, x minus mu transpose.\n00:07:20 So, now let's understand what the shape of this function is by thinking about an eigendecomposition of the matrix sigma.\n00:07:30 So, an eigendecomposition decomposes sigma into an eigenvector matrix mu, an eigenvalue matrix lambda that's diagonal, and the same eigenvector matrix mu transpose.\n00:07:40 So, these mus are orthonormal vectors, which means that the product of mu i and mu j is either 0 if i is not equal to j, or 1 if i equals to j.", "start_char_idx": 3093, "end_char_idx": 6719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7826277c-9417-4413-9b9a-8e6bc35f1f42": {"__data__": {"id_": "7826277c-9417-4413-9b9a-8e6bc35f1f42", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Multivariate Gaussian distributions.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=eho8xH3E6mE", "Link": "https://www.youtube.com/watch?v=eho8xH3E6mE"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11490156-0a15-4f83-b095-50e657d53446", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Multivariate Gaussian distributions.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=eho8xH3E6mE", "Link": "https://www.youtube.com/watch?v=eho8xH3E6mE"}, "hash": "949742e25526889534eedb79f94dbdaf74f1196fc3069aaefa6fb7c9c3f923bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3552e3f4-2f63-4406-935c-99c458285615", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Multivariate Gaussian distributions.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=eho8xH3E6mE", "Link": "https://www.youtube.com/watch?v=eho8xH3E6mE"}, "hash": "b925e98cd5d963862c88ad278770cb67ed26e7846a56f3f7b787807be3c199e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ef4dca3-1aaf-4e15-a582-37b278e9a79c", "node_type": "1", "metadata": {}, "hash": "d5945fa83906586fe87eb67e18c3806c1619d5463cfc33c162405105377c1c05", "class_name": "RelatedNodeInfo"}}, "text": "00:06:53 So this curve, this red curve.\n00:07:00 is given by delta squared equal a constant, where delta squared is this exponentiated term.\n00:07:10 So, x minus mu, sigma inverse, x minus mu transpose.\n00:07:20 So, now let's understand what the shape of this function is by thinking about an eigendecomposition of the matrix sigma.\n00:07:30 So, an eigendecomposition decomposes sigma into an eigenvector matrix mu, an eigenvalue matrix lambda that's diagonal, and the same eigenvector matrix mu transpose.\n00:07:40 So, these mus are orthonormal vectors, which means that the product of mu i and mu j is either 0 if i is not equal to j, or 1 if i equals to j.\n00:07:50 So, then we can envision this as sigma being a matrix U, which involves the eigenvectors in column-wise form.\n00:08:00 a diagonal matrix lambda, and u transpose, which now has the same vectors but transposed.\n00:08:10 If we now just define a variable y i as the projection of the vector x minus mu onto the ith eigenvector mu i.\n00:08:20 So that projection is given by mu i transpose, mu i dot producted with x minus mu.\n00:08:30 And then you can see that this delta squared has a very simple form in terms of these y's.\n00:08:40 It's just y1 squared scaled by lambda 1 plus y2 squared scaled by lambda 2.\n00:08:50 And this is precisely the equation of an ellipse with one axis of size 1 over lambda 1 and the other axis of size 1 over lambda 2.\n00:09:00 lambda 1 and lambda 2.\n00:09:12 Geometrically, this means that this ellipsoid of constant probability will be an ellipse with an axis in the u1 direction and an orthogonal axis in the u2 direction with the length of those axes being proportional to the eigenvalues or rather the square root of the eigenvalues associated with each eigenvector.\n00:09:24 This eigen decomposition view can also be used in a constructive way to sample values of Gaussians.\n00:09:36 I think this is a useful illustration of how this decomposition affects the shape of the Gaussian.\n00:09:48 Let's start by generating random vectors X using the simple MATLAB randn function which gives us m samples of independent\n00:10:00 Gaussians of vectors of two features.\n00:10:06 So this is a matrix of completely independent m by 2 entries of Gaussians with mean 0 and variance 1.\n00:10:12 You can see that the data are independent.\n00:10:18 They have a spherical shape and variance 1 in each direction.\n00:10:24 We can then scale x by multiplying it by some matrix where this is the scaling term in the x1 feature.\n00:10:30 This is the scaling term in the x2 feature.\n00:10:36 So when we do that, we see that what happens is the x1 feature values get spread out.\n00:10:42 The x2 feature values are not spread out because their scaling factor is 1.\n00:10:48 And so now we have this elongated axis-aligned ellipse with a longer x1 feature than x2.\n00:10:54 In mathematical terms, this thing where\n00:11:00 by is the square root of the eigenvector matrix on the previous slide.\n00:11:15 We can then multiply by a rotation matrix, so here's a matrix that rotates by angle theta, and this is precisely taking the role of this eigenvector matrix U.\n00:11:30 So this rotation is defining now the orthogonal axes, the first eigenvector and second eigenvector, U1 and U2.\n00:11:45 Finally, if we add the same vector mu to all the data points, we'll shift the values, the center of those values, up so that they have average value mu instead of average value zero, and this will give us a Gaussian distribution with centered at the vector point mu, so mu1, mu2, and we\n00:12:00 with a covariance describing an ellipsoidal shape with value U lambda U transpose, where U transpose is this rotation matrix, and lambda is given by the square of this scaling matrix.", "start_char_idx": 6060, "end_char_idx": 9805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ef4dca3-1aaf-4e15-a582-37b278e9a79c": {"__data__": {"id_": "7ef4dca3-1aaf-4e15-a582-37b278e9a79c", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Multivariate Gaussian distributions.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=eho8xH3E6mE", "Link": "https://www.youtube.com/watch?v=eho8xH3E6mE"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11490156-0a15-4f83-b095-50e657d53446", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Multivariate Gaussian distributions.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=eho8xH3E6mE", "Link": "https://www.youtube.com/watch?v=eho8xH3E6mE"}, "hash": "949742e25526889534eedb79f94dbdaf74f1196fc3069aaefa6fb7c9c3f923bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7826277c-9417-4413-9b9a-8e6bc35f1f42", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Multivariate Gaussian distributions.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=eho8xH3E6mE", "Link": "https://www.youtube.com/watch?v=eho8xH3E6mE"}, "hash": "7b84d6b19d3183b4155261f9c5490af22b9f8fcd28d392945c091fa107966dcd", "class_name": "RelatedNodeInfo"}}, "text": "00:11:15 We can then multiply by a rotation matrix, so here's a matrix that rotates by angle theta, and this is precisely taking the role of this eigenvector matrix U.\n00:11:30 So this rotation is defining now the orthogonal axes, the first eigenvector and second eigenvector, U1 and U2.\n00:11:45 Finally, if we add the same vector mu to all the data points, we'll shift the values, the center of those values, up so that they have average value mu instead of average value zero, and this will give us a Gaussian distribution with centered at the vector point mu, so mu1, mu2, and we\n00:12:00 with a covariance describing an ellipsoidal shape with value U lambda U transpose, where U transpose is this rotation matrix, and lambda is given by the square of this scaling matrix.\n00:12:30 So this illustrates both how we can generate a collection of samples that are drawn from a multivariate Gaussian with arbitrary covariance matrix sigma, just by decomposing sigma into its eigen decomposition and then following this procedure, and also how to understand the geometry and what the role of the eigenvectors and eigenvalues are of the covariance matrix in determining the shape that this Gaussian ellipsoidal\n00:13:00 a contour takes on.\n00:13:10 In summary, multivariate Gaussian distributions generalize the classic 1D Gaussian bell-shaped contour to vector-valued random variables by giving them a vector-valued mean of the same dimension as the data and a covariance matrix of size d squared that describes the shape and size of the bell-shaped mode.\n00:13:20 For independent variables, we find that we have a diagonal covariance.\n00:13:30 Diagonal covariance leads to an axis-aligned ellipsoidal shape.\n00:13:40 If the diagonal entries are equal, that means that the size in each of these directions is the same, and we see that the contours take on a spherical shape.\n00:13:50 More generally, for an arbitrary covariance matrix sigma, we get an ellipsoidal probability.\n00:14:00 contour that may have some rotational shape, and you can understand the size and orientation of that ellipse by looking at the eigenvectors and the eigenvalues of the covariance matrix, with the eigenvectors defining the directions of the principal axes of this ellipse, and the scale determined by the value of the eigenvalues.\n00:14:24 We can also follow this procedure in reverse to generate data from a multivariate Gaussian by first drawing data from independent unit normals, and then performing scaling and rotation and shifts to give us an arbitrary Gaussian.", "start_char_idx": 9029, "end_char_idx": 11580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c576900f-0b9f-4cb8-bea1-87f320e17447": {"__data__": {"id_": "c576900f-0b9f-4cb8-bea1-87f320e17447", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hiOQDsdOZ7I", "Link": "https://www.youtube.com/watch?v=hiOQDsdOZ7I"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4cbd714d-1dc7-4772-8616-809dd14dfa8c", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hiOQDsdOZ7I", "Link": "https://www.youtube.com/watch?v=hiOQDsdOZ7I"}, "hash": "54b0c2bb9aafe256f5c752f929540d58365d0f54c54a7b3b0ac125e0c674f99d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "149361b9-81a6-4f31-acd8-7ea4a8304c64", "node_type": "1", "metadata": {}, "hash": "2b0ef115538f4a8765eaba7d5addea6a8d87902f0d07cf7180db3d6141ea3730", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Linear regression is a simple and useful technique for prediction that will also help us illustrate many important principles of machine learning.\n00:00:06 Recall our supervised learning paradigm.\n00:00:13 We have some observed features, X, which we would like to use to predict a target value, Y.\n00:00:20 We create a flexible function called a learner, whose input-output relationship can be adjusted through some parameters, theta.\n00:00:26 Then, a collection of training examples are used to fit the parameters until the learner is able to make good predictions on those examples.\n00:00:33 In linear regression, our learner consists of an explicit functional form.\n00:00:40 In other words, we define a family of functions parameterized by theta, and any particular value of theta defines a member of that family.\n00:00:46 So, for example, we would compute theta0 plus theta1 x1 to produce an explicit linear relationship between the feature X and our target Y.\n00:00:53 Then, learning finds a good member within that family.\n00:00:59 meaning a good value for the thetas, using the training data set D, so that our predictions match those points.\n00:01:06 Some quick notation.\n00:01:13 Our prediction Y hat is a linear function.\n00:01:20 Theta zero plus theta one times feature one plus theta two times feature two, and so on.\n00:01:26 And for notational compactness, we'll define a feature zero, X zero for every data point, that always takes on value one.\n00:01:33 Then Y hat is simply the vector product between a parameter vector theta, consisting of theta zero through theta N, and a feature vector X, consisting of this new feature zero, the constant one, concatenated with our N features.\n00:01:40 We also need to define what it means to predict well.\n00:01:46 Define the error residual to be the signed error between the true or desired target Y and our predicted value Y hat.\n00:01:53 If our prediction is good, these values shown here is.\n00:02:00 blue bars from our prediction, the black line to the true value y, will be small on average.\n00:02:07 We can measure their average squared magnitude, for example, the mean squared error.\n00:02:15 So 1 over m times the sum of the differences between the true values in our predictions.\n00:02:22 We'll define our cost function, J of theta, that measures how good of a fit any particular setting of theta is by summing up the squared error residuals over the training data.\n00:02:30 Well, we could easily choose a different cost function, J, and we'll discuss some at another lecture.\n00:02:37 Mean squared error is a common choice since it's computationally convenient, and it corresponds to a Gaussian model on the noise that we're unable to predict.\n00:02:45 If this noise is the sum of many small influences, then the central limit theorem suggests that such a Gaussian model might be a good choice.\n00:02:52 Since we're using MATLAB and its vector and matrix operations, let's rewrite this function J in a more.\n00:03:00 a convenient vector and matrix form, where we use the parameter vector, theta, and a vector of our targets, a column vector of the target values, and a data matrix, X, where each row corresponds to a data example, so example one through example N, and each column corresponds to a different feature, so the constant feature, and then feature one through N.\n00:03:15 Then our cost function is just the inner product of the error residuals, so the error residual vector is Y minus our prediction vector, theta times X transpose, and taking the dot product, we get the sum over all the data points.\n00:03:30 In MATLAB, this is very convenient.\n00:03:45 We can just compute the error residual E as the difference between the vector Y and the vector theta times X transpose, and then we compute its total average magnitude by, say, E.\n00:04:00 times e transpose divided by M.\n00:04:07 So now that we've chosen a form for the learner and a cost function, learning for linear regression just comes down to selecting a value for theta that scores well.\n00:04:15 The cost function J of theta is a function of the parameter values theta, so we can plot it in a parameter space.", "start_char_idx": 0, "end_char_idx": 4160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "149361b9-81a6-4f31-acd8-7ea4a8304c64": {"__data__": {"id_": "149361b9-81a6-4f31-acd8-7ea4a8304c64", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hiOQDsdOZ7I", "Link": "https://www.youtube.com/watch?v=hiOQDsdOZ7I"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4cbd714d-1dc7-4772-8616-809dd14dfa8c", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hiOQDsdOZ7I", "Link": "https://www.youtube.com/watch?v=hiOQDsdOZ7I"}, "hash": "54b0c2bb9aafe256f5c752f929540d58365d0f54c54a7b3b0ac125e0c674f99d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c576900f-0b9f-4cb8-bea1-87f320e17447", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear regression (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hiOQDsdOZ7I", "Link": "https://www.youtube.com/watch?v=hiOQDsdOZ7I"}, "hash": "84bc2c04dc46c8d06ffb6bfdccbbfbaaeb9ecbc9b09323f1986b35e7042ec2f0", "class_name": "RelatedNodeInfo"}}, "text": "00:03:15 Then our cost function is just the inner product of the error residuals, so the error residual vector is Y minus our prediction vector, theta times X transpose, and taking the dot product, we get the sum over all the data points.\n00:03:30 In MATLAB, this is very convenient.\n00:03:45 We can just compute the error residual E as the difference between the vector Y and the vector theta times X transpose, and then we compute its total average magnitude by, say, E.\n00:04:00 times e transpose divided by M.\n00:04:07 So now that we've chosen a form for the learner and a cost function, learning for linear regression just comes down to selecting a value for theta that scores well.\n00:04:15 The cost function J of theta is a function of the parameter values theta, so we can plot it in a parameter space.\n00:04:22 Suppose we have a linear function, theta 0 plus theta 1 X, so we have two parameters.\n00:04:30 So we plot two dimensions for that and one dimension for J, so it's a 3D plot.\n00:04:37 The 3D is a little difficult to look at, so we can plot the value of J as a color with blue being small values and red large, and we get the following color map.\n00:04:45 Even this is often inconvenient to draw, so we'll often just visualize the contours or isosurfaces of J.\n00:04:52 So for any value of J, there's a surface that's formed by the\n00:05:00 values of J of theta that take on that particular setting.\n00:05:07 And if we just find that set of points, we can plot that with a red line.\n00:05:15 And then plot those contours on a space where just theta zero and theta one are expressed, showing a sort of topological map of the function J, where here the minimum is in this place and increases happen with the contour lines.\n00:05:23 Given a representation of the cost function J of theta, the problem of learning is converted into a basic optimization problem.\n00:05:30 How can we find the values of theta that minimize J of theta?\n00:05:38 In the next segments, we'll explore several methods of doing so.", "start_char_idx": 3350, "end_char_idx": 5370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fa99543-adb9-4c6b-9e25-8a327c1ed2af": {"__data__": {"id_": "5fa99543-adb9-4c6b-9e25-8a327c1ed2af", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (2) Gradient descent.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WnqQrPNYz5Q", "Link": "https://www.youtube.com/watch?v=WnqQrPNYz5Q"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9bd8381-f1ca-4988-9f69-905b663fcd63", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (2) Gradient descent.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WnqQrPNYz5Q", "Link": "https://www.youtube.com/watch?v=WnqQrPNYz5Q"}, "hash": "9c66147e4b778de82ed3589b3baef6fa462c7c78a8898a4e393f21dcf781cce0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dca1e48-c968-4bba-940e-f5b9d6a7019b", "node_type": "1", "metadata": {}, "hash": "442ddf65bdbacb2fdbc47f3262a36d765515b8fd135b5f5fac112655fcad8def", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 A basic toolset for optimization in machine learning are gradient-based methods.\n00:00:06 They're simple, easy to use, and very widely applicable.\n00:00:12 Suppose that we have a cost function J of theta, here plotted for a single parameter theta, and a current value of theta indicated by the blue dot.\n00:00:18 We'd like to evolve theta in such a way that the cost J decreases.\n00:00:24 To do this, should we increase or decrease theta?\n00:00:30 If we look at the derivative of J at the point theta, we'll find that it's positive, which indicates that the function is increasing as theta increases.\n00:00:36 So to decrease J, we should go in the opposite direction, decrease theta, i.e., move in the direction of the negative derivative.\n00:00:42 In higher dimensions, the analog of the derivative is the gradient.\n00:00:48 The gradient measures the vector direction of steepest ascent for the function.\n00:00:54 It's a vector whose entries are the partial derivatives of J with respect to each dimension.\n00:01:00 dimension, so each parameter theta i.\n00:01:07 So to decrease j, we can take steps in the negative gradient direction.\n00:01:15 This is gradient descent.\n00:01:22 We evaluate the gradient, del j, at a particular point and take a step in its negative direction.\n00:01:30 Evaluating the gradient at the new point, we get a new vector and take a step in its negative direction, and so on, until we find a local minimum of the function.\n00:01:37 This gives us the extremely simple gradient descent algorithm.\n00:01:45 We initialize our parameters in some way, often just randomly or setting them to 0, and then iterate, finding the gradient of our function j and updating it by a small amount called the step size, or sometimes the learning rate, which may be constant or may change according to the iteration.\n00:01:52 Finally, we have some stopping conditions.\n00:02:00 stopping the procedure either when J is not changing sufficiently quickly or when, say, our gradient is sufficiently small.\n00:02:07 In 1D, this procedure looks like this.\n00:02:15 We start off at some initial theta, evaluate the gradient, and take a step to decrease the function.\n00:02:22 That gives us a new value of theta.\n00:02:30 We evaluate the gradient again and take another step, followed by another step, until we find that we are at a local minimum as measured by, say, the gradient being sufficiently small.\n00:02:37 For linear regression, we can write the mean squared error cost function as before and then compute its gradient.\n00:02:45 Let's expand our prediction into each term and now denote the error residual here in the interior by E sub J, the error residual at data point J.\n00:02:52 We can take the derivative with respect to J.\n00:03:00 to say one parameter theta zero, we find that just by following the chain rule we can derive the answer.\n00:03:06 So the 1 over m sum over j are linear so they commute with the derivative and it moves into the interior.\n00:03:13 The derivative of the square of e is just 2 times e times the derivative of e and then we can evaluate this term by looking at the dependence on e.\n00:03:19 The term y is just the target so that value is constant.\n00:03:26 Its derivative is zero.\n00:03:33 This term will have a derivative of x.\n00:03:39 This term is constant with respect to theta zero so it will have a derivative of zero as will the others.\n00:03:46 This leaves us with just one term here the derivative with theta zero x zero and we find that the derivative of e is negative x zero, the zeroth feature.\n00:03:53 We can repeat this\n00:03:59 for other parameters, theta one, theta two, and we'll find essentially the same thing.\n00:04:07 This part will remain the same, and over here instead of feature zero, we'll get feature one or feature two.\n00:04:14 Collecting these terms into a vector, the gradient is given by this formula here, where each term is fairly similar.", "start_char_idx": 0, "end_char_idx": 3926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1dca1e48-c968-4bba-940e-f5b9d6a7019b": {"__data__": {"id_": "1dca1e48-c968-4bba-940e-f5b9d6a7019b", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (2) Gradient descent.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WnqQrPNYz5Q", "Link": "https://www.youtube.com/watch?v=WnqQrPNYz5Q"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9bd8381-f1ca-4988-9f69-905b663fcd63", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (2) Gradient descent.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WnqQrPNYz5Q", "Link": "https://www.youtube.com/watch?v=WnqQrPNYz5Q"}, "hash": "9c66147e4b778de82ed3589b3baef6fa462c7c78a8898a4e393f21dcf781cce0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fa99543-adb9-4c6b-9e25-8a327c1ed2af", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear regression (2) Gradient descent.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WnqQrPNYz5Q", "Link": "https://www.youtube.com/watch?v=WnqQrPNYz5Q"}, "hash": "c9bdec4953811d8cb3c75859ff05267d0efcf65b9c36e0cea229e41e470db086", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff47ee7d-d1cb-4b0e-af68-2d9c63118990", "node_type": "1", "metadata": {}, "hash": "be44f587b7c44270baaafdcdcd20a7b1c9ff7574003944bd03ef0d00730023ac", "class_name": "RelatedNodeInfo"}}, "text": "00:03:19 The term y is just the target so that value is constant.\n00:03:26 Its derivative is zero.\n00:03:33 This term will have a derivative of x.\n00:03:39 This term is constant with respect to theta zero so it will have a derivative of zero as will the others.\n00:03:46 This leaves us with just one term here the derivative with theta zero x zero and we find that the derivative of e is negative x zero, the zeroth feature.\n00:03:53 We can repeat this\n00:03:59 for other parameters, theta one, theta two, and we'll find essentially the same thing.\n00:04:07 This part will remain the same, and over here instead of feature zero, we'll get feature one or feature two.\n00:04:14 Collecting these terms into a vector, the gradient is given by this formula here, where each term is fairly similar.\n00:04:22 Each entry in the vector is a sum over all the data, and each contains the same two over m sum over e sub j times something terms.\n00:04:29 So we can pull these out using the distributive rule for a more compact rewriting of the vector, given here.\n00:04:37 The gradient value has an intuitive interpretation, the product of one term that measures the error magnitude and its direction.\n00:04:44 So how much error have we made on data point j, and which direction should we go to decrease it?\n00:04:52 And another term over here that indicates how fast our error.\n00:04:59 changes if we change one of the parameters, say, theta 0 or theta 1.\n00:05:07 This term was the derivative of E with respect to theta.\n00:05:14 Since we're interested in taking a very small step, we should go in a direction that will have a large impact on the error.\n00:05:22 So, this is measuring the sensitivity of the error to each parameter theta i.\n00:05:29 Again, in MATLAB, this is compactly represented using matrix notation.\n00:05:37 Using our same definition before of the parameter vector theta, the vector of targets y, and the data matrix x, we can write the gradient as being the inner product between the error residual vector as before and the data matrix x.\n00:05:44 In MATLAB, this is easy to express.\n00:05:52 We just compute the error residual, and then we can compute the gradient, del j, just using that formula.\n00:05:59 And if we want to do one step of gradient descent, we simply update theta in the negative gradient direction.\n00:06:05 So just one compact line of MATLAB implements the inner loop of gradient descent.\n00:06:10 Now let's visualize the gradient descent procedure.\n00:06:16 We start with an initial parameter setting.\n00:06:21 So some blue dot indicates a particular value of theta.\n00:06:27 In this case, x1 equals 0, x0 equal minus 20.\n00:06:32 Those parameters correspond to some predictor.\n00:06:38 So we can plot that in the data space over here as the line minus 20 plus 0 times x.\n00:06:43 Evaluating the gradient and then taking a step, this moves us in parameter space from the initial value of theta to our first iteration's value of theta.\n00:06:49 This one is at theta 0 equal minus 10, theta 1 equal 1.\n00:06:54 It corresponds to a different model, this one with intercept minus 10 and slope 1.\n00:06:59 and so it produces these predictions for X, which we can see are considerably better than the initial ones.\n00:07:09 As we take more and more steps, we get an evolving value of theta and an evolving model and different predictions for each value of X, with the overall mean squared error tending to decrease.\n00:07:19 More steps changes the model parameters further, and the mean squared error slowly moves toward its minimum value.\n00:07:29 Gradient descent is an extremely general and useful algorithm that we'll use numerous times.\n00:07:39 However, being a descent procedure, it can be sensitive to initialization and can get stuck in local minima.\n00:07:49 Given two different initializations, we may be attracted to the same local minimum, say, this one here, but even a smaller initialization may be attracted to the same local minimum.", "start_char_idx": 3134, "end_char_idx": 7111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff47ee7d-d1cb-4b0e-af68-2d9c63118990": {"__data__": {"id_": "ff47ee7d-d1cb-4b0e-af68-2d9c63118990", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (2) Gradient descent.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WnqQrPNYz5Q", "Link": "https://www.youtube.com/watch?v=WnqQrPNYz5Q"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9bd8381-f1ca-4988-9f69-905b663fcd63", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (2) Gradient descent.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WnqQrPNYz5Q", "Link": "https://www.youtube.com/watch?v=WnqQrPNYz5Q"}, "hash": "9c66147e4b778de82ed3589b3baef6fa462c7c78a8898a4e393f21dcf781cce0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dca1e48-c968-4bba-940e-f5b9d6a7019b", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear regression (2) Gradient descent.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WnqQrPNYz5Q", "Link": "https://www.youtube.com/watch?v=WnqQrPNYz5Q"}, "hash": "e3377abcb0a889aaa7cb21359a8961ca6eebc6d540eb20c5c1610554931affa4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "292270a7-ab53-42ef-83d9-fc1d0bee6e04", "node_type": "1", "metadata": {}, "hash": "fb3460f6db9acc37347da12c46580d36be189a346571dad6d73b2ef1769696fb", "class_name": "RelatedNodeInfo"}}, "text": "00:06:59 and so it produces these predictions for X, which we can see are considerably better than the initial ones.\n00:07:09 As we take more and more steps, we get an evolving value of theta and an evolving model and different predictions for each value of X, with the overall mean squared error tending to decrease.\n00:07:19 More steps changes the model parameters further, and the mean squared error slowly moves toward its minimum value.\n00:07:29 Gradient descent is an extremely general and useful algorithm that we'll use numerous times.\n00:07:39 However, being a descent procedure, it can be sensitive to initialization and can get stuck in local minima.\n00:07:49 Given two different initializations, we may be attracted to the same local minimum, say, this one here, but even a smaller initialization may be attracted to the same local minimum.\n00:07:59 change in initialization to, say, this point, could give us a very different gradient direction, and thus a very different local minimum found by the procedure.\n00:08:09 In fact, any local minimum is an attractor for this procedure.\n00:08:19 And so depending on how we initialize theta and the step size we take, we can end up in any one of these local minima.\n00:08:29 Another problem is the choice of step size, which can influence both the convergence rate and behavior of our procedure.\n00:08:39 If the step size is chosen too large, we can jump over the minimum, perhaps jumping back, and it may take us a long time to converge.\n00:08:49 On the other hand, if we take our step size too small, we may take very small steps.\n00:08:59 and make very little progress at each one, also increasing the amount of time it takes to converge.\n00:09:06 We may also want to adapt or change the step size as we run the algorithm, often to, say, decrease it with iteration to improve convergence properties.\n00:09:13 Some common choices are to just leave it fixed over all iterations.\n00:09:19 Decrease it linearly with the number of iterations.\n00:09:26 This has nice convergence properties and proofs of convergence for fairly weak conditions.\n00:09:33 We also may want to use local properties of the function to decide what size step to take.\n00:09:39 So for instance, Newton's method uses the local curvature of the function, so the second order derivatives, to tell us how big of a step we should take.\n00:09:46 Another very useful variant of gradient descent is called stochastic or online gradient descent.\n00:09:53 In most machine learning problems, the loss function J decomposes into a.\n00:09:59 a sum or average over the loss having to do with each individual data point.\n00:10:07 So each data point might measure, say, a squared error, and our loss function would be the mean squared error, the average over them.\n00:10:14 By linearity, if this cost function decomposes, then the gradient also decomposes.\n00:10:22 So the gradient of J is also a sum or average over local gradients having to do with the gradient to improve the squared error of just data point J.\n00:10:29 So the idea underlying stochastic gradient descent is to update using only one data point at a time.\n00:10:37 We pick a data point J at random, update using only the gradient of J, and then repeat.\n00:10:44 In practice, usually, instead of choosing a random data point, we usually choose a random ordering over the data and then walk sequentially through them.\n00:10:52 This has the property that at any optimum of original gradient descent, so any minimum of J.\n00:10:59 where the gradient of J is 0, we have that on average over the random choice of the data point, our update will be 0.\n00:11:07 Of course, no individual update will actually be 0, just on average over the data point.\n00:11:14 Let's see how this works in the same plot.\n00:11:22 We have a parameter space, the values of theta, we have a model space with the observations and their targets, and a prediction given by our current model values theta.\n00:11:29 Given an initial value of theta, we pick a data point, compute its error, and then take a step toward reducing that particular data point's error.", "start_char_idx": 6259, "end_char_idx": 10369, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "292270a7-ab53-42ef-83d9-fc1d0bee6e04": {"__data__": {"id_": "292270a7-ab53-42ef-83d9-fc1d0bee6e04", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (2) Gradient descent.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WnqQrPNYz5Q", "Link": "https://www.youtube.com/watch?v=WnqQrPNYz5Q"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9bd8381-f1ca-4988-9f69-905b663fcd63", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (2) Gradient descent.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WnqQrPNYz5Q", "Link": "https://www.youtube.com/watch?v=WnqQrPNYz5Q"}, "hash": "9c66147e4b778de82ed3589b3baef6fa462c7c78a8898a4e393f21dcf781cce0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff47ee7d-d1cb-4b0e-af68-2d9c63118990", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear regression (2) Gradient descent.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WnqQrPNYz5Q", "Link": "https://www.youtube.com/watch?v=WnqQrPNYz5Q"}, "hash": "21bbab4650e76b18939020299ddbce68337f484aca189329916f6e34ae2f09bc", "class_name": "RelatedNodeInfo"}}, "text": "00:10:52 This has the property that at any optimum of original gradient descent, so any minimum of J.\n00:10:59 where the gradient of J is 0, we have that on average over the random choice of the data point, our update will be 0.\n00:11:07 Of course, no individual update will actually be 0, just on average over the data point.\n00:11:14 Let's see how this works in the same plot.\n00:11:22 We have a parameter space, the values of theta, we have a model space with the observations and their targets, and a prediction given by our current model values theta.\n00:11:29 Given an initial value of theta, we pick a data point, compute its error, and then take a step toward reducing that particular data point's error.\n00:11:37 We get a new model here, usually where that data point has a better prediction associated with it.\n00:11:44 And picking another data point, we then take a small step designed to reduce that data point's error.\n00:11:52 Note that this step is designed to decrease\n00:11:59 that particular point's error, it might even increase the error overall, but on average over these steps, the loss will decrease.\n00:12:09 Then we pick another data point, update, and so on until we feel we've converged.\n00:12:19 Stochastic gradient descent is often preferred for systems where we have many data, because it has better speed properties, while gradient descent needs to touch all M data points before updating the parameter vector even once, stochastic gradient descent begins updating theta immediately and updates it M times per pass through the data.\n00:12:29 So if we have lots of data, that will be many more updates per pass through the data.\n00:12:39 When the parameters are very far from optimal, say when we've just initialized them, almost all the data tend to indicate theta should be evolving in one direction.\n00:12:49 And so updating it quickly after only seeing a few data points is much more efficient than looking at all the data before changing it.\n00:12:59 it at all.\n00:13:08 However, since the procedure is now somewhat random and is no longer strictly descent, it can be harder to debug and also harder to assess convergence and stopping criteria in the code.\n00:13:17 For example, even just evaluating J for our stopping criteria requires actually touching all the data for a fixed value of theta.\n00:13:25 So often we may not want to do that, and so we might use a substitute approximate value of J where we just accumulate the error as we go through this loop.\n00:13:34 But that means that every error, J sub J, is associated with the value of theta that we saw at that particular point, which means the loss we calculate isn't associated with any particular value of theta.\n00:13:42 So it's not really the same as doing descent or calculating the value of J to stop.\n00:13:51 Another method one can use is to use mini-batch updates.\n00:13:59 this interpolates between stochastic gradient descent and batch gradient by using small groups of data to do these estimates.\n00:14:10 Overall, if we have a sufficient number of data, we typically prefer stochastic gradient descent or mini-batch descent to batch descent.", "start_char_idx": 9657, "end_char_idx": 12806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "deb239fa-0355-44af-831e-3e846245ebfe": {"__data__": {"id_": "deb239fa-0355-44af-831e-3e846245ebfe", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (3) Normal equations.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=N4d_9GQ9QFc", "Link": "https://www.youtube.com/watch?v=N4d_9GQ9QFc"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bfd7ab5a-ef0c-467c-81d7-3b0f04b82a7e", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (3) Normal equations.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=N4d_9GQ9QFc", "Link": "https://www.youtube.com/watch?v=N4d_9GQ9QFc"}, "hash": "4daa41264bf7a7461f85152d878e37def8e438a040b2f4de4e99f0d58dba3a38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f738bc61-20ba-4adb-a594-ea9e61a81636", "node_type": "1", "metadata": {}, "hash": "ddcabae5013d83e9f4e4c008701909fa6714ac17e0409017cb60c1d9cebd164d", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 For linear regression with the mean squared error loss, it turns out that we can also compute the optimal values of theta in closed form.\n00:00:10 Consider the following simple problem in which we have only two data points to fit a line.\n00:00:20 Our two data points give us two equations corresponding to each data, each of which has two unknowns, theta 0 and theta 1, and we can just solve this set of linear equations.\n00:00:30 As you may recall from linear algebra, in matrix form, we can simply write this down, y equals theta x, and solve it, which corresponds to the matrix inverse of x transpose.\n00:00:40 More generally, of course, x is not square and thus not invertible, but if m is greater than n, this corresponds to there being no linear function that hits all the data exactly, and instead we'll solve directly for the minimum of the mean squared error function.\n00:00:50 The minimum of J corresponds to the minimum of theta.\n00:01:00 points to the point where the gradient of J equals 0.\n00:01:05 So computing this and setting it equal to 0, we can distribute x inside the parentheses and move theta x transpose x to the other side.\n00:01:10 x transpose x is a square matrix.\n00:01:16 Assuming it's full rank, we can solve by multiplying by its inverse.\n00:01:21 And we find that the minimum of J is given at theta equal to y transpose x.\n00:01:27 x transpose x inverse.\n00:01:32 This quantity, x x transpose x inverse, is called the Moore-Penrose pseudo-inverse of x transpose.\n00:01:38 If x transpose is square and full rank, it's exactly equal to x transpose inverse.\n00:01:43 But in most practical situations, we have more data than features.\n00:01:49 So meaning m greater than n, in which case the pseudo-inverse gives the minimum mean squared error fit.\n00:01:54 Now that we have a linear algebraic square, let's definition the square.\n00:01:59 form for the optimal value of theta, this is easy to solve in MATLAB.\n00:02:10 We again write our Y and X matrix, and we can solve this either manually by simply executing the form of the equation, Y transpose X inverse of X transpose X, or by using a special MATLAB operator, the matrix right divide operation.\n00:02:20 This operator explicitly solves a set of linear equations defined by Y equals theta times X transpose.\n00:02:30 While this may look less obviously equivalent to our derivation, it lets MATLAB use a more numerically stable solution technique.\n00:02:40 One useful interpretation of the stationary point equation we solved gives them the name the normal equations.\n00:02:50 Consider the vector of target values, Y1 through Ym, an m-dimensional space where.\n00:03:00 M is the number of data points.\n00:03:06 We can also regard the data matrix as a collection of column vectors.\n00:03:13 So Xi will be a vector consisting of the i-th feature for all M data points.\n00:03:19 This is, again, a vector in M-dimensional space.\n00:03:26 So here, X1 is the vector of feature one across all the data, X2 is the vector of feature two across all the data, and so forth.\n00:03:33 Now, any linear combination, a linear predictor, must live in the span of these vectors.\n00:03:39 So our vector of predictions at all the data points is also an M-dimensional vector, which lives somewhere in this subspace, spanned by X1, X2.\n00:03:46 The normal equations say that the error residual, the difference between Y and Y hat, when dot-producted with the matrix X, equals 0.\n00:03:53 So that means this error vector is orthogonal to every column of X.\n00:03:59 that the difference between y, the vector of true targets, and y hat are predictions, that error vector is orthogonal to the space spanned by x.\n00:04:07 So far we've been looking at mean squared error, which is a useful and widely used loss function.\n00:04:14 One particularly nice aspect of it is that we can solve it exactly and fairly easily in closed form.\n00:04:22 However, mean squared error is not always the best choice.", "start_char_idx": 0, "end_char_idx": 3962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f738bc61-20ba-4adb-a594-ea9e61a81636": {"__data__": {"id_": "f738bc61-20ba-4adb-a594-ea9e61a81636", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (3) Normal equations.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=N4d_9GQ9QFc", "Link": "https://www.youtube.com/watch?v=N4d_9GQ9QFc"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bfd7ab5a-ef0c-467c-81d7-3b0f04b82a7e", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (3) Normal equations.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=N4d_9GQ9QFc", "Link": "https://www.youtube.com/watch?v=N4d_9GQ9QFc"}, "hash": "4daa41264bf7a7461f85152d878e37def8e438a040b2f4de4e99f0d58dba3a38", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "deb239fa-0355-44af-831e-3e846245ebfe", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear regression (3) Normal equations.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=N4d_9GQ9QFc", "Link": "https://www.youtube.com/watch?v=N4d_9GQ9QFc"}, "hash": "e2dba19a664add2082906176aee9a76055c90d0f32889da3097ea44631b1bf93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef3d2650-b6d0-4a66-81e4-acfed19b2906", "node_type": "1", "metadata": {}, "hash": "c2e2af8c765b438ee67ea5e00e0f0508550ac0bc3ef91ab79c175056c7fa60e4", "class_name": "RelatedNodeInfo"}}, "text": "00:03:39 So our vector of predictions at all the data points is also an M-dimensional vector, which lives somewhere in this subspace, spanned by X1, X2.\n00:03:46 The normal equations say that the error residual, the difference between Y and Y hat, when dot-producted with the matrix X, equals 0.\n00:03:53 So that means this error vector is orthogonal to every column of X.\n00:03:59 that the difference between y, the vector of true targets, and y hat are predictions, that error vector is orthogonal to the space spanned by x.\n00:04:07 So far we've been looking at mean squared error, which is a useful and widely used loss function.\n00:04:14 One particularly nice aspect of it is that we can solve it exactly and fairly easily in closed form.\n00:04:22 However, mean squared error is not always the best choice.\n00:04:29 Take the following data, say maybe y is house price where x is house size, and suppose we make a fit to it, in which case we learn the following model, the linear model of x.\n00:04:37 Now suppose we add a data point that doesn't follow the general trend.\n00:04:44 So here's an outlier, it's perhaps a particularly large house for which almost nothing was paid.\n00:04:52 Perhaps we've entered the data incorrectly, perhaps the householder simply sold to a relative.\n00:04:59 very cheaply.\n00:05:05 The error residual on this point is very large.\n00:05:11 So here it's 16, so the squared error is 16 squared.\n00:05:17 Because it's squared, reducing an error of 16 is much more important than reducing, say, 16 errors of size 1.\n00:05:23 Viewed another way, mean squared error is in some senses equivalent to a Gaussian assumption on error residuals.\n00:05:29 And if we look at the histogram of error residuals in this model, it's clearly not Gaussian.\n00:05:35 There's one mode over here near 0, and another mode over here.\n00:05:41 It's extremely far away, far out in the tails and outlier.\n00:05:47 If we optimize our model to minimize mean squared error with this data point included, we'll get a significant distortion in the model.\n00:05:53 So instead of learning the black line, we'll learn the blue line, because reducing the error on this...\n00:05:59 large squared error is extremely important and is worthwhile to distort ourselves from several of these other things that have smaller error.\n00:06:06 So now let's see how a different loss function might behave.\n00:06:13 Suppose instead of the mean squared error, we look at the mean absolute error, sometimes called the L1 error.\n00:06:19 So the sum or average of absolute differences between the target and our prediction.\n00:06:26 So here again in black is our squared error minimum using the original data points.\n00:06:33 If we fit this using mean absolute error instead, we get the green line, which is quite similar and still a quite good looking fit to the data.\n00:06:40 If we now include this outlier and retrain using the mean absolute error, we find that we get very little distortion.\n00:06:46 So in this case, we still have an error of 16, but we don't square that.\n00:06:53 So the error of 16 is worth the...\n00:07:00 same amount as 16 smaller errors of size 1.\n00:07:07 So it's not as important to our optimization procedure to reduce this big error.\n00:07:15 We often visualize the behavior of our loss functions by showing the total loss as a function of the error magnitude.\n00:07:22 So we find that mean squared error increases quadratically, so that small errors are relatively unimportant, but outliers or extreme large errors have a very big impact.\n00:07:30 Mean absolute error, on the other hand, the red curve, increases only linearly, so large outlier errors will have less impact.\n00:07:37 If we really want to reduce large errors, we could even design an alternative loss function that accounts for this.\n00:07:45 So for example, this blue one, which looks quadratic near zero, but as we get to higher and higher errors, asymptotes to some constant value.", "start_char_idx": 3151, "end_char_idx": 7113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef3d2650-b6d0-4a66-81e4-acfed19b2906": {"__data__": {"id_": "ef3d2650-b6d0-4a66-81e4-acfed19b2906", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (3) Normal equations.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=N4d_9GQ9QFc", "Link": "https://www.youtube.com/watch?v=N4d_9GQ9QFc"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bfd7ab5a-ef0c-467c-81d7-3b0f04b82a7e", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (3) Normal equations.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=N4d_9GQ9QFc", "Link": "https://www.youtube.com/watch?v=N4d_9GQ9QFc"}, "hash": "4daa41264bf7a7461f85152d878e37def8e438a040b2f4de4e99f0d58dba3a38", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f738bc61-20ba-4adb-a594-ea9e61a81636", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear regression (3) Normal equations.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=N4d_9GQ9QFc", "Link": "https://www.youtube.com/watch?v=N4d_9GQ9QFc"}, "hash": "458cd6120888a5e77404ac50216b88b8a1c557956b3493dfc429afaec5b6c0b5", "class_name": "RelatedNodeInfo"}}, "text": "00:07:07 So it's not as important to our optimization procedure to reduce this big error.\n00:07:15 We often visualize the behavior of our loss functions by showing the total loss as a function of the error magnitude.\n00:07:22 So we find that mean squared error increases quadratically, so that small errors are relatively unimportant, but outliers or extreme large errors have a very big impact.\n00:07:30 Mean absolute error, on the other hand, the red curve, increases only linearly, so large outlier errors will have less impact.\n00:07:37 If we really want to reduce large errors, we could even design an alternative loss function that accounts for this.\n00:07:45 So for example, this blue one, which looks quadratic near zero, but as we get to higher and higher errors, asymptotes to some constant value.\n00:07:52 So that no matter how wrong our prediction, no matter how large the error, the value in our\n00:08:00 our loss function will be bounded by some constant.\n00:08:07 Of course, optimizing these more arbitrary loss functions means that we can no longer use a closed form solution the way we did with mean squared error.\n00:08:14 But we can still optimize our loss function using something like gradient descent.", "start_char_idx": 6306, "end_char_idx": 7529, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b6c04fb-57ae-431f-98be-966ce6ac433b": {"__data__": {"id_": "4b6c04fb-57ae-431f-98be-966ce6ac433b", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (4) Nonlinear features.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=py8QrZPT48s", "Link": "https://www.youtube.com/watch?v=py8QrZPT48s"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fdf25942-8649-42aa-8b88-e46636dafd71", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (4) Nonlinear features.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=py8QrZPT48s", "Link": "https://www.youtube.com/watch?v=py8QrZPT48s"}, "hash": "758090465317e3c1ccbf8f062a5148968790130bb06852b9fc3ea0dde23c0153", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5149e57b-559b-4ff0-911e-1d550e65289b", "node_type": "1", "metadata": {}, "hash": "6c26ebe3f7226d82be2a18aa73e615d9a9fda2cfdbe08c0fb7b38efae46d8faa", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Now let's look at the impact of features on our linear regression problem and how using nonlinear features can extend our class of predictors.\n00:00:06 In a multidimensional feature space, our linear regression remains a linear function.\n00:00:12 So if we have two features, x1 and x2, and y is a linear function of those, we'll have a plane.\n00:00:18 We'll be predicting on a plane.\n00:00:24 If we have three features, a hyperplane, and so on.\n00:00:30 Otherwise, everything about the procedure remains essentially the same.\n00:00:36 We have a parameter vector theta, a feature vector for each data point, and our predictor is the dot product between those vectors.\n00:00:42 So what if we want to learn regression functions that aren't lines?\n00:00:48 For example, perhaps a cubic or other polynomial regression function.\n00:00:54 From this viewpoint, we'd like to use a scalar x to predict y, and we'd like to use a polynomial function.\n00:01:00 function of x, so that y-hat will be, say, theta 0 plus theta 1 x plus theta 2 x-squared plus theta 3 x-cubed.\n00:01:07 So we have a data set consisting of that scalar feature and our target.\n00:01:15 But now let's pretend that instead of observing one feature, we observe three.\n00:01:22 Denote feature 2 by x2 and set its value equal to the square of the first feature.\n00:01:30 Feature 3 will be x3, and set its value equal to the cube of the first feature.\n00:01:37 Rewriting our predictor, we see that now it's just a linear regression in the new features, theta 0, theta 1 times our first feature, and now theta 2 times the second and theta 3 times the third.\n00:01:45 So we can solve it using exactly the same technique as before.\n00:01:52 It's often helpful to think of this feature augmentation process as a feature transform phi.\n00:02:00 converts our original input features x into some new set of features that will be more useful and be used by our algorithm.\n00:02:07 To avoid confusion, sometimes we'll explicitly reference such a transform phi, in which case our linear regression function would be theta times the feature transform of x.\n00:02:15 So notice that this is just the same equation as before, except we've replaced the input features x by some post-processed features phi of x.\n00:02:22 We can then directly fit these polynomial functions using exactly the same framework and plot the prediction phi of x, where, for example, here our features phi of x are just the constant 1 and the input feature x.\n00:02:30 Here it's those plus x squared.\n00:02:37 Here it's those and x cubed as well.\n00:02:45 In our prediction step, whenever we want to predict a value of x, we just put it through this feature map phi.\n00:02:52 So here we put x through and we get the constant x at x squared and x cubed.\n00:03:00 before predicting.\n00:03:06 More generally, including more features can often be very helpful.\n00:03:13 In addition to collecting more information about each data example to produce new features, we can also use transforms of the features we've already observed.\n00:03:19 So if we observed some input features, we might define a feature transform that will give us polynomial features to learn more complex functions.\n00:03:26 Or other non-linear transforms as well.\n00:03:33 Whatever we think will be useful and have a direct linear relationship to the target.\n00:03:39 In general, the concept of linear regression, we mean linear in the parameters.\n00:03:46 Since the features were allowed to do whatever transforms we like.\n00:03:53 The most useful transforms will give us features that do, in fact, have a linear relationship with the target variable.\n00:03:59 to generate more features such as polynomials, the question is should we and where should we stop?\n00:04:07 Note that increasing the polynomial degree of our fit creates a nested sequence of models.", "start_char_idx": 0, "end_char_idx": 3852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5149e57b-559b-4ff0-911e-1d550e65289b": {"__data__": {"id_": "5149e57b-559b-4ff0-911e-1d550e65289b", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (4) Nonlinear features.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=py8QrZPT48s", "Link": "https://www.youtube.com/watch?v=py8QrZPT48s"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fdf25942-8649-42aa-8b88-e46636dafd71", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (4) Nonlinear features.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=py8QrZPT48s", "Link": "https://www.youtube.com/watch?v=py8QrZPT48s"}, "hash": "758090465317e3c1ccbf8f062a5148968790130bb06852b9fc3ea0dde23c0153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b6c04fb-57ae-431f-98be-966ce6ac433b", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear regression (4) Nonlinear features.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=py8QrZPT48s", "Link": "https://www.youtube.com/watch?v=py8QrZPT48s"}, "hash": "413ee06879b2ad0cd094bb41ce5159c252841b17d8991a9f115caeffc0af916a", "class_name": "RelatedNodeInfo"}}, "text": "00:03:19 So if we observed some input features, we might define a feature transform that will give us polynomial features to learn more complex functions.\n00:03:26 Or other non-linear transforms as well.\n00:03:33 Whatever we think will be useful and have a direct linear relationship to the target.\n00:03:39 In general, the concept of linear regression, we mean linear in the parameters.\n00:03:46 Since the features were allowed to do whatever transforms we like.\n00:03:53 The most useful transforms will give us features that do, in fact, have a linear relationship with the target variable.\n00:03:59 to generate more features such as polynomials, the question is should we and where should we stop?\n00:04:07 Note that increasing the polynomial degree of our fit creates a nested sequence of models.\n00:04:14 For example, out of all lines like this, the constant feature is one such line, so the best line fit will always be at least as good or better than the best constant fit.\n00:04:22 Similarly, all cubic functions include all lines, so the best cubic will be at least as good as the best line and so on.\n00:04:29 As we increase the number of features, we always will get some improvement in fit quality.\n00:04:37 At an extreme, given enough features, we'll actually hit all of the points exactly since we'll have m equations and m unknowns.\n00:04:44 But this kind of extreme is not such a good thing.\n00:04:52 Remember our example where we could explain the data with just a line plus noise or as an extremely complex function that hit all of the data.\n00:04:59 data.\n00:05:08 Even though more complex models will always fit the training examples better, they may not perform as well in the future, since they may overfit and learn patterns that aren't really present in the real system.\n00:05:17 To gauge our real performance once we've learned a model, we need additional data to see what the error rate will be once we go out and test on new examples.\n00:05:25 So, in this case, once we collect more examples, we can see that the linear fit is actually much better than the high-order polynomial.\n00:05:34 One way we can assess this is by holding out a set of validation data, here the green dots, or doing some kind of cross-validation to test for the performance of the model on data that it has not seen.\n00:05:42 We explicitly hide some of the data from our learning algorithm, and then we can use those points to estimate its performance on future data.\n00:05:51 When we plot performance as a function\n00:05:59 of features, in this case, polynomial order, we find the following trends.\n00:06:07 As we increase the polynomial order, so the number of features, we find that the performance on the training data, the red points that the algorithm sees as it optimizes data, gets better and better, it decreases.\n00:06:15 But if we look at the performance of new data that the model's not able to see when it selects data, we find that for a short period of time, as the model becomes more complex, our performance improves.\n00:06:23 So here, from constant to first order, linear, it gets better.\n00:06:31 But then, as we increase still further to second order, third order, and so forth, our performance on the test data may actually start to get worse.\n00:06:39 This is the overfitting phenomenon.", "start_char_idx": 3052, "end_char_idx": 6362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f0bee1b-c68d-453d-bc81-ecbf8ac787f3": {"__data__": {"id_": "8f0bee1b-c68d-453d-bc81-ecbf8ac787f3", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (5) Bias and variance.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=djRh0Rkqygw", "Link": "https://www.youtube.com/watch?v=djRh0Rkqygw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d68407ec-67f4-4efb-b3c6-b4fe4a6be662", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (5) Bias and variance.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=djRh0Rkqygw", "Link": "https://www.youtube.com/watch?v=djRh0Rkqygw"}, "hash": "80ff7e84579176ce5a844ddd49a8f4fca8e10d9cd5b891e731d034b1e2f272ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3b790f1-080e-42c1-b2f2-f119dfffbd4d", "node_type": "1", "metadata": {}, "hash": "d4fc46e21c5893e983e949fb2a575de35e9dec2bcd07aa11fda18468587322d2", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 A useful way of understanding underfitting and overfitting is as a balance between bias and variance.\n00:00:10 The term inductive bias refers to the set of assumptions that let us extrapolate from examples we have seen to similar examples that we have not.\n00:00:20 Bias is what makes us prefer one model over another, usually framed as types of functions that we think might be likely, like polynomial functions, or assumptions about functions, like smoothness.\n00:00:30 Some notion of bias is intrinsically necessary for learning, since there are many possible perfectly valid explanations of the data.\n00:00:40 For example, these data points might be explained equally well by a simple function that uses noise to explain the errors in its predictions, or as a complex function that needs no noise to explain them.\n00:00:50 Inductive bias is what lets us select between these equally valid explanations for the thing we think is most likely.\n00:01:00 The principle of Occam's razor leads us to prove the simpler of these two models typically.\n00:01:05 The variance of an estimator is another important effect.\n00:01:10 Imagine we had several draws of data sets, perhaps made in different universes, keeping everything else the same.\n00:01:16 So we'd go out to collect our data, and in these different universes, we happened to pull a different set of people or other examples.\n00:01:21 So the system that gave rise to these data is exactly the same, but the measurement set is a little bit different.\n00:01:27 So in Universe 1, we get red points.\n00:01:32 In Universe 2, we get these green points.\n00:01:38 In Universe 3, we get these blue points, but all of them from the same data-generating system.\n00:01:43 In each universe, we go out and we learn a model for these data, and the question is, how different are they?\n00:01:49 For simple models, we'll find that they're close to the same.\n00:01:54 So if we learn a constant model, we find the blue line, green line, and red line are all quite close to one another.\n00:01:59 In this case, the prediction is the average of the data, and the averages are all quite similar.\n00:02:10 A more complex model, however, will fit the training data more closely, and as a result, they'll tend to differ more and more across the data sets.\n00:02:20 So, for example, these lines, red, green, and blue, are slightly more different, and by the time we learn cubics or higher-order functions, we find that the shapes of the functions we learn are quite different across the data sets.\n00:02:30 Since we could just as easily have ended up with any of these sets, if our predictions vary too wildly, that suggests that our performance on new data will also be poor, since, for example, the green curve is not close to the red curve, and hence, it's also not close to the red points, which we could easily have gotten for test data in the future.\n00:02:40 High variance is exactly the overfitting effect.\n00:02:50 We do better on the data that we see than we will on future data.\n00:03:00 future data, alternative samples.\n00:03:06 To balance these effects, we need to choose the right model complexity.\n00:03:13 As we've seen, one approach is to use holdout data.\n00:03:19 So we split the data into a validation or test set that's not seen by the model and then use it to estimate the model's future performance.\n00:03:26 We can then compare several different models and complexities and choose the one that performs the best.\n00:03:33 All good competitions use this formulation, often with multiple splits.\n00:03:39 For example, one test set may be used to give feedback, like a leaderboard, but since this can then be optimized, since the predictors can see its value and select models that will do well on it, another test set needs to be held out for the final scoring.\n00:03:46 Furthermore, even within the training set, you may want to split the data one or more times to do your own model selection and evaluation as well.\n00:03:53 So what can we do about under and overfitting?", "start_char_idx": 0, "end_char_idx": 4032, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3b790f1-080e-42c1-b2f2-f119dfffbd4d": {"__data__": {"id_": "f3b790f1-080e-42c1-b2f2-f119dfffbd4d", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (5) Bias and variance.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=djRh0Rkqygw", "Link": "https://www.youtube.com/watch?v=djRh0Rkqygw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d68407ec-67f4-4efb-b3c6-b4fe4a6be662", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (5) Bias and variance.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=djRh0Rkqygw", "Link": "https://www.youtube.com/watch?v=djRh0Rkqygw"}, "hash": "80ff7e84579176ce5a844ddd49a8f4fca8e10d9cd5b891e731d034b1e2f272ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f0bee1b-c68d-453d-bc81-ecbf8ac787f3", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear regression (5) Bias and variance.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=djRh0Rkqygw", "Link": "https://www.youtube.com/watch?v=djRh0Rkqygw"}, "hash": "a603b2d022349e0f89ef4a68c94d79331a1a10d9c777909e92c8d2b525b989a4", "class_name": "RelatedNodeInfo"}}, "text": "00:03:19 So we split the data into a validation or test set that's not seen by the model and then use it to estimate the model's future performance.\n00:03:26 We can then compare several different models and complexities and choose the one that performs the best.\n00:03:33 All good competitions use this formulation, often with multiple splits.\n00:03:39 For example, one test set may be used to give feedback, like a leaderboard, but since this can then be optimized, since the predictors can see its value and select models that will do well on it, another test set needs to be held out for the final scoring.\n00:03:46 Furthermore, even within the training set, you may want to split the data one or more times to do your own model selection and evaluation as well.\n00:03:53 So what can we do about under and overfitting?\n00:03:59 If we believe our model is underfitting, we can reduce the underfitting by increasing the complexity of the model.\n00:04:08 For example, we could add extra features and hence increase the number of parameters.\n00:04:16 To reduce overfitting, we need to decrease the complexity of the model, often by increasing our bias, say, reducing the number of features, for example, feature selection, or even just forcing our model to underperform and hence fail to memorize the data.\n00:04:24 One trivial historical way of doing this is a technique called early stopping.\n00:04:32 During optimization, we simply don't fully optimize the function, but we stop after a fixed number of iterations, say.\n00:04:40 But a more principled and common way is to add a regularization penalty, which we'll discuss in the next module.", "start_char_idx": 3211, "end_char_idx": 4854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2132b4c6-ed46-4f94-a04f-d653467bb4fb": {"__data__": {"id_": "2132b4c6-ed46-4f94-a04f-d653467bb4fb", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (6) Regularization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sO4ZirJh9ds", "Link": "https://www.youtube.com/watch?v=sO4ZirJh9ds"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "824186cb-a303-46b4-b44b-aff2c4e05976", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (6) Regularization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sO4ZirJh9ds", "Link": "https://www.youtube.com/watch?v=sO4ZirJh9ds"}, "hash": "928cb325b0e366a60e0ea3fc35eff3ec5c7ea2d6b5fe7de48ab1769efdc8a650", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b2225aa-9426-4650-87a8-ae02e6fa24ac", "node_type": "1", "metadata": {}, "hash": "6d9924fd8b6e56834fc8925b36b7b5053e5616e0512c4dbf668c69476b8a121c", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Regularization is a common way of controlling for and reducing overfitting in a flexible and tunable manner.\n00:00:04 Consider fitting a linear model to just two data points, resulting in a unique model with zero error.\n00:00:09 So we fit this line.\n00:00:13 Suppose now we want to make our model more flexible and use quadratic functions.\n00:00:18 Now instead of one unique solution, there are actually an infinite number of quadratic functions that have zero error.\n00:00:23 They also pass through those two points.\n00:00:27 So how can we choose among them?\n00:00:32 One trivial way would be to always choose the lowest order polynomial that fits.\n00:00:36 So ensure that all higher coefficients are zero if they can be.\n00:00:41 But this technique is pretty specific to polynomials.\n00:00:46 It requires that we know exactly what these features are.\n00:00:50 A method that's agnostic to the feature definitions might be to choose the parameter values that have the minimum magnitude.\n00:00:55 So the minimum size.\n00:00:59 or Euclidean norm among all models that have zero error.\n00:01:06 Either of these choices is a type of bias.\n00:01:13 They tell us which models to prefer in the absence of evidence from the data.\n00:01:19 This is exactly how regularization works.\n00:01:26 We bias the values of theta toward particular values, for example, small values near zero, by adding a penalty term that encourages those values.\n00:01:33 Optimizing the same way as before, we now balance one term, the data reconstruction error, with another term, the regularization penalty.\n00:01:40 If we optimize the same way as before, we get a similar closed-form solution.\n00:01:46 But now, instead of just y times the pseudo-inverse of x, we now have x transpose x plus an additional term, alpha times the identity.\n00:01:53 This added value regularizes this matrix inverse.\n00:02:00 so that now it's better conditioned and it will have a unique solution for any degree polynomial and number of data.\n00:02:07 Using this squared error regularization term on linear regression with mean squared error is sometimes called ridge regression.\n00:02:15 The overall effect is to shrink the values of theta toward zero by an amount that depends on the value of alpha.\n00:02:22 If alpha is very large then we tend to care more about reducing the parameter magnitude than about reducing mean squared error.\n00:02:30 For alpha going toward infinity we actually choose theta equals zero.\n00:02:37 Note that the regularization term is actually independent of the data so if we emphasize it it reduces the dependence on the actual draw of data and hence reduces the variance of our model but may increase its bias.\n00:02:45 Here's a picture of how this works in practice using the same data point in each plot.\n00:02:52 If we just fit polynomials\n00:03:00 minimizing only the squared error term.\n00:03:08 We start off with a linear function, but as we go to higher and higher order polynomials, we get some very crazy looking functions with pretty extreme values trying to hit every red point.\n00:03:17 But if we add a regularization term, here we chose alpha equal to 1, our model is unwilling to choose the extremely large parameter values that would be necessary to reduce the mean squared error so much.\n00:03:25 The decrease in the mean squared error would be offset by the increase in the norm of theta.\n00:03:34 So as we do higher order polynomials, even very high polynomials, with a regularization, we tend to still learn a simple looking function.\n00:03:42 So the regularization is helping us balance the concepts of not having too much error with keeping small parameter values.\n00:03:51 There are many different possible choices of regularization with different effects on the parameters learned.\n00:04:00 A common class of regularizers is to choose one based around the family of Lp vector norms for different values of p.\n00:04:08 For p equals 2, we sum the squared values of theta and take the square root, and so this is just the Euclidean distance.\n00:04:17 It's easy to visualize these Lp norms by sketching the shape of their isosurfaces, so the surface on which the norm takes on constant value, say 1.", "start_char_idx": 0, "end_char_idx": 4207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b2225aa-9426-4650-87a8-ae02e6fa24ac": {"__data__": {"id_": "4b2225aa-9426-4650-87a8-ae02e6fa24ac", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (6) Regularization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sO4ZirJh9ds", "Link": "https://www.youtube.com/watch?v=sO4ZirJh9ds"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "824186cb-a303-46b4-b44b-aff2c4e05976", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (6) Regularization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sO4ZirJh9ds", "Link": "https://www.youtube.com/watch?v=sO4ZirJh9ds"}, "hash": "928cb325b0e366a60e0ea3fc35eff3ec5c7ea2d6b5fe7de48ab1769efdc8a650", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2132b4c6-ed46-4f94-a04f-d653467bb4fb", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear regression (6) Regularization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sO4ZirJh9ds", "Link": "https://www.youtube.com/watch?v=sO4ZirJh9ds"}, "hash": "596ff3a3b849a5e6b2ce817eb28ef1719deb6ffae58029903998091ef2b63ef2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46c8cbc6-b40e-40b0-ba93-31e8ed3a7153", "node_type": "1", "metadata": {}, "hash": "5a2bb043334388c20c8600c6093e9e0d2c95527867b480ea8606277fc74f1610", "class_name": "RelatedNodeInfo"}}, "text": "00:03:34 So as we do higher order polynomials, even very high polynomials, with a regularization, we tend to still learn a simple looking function.\n00:03:42 So the regularization is helping us balance the concepts of not having too much error with keeping small parameter values.\n00:03:51 There are many different possible choices of regularization with different effects on the parameters learned.\n00:04:00 A common class of regularizers is to choose one based around the family of Lp vector norms for different values of p.\n00:04:08 For p equals 2, we sum the squared values of theta and take the square root, and so this is just the Euclidean distance.\n00:04:17 It's easy to visualize these Lp norms by sketching the shape of their isosurfaces, so the surface on which the norm takes on constant value, say 1.\n00:04:25 For the Euclidean distance, p equals 2, this is just the circle, the set of points with radius 1, with distance 1 from the origin.\n00:04:34 On the other hand, if we pick p equals 1, we get the sum of absolute values, and the isosurface looks like this star shape, a rotated square.\n00:04:42 Increases in one value of one parameter theta must be exactly offset by decreases in the other.\n00:04:51 If p is very large, the shape of the isosurface tends to look like this.\n00:05:00 to approach a square, so the set of theta such that the maximum value is 1.\n00:05:07 For p very small, here I'm picturing 0.5, the picture, the shape is very peaked, where we're allowed to have large values of one parameter, like theta 0, only if the other parameter, theta 1, is very near 0.\n00:05:15 In the limit, as p approaches 0, this approaches the counting norm.\n00:05:22 So it counts the number of non-zero parameter values.\n00:05:30 And this is a natural notion of complexity, since it measures how many parameter values are actually used by the model.\n00:05:37 The two most common choices of regularization are the L2 and the L1 norm.\n00:05:45 Optimizing a data term plus the regularization balances the mean squared error with a regularization cost.\n00:05:52 And we can picture this as the balance of two different losses, one loss.\n00:06:00 that determines how well we reconstruct the data.\n00:06:05 And that's minimized at this blue point.\n00:06:10 And as we move away from it, it increases quadratically.\n00:06:15 And the other term is a regularization term that's minimized at the origin, so all parameters equal to zero.\n00:06:20 And it increases, again, in the case of the L2 norm, quadratically away.\n00:06:25 So the combination of the data plus the regularization will be minimized at some point that touches both surfaces.\n00:06:30 Since in order to reduce the regularization, we would have to leave this isosurface and increase the data loss, and vice versa.\n00:06:35 So we can't reduce either loss without increasing the other.\n00:06:40 L1 penalties have the effect that they can encourage sparse parameter vectors.\n00:06:45 A sparse vector is one that lies exactly on some axis.\n00:06:50 For example, one of the parameters, say theta 1, is exactly zero.\n00:06:55 The sparser, the more parameters will be exactly zero.\n00:07:00 The L2 optimum over here will only be on one of the axes, so it will only be sparse if this minimum MSE point is also exactly on the axis.\n00:07:12 This will happen only with probability zero.\n00:07:24 But on the L1 optimized regularized system, the L1 optimum can be on the axis even when the blue point is not sometimes.\n00:07:36 So because its contour is sharp at that point, it means that it's possible for it to intersect with these blue ovals on the axis even when the minimum MSE point is not located on the axis.", "start_char_idx": 3395, "end_char_idx": 7065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46c8cbc6-b40e-40b0-ba93-31e8ed3a7153": {"__data__": {"id_": "46c8cbc6-b40e-40b0-ba93-31e8ed3a7153", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear regression (6) Regularization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sO4ZirJh9ds", "Link": "https://www.youtube.com/watch?v=sO4ZirJh9ds"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "824186cb-a303-46b4-b44b-aff2c4e05976", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear regression (6) Regularization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sO4ZirJh9ds", "Link": "https://www.youtube.com/watch?v=sO4ZirJh9ds"}, "hash": "928cb325b0e366a60e0ea3fc35eff3ec5c7ea2d6b5fe7de48ab1769efdc8a650", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b2225aa-9426-4650-87a8-ae02e6fa24ac", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear regression (6) Regularization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sO4ZirJh9ds", "Link": "https://www.youtube.com/watch?v=sO4ZirJh9ds"}, "hash": "4d38d4ccf9ccaf85fb5fd251920d3a36c7f5416431711e11311e5d3f9e145d7f", "class_name": "RelatedNodeInfo"}}, "text": "00:06:45 A sparse vector is one that lies exactly on some axis.\n00:06:50 For example, one of the parameters, say theta 1, is exactly zero.\n00:06:55 The sparser, the more parameters will be exactly zero.\n00:07:00 The L2 optimum over here will only be on one of the axes, so it will only be sparse if this minimum MSE point is also exactly on the axis.\n00:07:12 This will happen only with probability zero.\n00:07:24 But on the L1 optimized regularized system, the L1 optimum can be on the axis even when the blue point is not sometimes.\n00:07:36 So because its contour is sharp at that point, it means that it's possible for it to intersect with these blue ovals on the axis even when the minimum MSE point is not located on the axis.\n00:07:48 So L1 regularized solutions tend to encourage some level of sparsity, which is often helpful because it makes our model more efficient to store, more efficient to compute, and it can also reveal which features are most important to the prediction, since less important features may be forced to be exactly zero.\n00:08:00 Interestingly, LP norms form a balance between sparsity and convexity.\n00:08:07 So for P greater than or equal to 1, the norm is convex and thus relatively easy to optimize.\n00:08:14 But for P less than or equal to 1, it induces some sparsity.\n00:08:22 So the L1 norm is the only norm which both induces some sparsity in the solution and remains convex for easy optimization.", "start_char_idx": 6333, "end_char_idx": 7771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c60c1d2e-2cd0-4364-8584-fdb48fc08239": {"__data__": {"id_": "c60c1d2e-2cd0-4364-8584-fdb48fc08239", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Decision Trees (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WOOTNBxbi8c", "Link": "https://www.youtube.com/watch?v=WOOTNBxbi8c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5968ec8e-fdf7-4303-9893-a11fcf769a1d", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Decision Trees (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WOOTNBxbi8c", "Link": "https://www.youtube.com/watch?v=WOOTNBxbi8c"}, "hash": "9018f1decf1c68358b189b5a97251ea0c6148a15bd16e767e6fee7997108b67d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "415f5947-a43c-4c6d-a488-21de9d7b8b11", "node_type": "1", "metadata": {}, "hash": "51d8417f62e052d55f9e65865bbf55826d708f8ed0846809938a2fbdac9daf1f", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Decision trees are another flexible class of functions, or learners, that are commonly used in machine learning.\n00:00:07 A decision tree represents a function consisting of a series of comparisons, or if-then-else statements.\n00:00:15 Each branch may lead to another comparison in branch, or to an output value.\n00:00:22 We can draw this as a tree, where the outputs are the leaf nodes.\n00:00:30 I'll draw true values going to the right, and false values going to the left.\n00:00:37 For example, this decision tree corresponds to a function that outputs class red if feature 1 compares to greater than 0.5. If it compares to less than 0.5, we move to another decision node, where we compare feature 2 to another threshold, say in this case 0.5 again.\n00:00:45 If that comparison is false, we output class blue.\n00:00:52 If its output is true, we continue to yet another branch in comparison, where we look at feature 1 again, compared to a threshold 0.5.\n00:01:00 And if that's true, we output red.\n00:01:06 If it's false, we output blue.\n00:01:13 The output for this function is given over on the right, where the first comparison, x1 greater than 0.5, leads us to determine that this entire region will predict red.\n00:01:20 The second comparison, x2 greater than 0.5, leads us to make a decision in this region.\n00:01:26 If x2 is less than 0.5, we decide blue, leading to blue in that entire region.\n00:01:33 And finally, we compare x1 to 0.1 in the remaining region.\n00:01:40 If x1 is less than 0.1, we predict blue, thus blue in that region.\n00:01:46 If it's red, if it's greater, we predict red, thus red in this region here.\n00:01:53 For features that have discrete values, we can do one of several things.\n00:02:00 since the notion of threshold is not very meaningful.\n00:02:07 Instead, we typically have a few possible options.\n00:02:15 First, we could branch on all possible values of the feature.\n00:02:22 So if we compare a discrete feature, x1, that has four possible values, a, b, c, and d, then perhaps we make a branch for each of those.\n00:02:30 On the other hand, we may prefer to have a binary tree.\n00:02:37 In that case, we might compare feature x1 and branch, say, to the left only if it equals a, and branch to the right if it equals any other value.\n00:02:45 Or we might have a more general comparison where we say if x1 is in some set, say a and d, branch left, and if it's not, if it's in any other value, branch right.\n00:02:52 The advantages and disadvantages of this in the left-hand case, that since everything down the far left path\n00:03:00 we'll have x1 equal to a.\n00:03:07 There's never any advantage to comparing x1 to any other value below that point, because x1 will always equal a in that branch.\n00:03:15 On the other hand, in the right-hand branch, many of these branches correspond to multiple possible values of x1, and thus x1 might appear again and need to be split.\n00:03:22 However, in that case, we'll have a binary tree, and that may be easier for us to represent.\n00:03:30 The complexity of the function of a decision tree depends on the depth of the tree.\n00:03:37 For example, a depth one decision tree, which is often called a decision stump, since it's cut off at the root, is an extremely simple classifier.\n00:03:45 Here we could look at only one feature, we compare to some threshold, and if the comparison is false, we output one value, and if the comparison is true, we output another value.\n00:03:52 So in, for example, a two-dimensional feature space, x1, x2, this would lead to functions which partition the space up.\n00:04:00 either left-right or top-bottom into two different classes.", "start_char_idx": 0, "end_char_idx": 3658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "415f5947-a43c-4c6d-a488-21de9d7b8b11": {"__data__": {"id_": "415f5947-a43c-4c6d-a488-21de9d7b8b11", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Decision Trees (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WOOTNBxbi8c", "Link": "https://www.youtube.com/watch?v=WOOTNBxbi8c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5968ec8e-fdf7-4303-9893-a11fcf769a1d", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Decision Trees (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WOOTNBxbi8c", "Link": "https://www.youtube.com/watch?v=WOOTNBxbi8c"}, "hash": "9018f1decf1c68358b189b5a97251ea0c6148a15bd16e767e6fee7997108b67d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c60c1d2e-2cd0-4364-8584-fdb48fc08239", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Decision Trees (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WOOTNBxbi8c", "Link": "https://www.youtube.com/watch?v=WOOTNBxbi8c"}, "hash": "7b9b7903d525c42c3f421e390176871d2646bfef5f1f120ecbf7bf0f3ca14e89", "class_name": "RelatedNodeInfo"}}, "text": "00:03:22 However, in that case, we'll have a binary tree, and that may be easier for us to represent.\n00:03:30 The complexity of the function of a decision tree depends on the depth of the tree.\n00:03:37 For example, a depth one decision tree, which is often called a decision stump, since it's cut off at the root, is an extremely simple classifier.\n00:03:45 Here we could look at only one feature, we compare to some threshold, and if the comparison is false, we output one value, and if the comparison is true, we output another value.\n00:03:52 So in, for example, a two-dimensional feature space, x1, x2, this would lead to functions which partition the space up.\n00:04:00 either left-right or top-bottom into two different classes.\n00:04:06 Extending our decision tree to two levels leads to a more complex set of functions, since we can now compare x1 to some threshold.\n00:04:13 And then within each of those domains, we can further compare x1 or x2 to another threshold.\n00:04:20 And thus, we'll be partitioning our space up to four different output regions, each of which can have its own class assigned to it.\n00:04:26 In general, for a tree of depth d, we can end up with up to 2 to the d regions, each of which has their own prediction in it.\n00:04:33 Decision trees can be used for regression in almost exactly the same way.\n00:04:40 Here, instead of outputting a discrete class value, we output a real-valued number at the leaf nodes, since our prediction y is over some real-valued set.\n00:04:46 Again, the functions look essentially the same.\n00:04:53 So here, we're drawing pictures with a single.\n00:05:00 feature X, single feature X to predict a real value target Y.\n00:05:08 And if we have a single depth, we partition that X feature space up into two parts, left and right, each of which has its own real value prediction associated with it.\n00:05:16 For a depth 2 tree, we divide the feature space up into up to four regions based on thresholds.\n00:05:24 So here we might have four different real value predictions.", "start_char_idx": 2922, "end_char_idx": 4959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78d4fb6d-a10d-4ebb-a3f1-eb836fe2f061": {"__data__": {"id_": "78d4fb6d-a10d-4ebb-a3f1-eb836fe2f061", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81b9bd71-f7de-42af-b524-7a932c901501", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "fe386147345284bf23631edcc7473fa35a7821d19263743c2b6068b0d6ced6bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0951632a-9f6d-4adc-bc0b-893acbdce41c", "node_type": "1", "metadata": {}, "hash": "d4be97a9ba5168462fe77cc073ce34fc9fffcb0c87590ca23b16a47bd7bdee31", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Now that we understand the prediction structure of a decision tree, we would like to be able to select a good decision tree structure, given some training examples, in other words, learn a decision tree from data.\n00:00:06 The most common learning procedure is to greedily select a way of splitting the training data.\n00:00:12 In particular, we proceed recursively.\n00:00:18 Given a set of training points, we make a few decisions.\n00:00:24 First, we decide whether or not this should be a leaf node.\n00:00:30 If it should, we'll be outputting a prediction.\n00:00:36 If not, we decide how we should further split the data.\n00:00:42 Example algorithms that are of this type include CART, which stands for Classification and Regression Tree Methods, and specific instances like the ID3 and C4.5 algorithms.\n00:00:48 You can see all of these in Wikipedia.\n00:00:54 For leaf nodes, we output a prediction, and it's fairly easy to think about what the best prediction, given this type of data, is.\n00:01:00 data might be.\n00:01:08 In particular, for classification, we usually pick the majority class, since that will get the majority, the largest set of the training data for that group correct.\n00:01:17 In regression, we typically do something like predict the average value, since this is the constant value that will minimize the mean squared error.\n00:01:25 For non-leaf nodes, if we decide that we'd like to split the data further, we need to choose two things.\n00:01:34 First, which feature we should split on, and secondly, which test we will perform in order to split the data into, say, two parts.\n00:01:42 And typically, we use a greedy scoring method, where we score every possible feature and every possible split that we could do among those data, and we score each split using some function that measures the purity of the class data after the split.\n00:01:51 The idea being that if we split the data and the subsets are of a strong majority,\n00:01:59 of one class or another, our predictions will be easier at that point.\n00:02:05 Finally, we need to decide when we will make a leaf node or whether we will continue to split the data.\n00:02:12 And these usually will return to this point, but we can make decisions like when all the training data are in the same class.\n00:02:18 Obviously, we can predict that class and get those training examples correct.\n00:02:24 When all the data are indistinguishable, there are no further features we could split on to do better.\n00:02:30 Or we might use heuristics like a fixed depth, which will give us a fixed complexity of the function.\n00:02:36 So let's think about how to score a possible split.\n00:02:42 Suppose we're considering splitting one feature, say Feature 1.\n00:02:48 And Feature 1 is, let's say, a continuous feature.\n00:02:54 One greedy method we could do is to simply choose the split that has the best accuracy in terms of the lowest misclassification rate if we had to predict\n00:03:00 at the next step.\n00:03:07 So that's a possibility, but it turns out that in practice, a soft scoring method can often work better.\n00:03:15 One measure of the purity of a variable or its concentration into one or a few values is called the entropy.\n00:03:22 Entropy is a notion from information theory which is commonly used in communications, data compression, and many other areas.\n00:03:30 It essentially measures how random an outcome is, and the randomness of the outcome measures also how hard it is to communicate that outcome to you.\n00:03:37 So the difficulty of representing something depends on the relative probability of all the outcomes.\n00:03:45 In particular, consider an example where I need to communicate 365 bits of information to you.\n00:03:52 Let's say I first do 365 fair coin toss.\n00:04:00 So the output is some sequence of heads and tails, which each of which is equally likely.\n00:04:07 So I have two to the three hundred and sixty five possible outcomes.\n00:04:15 And you might imagine rightly that the sequence will take three hundred and sixty five bits to represent.", "start_char_idx": 0, "end_char_idx": 4065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0951632a-9f6d-4adc-bc0b-893acbdce41c": {"__data__": {"id_": "0951632a-9f6d-4adc-bc0b-893acbdce41c", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81b9bd71-f7de-42af-b524-7a932c901501", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "fe386147345284bf23631edcc7473fa35a7821d19263743c2b6068b0d6ced6bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78d4fb6d-a10d-4ebb-a3f1-eb836fe2f061", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "2d4be6885cc243fb031744d4338c6dc0dc2691dfb4c53289b3080927dd0f558a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c904fd0-7790-46f6-9dc0-0b7bdcecb61e", "node_type": "1", "metadata": {}, "hash": "44d1690ba0dab7eff4c273de5d976f5ccf8d0b35f829ab9d5e62e634101ca8cd", "class_name": "RelatedNodeInfo"}}, "text": "00:03:22 Entropy is a notion from information theory which is commonly used in communications, data compression, and many other areas.\n00:03:30 It essentially measures how random an outcome is, and the randomness of the outcome measures also how hard it is to communicate that outcome to you.\n00:03:37 So the difficulty of representing something depends on the relative probability of all the outcomes.\n00:03:45 In particular, consider an example where I need to communicate 365 bits of information to you.\n00:03:52 Let's say I first do 365 fair coin toss.\n00:04:00 So the output is some sequence of heads and tails, which each of which is equally likely.\n00:04:07 So I have two to the three hundred and sixty five possible outcomes.\n00:04:15 And you might imagine rightly that the sequence will take three hundred and sixty five bits to represent.\n00:04:22 But now consider a different three hundred and sixty five length sequence where instead of communicating fair coin tosses, I'm communicating the outcome of my daily lottery playing.\n00:04:30 Now, since I almost always lose, the output is most likely to be lose, lose, lose, lose all the way through the entire year.\n00:04:37 So I could choose a representation for this, which would be much more efficient to represent the outcome than sending three hundred and sixty five bits.\n00:04:45 I'll choose a encoding that tells you first just whether or not I lost.\n00:04:52 And that takes only one bit since almost all.\n00:05:00 time I lose, I almost all the time only need to send you one bit.\n00:05:07 With some very small chance, I'll have won sometime during the year, and I'll have to send more bits.\n00:05:15 I'll send a bit that tells you that I won, and then I'll have to send some information to tell you when I won.\n00:05:22 If I won twice, I'd have to send even more bits to tell you that I had won more than once.\n00:05:30 But these outcomes have increasingly low probability, and so they almost never will happen.\n00:05:37 And so on average, as I send these to you, most of the time it will only take one bit to communicate them.\n00:05:45 So the reason that it takes less work for me to communicate my lottery averages is essentially because the outcome is much less random.\n00:05:52 Since I can manage to just use a few bits for the most likely outcome, and use more bits for the less likely ones, on average I'll have to send very few bits.\n00:06:00 The entropy of a random variable X is a notion that makes this idea concrete.\n00:06:10 So, H of X measures the randomness of the random variable X, and it's just calculated by summing over all possible outcomes of X, weighting by the probability of that outcome, and multiplying by log one over the probability of the outcome, or equivalently minus log P of X.\n00:06:20 Typically, we'll use log base two here, although I didn't write log sub two, since that makes the units of entropy bits, and that measures how many bits one would take to represent this outcome on average.\n00:06:30 So, here are a few examples.\n00:06:40 Suppose we have a random variable X that takes on one of four possible values, and each of those values is equally likely, so they each have probability 0.25.\n00:06:50 Then, following this formula, H of X is probability of outcome one, so 0.25, times log one over P, so log of four.\n00:07:00 Same thing for the outcome 2, same thing for outcome 3, same term for outcome 4, because they all have the same probability.\n00:07:07 If you calculate this out, you'll find that it's log 4, or 2 bits.\n00:07:15 Here's another example where X also has four outcomes, but two of these outcomes, outcomes 3 and 4, have zero probability.\n00:07:22 Outcome 1 has probability .75, and outcome 2 has probability .25.", "start_char_idx": 3217, "end_char_idx": 6956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c904fd0-7790-46f6-9dc0-0b7bdcecb61e": {"__data__": {"id_": "0c904fd0-7790-46f6-9dc0-0b7bdcecb61e", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81b9bd71-f7de-42af-b524-7a932c901501", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "fe386147345284bf23631edcc7473fa35a7821d19263743c2b6068b0d6ced6bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0951632a-9f6d-4adc-bc0b-893acbdce41c", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "72cc29d5ac2723e7c70d0456d610b9922bc9431825bb6e9746efb9aaa7904e42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2d9ba94-8a2b-42e1-be77-0852437937b9", "node_type": "1", "metadata": {}, "hash": "aa7d465df8c5ff9d0bda8f7f7716eb5993a0de8dab2b6ee2335fe760201cc55e", "class_name": "RelatedNodeInfo"}}, "text": "00:06:40 Suppose we have a random variable X that takes on one of four possible values, and each of those values is equally likely, so they each have probability 0.25.\n00:06:50 Then, following this formula, H of X is probability of outcome one, so 0.25, times log one over P, so log of four.\n00:07:00 Same thing for the outcome 2, same thing for outcome 3, same term for outcome 4, because they all have the same probability.\n00:07:07 If you calculate this out, you'll find that it's log 4, or 2 bits.\n00:07:15 Here's another example where X also has four outcomes, but two of these outcomes, outcomes 3 and 4, have zero probability.\n00:07:22 Outcome 1 has probability .75, and outcome 2 has probability .25.\n00:07:30 Now, following the same formula, we use the convention that 0 log 0 is 0, and we compute that the entropy is .75 times log of 1 over .75, plus .25 times log of 1 over .25, and the other terms are 0.\n00:07:37 And if you calculate this out, you'll find it's about .8 bits, so slightly less than 1 bit.\n00:07:45 If we had two equally probable outcomes, it would actually be 1 bit.\n00:07:52 So you can see that having one outcome be more likely than not.\n00:08:00 the other has reduced the entropy.\n00:08:06 Finally, here's an example where we have a random variable X that could take on four values, but with probability 1, it takes on outcome 1.\n00:08:13 So this actually is a deterministic variable.\n00:08:20 It's not actually random.\n00:08:26 It always takes on value 1.\n00:08:33 If you compute the entropy, you find 1 log 1, and then 0 log 0 for the other terms are 0, and the total entropy is 0 bits, meaning that representing a deterministic thing takes me 0 bits, since we both know what the outcome will be before I send anything.\n00:08:40 Turns out that the uniform distribution over here on the left is the maximum entropy for the number of outcomes that can happen.\n00:08:46 So here, four outcomes can take a maximum of 2 bits, and deterministic functions like the one on the right are the minimum entropy, or 0 bits.\n00:08:53 So how can we use entropy to decide how to split\n00:08:59 our data into two parts to make our prediction problem easier.\n00:09:07 There's a quantity called information, or information gain, that measures the change in randomness once you know something.\n00:09:14 So this tells you how much a piece of information will tell you about an outcome that was random.\n00:09:22 So we compute the information gain by a difference of entropies.\n00:09:29 We start by computing the entropy of the class variable, so the outcome, blue and red, in our total data set here.\n00:09:37 So here I have 10 blue points and 8 red points.\n00:09:44 And so if I treat that as a distribution, I have 10 18ths probability of outcome blue and 8 18ths probability of outcome red.\n00:09:52 And if you compute the entropy of that, you find that it's nearly 1 bit, so the entropy is almost 1 bit.\n00:09:59 Now consider some split where let's say I've divided the data into those data points that fall in X1 less than a half and those data that fall in X1 greater than a half.\n00:10:05 And so this is going to put most of the blue over on the left and on the right we have only red points.\n00:10:10 So again, let's compute the entropy of the left-hand set and the right-hand set.\n00:10:16 So these are the data sets that we would split into at the next level of the tree.\n00:10:21 So the left-hand set has 10 outcomes of blue and two outcomes of red.\n00:10:27 So now it's 10 twelfths probability of outcome blue and 2 twelfths probability of outcome red.", "start_char_idx": 6248, "end_char_idx": 9822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2d9ba94-8a2b-42e1-be77-0852437937b9": {"__data__": {"id_": "e2d9ba94-8a2b-42e1-be77-0852437937b9", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81b9bd71-f7de-42af-b524-7a932c901501", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "fe386147345284bf23631edcc7473fa35a7821d19263743c2b6068b0d6ced6bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c904fd0-7790-46f6-9dc0-0b7bdcecb61e", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "8ee9c7593a4a5d9a9d38ca540e53c2916b059d24b74dec42c78b97d4e831e9d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59503582-ad79-4685-a003-1c1b84ae1ac5", "node_type": "1", "metadata": {}, "hash": "6824ef2e6b7338f1f12e28b0797040a5384657c8aeeb567292c0f08181b73359", "class_name": "RelatedNodeInfo"}}, "text": "00:09:59 Now consider some split where let's say I've divided the data into those data points that fall in X1 less than a half and those data that fall in X1 greater than a half.\n00:10:05 And so this is going to put most of the blue over on the left and on the right we have only red points.\n00:10:10 So again, let's compute the entropy of the left-hand set and the right-hand set.\n00:10:16 So these are the data sets that we would split into at the next level of the tree.\n00:10:21 So the left-hand set has 10 outcomes of blue and two outcomes of red.\n00:10:27 So now it's 10 twelfths probability of outcome blue and 2 twelfths probability of outcome red.\n00:10:32 If you compute the entropy of that, it's gone down slightly.\n00:10:38 It's now 0.77 bits.\n00:10:43 The entropy on the right-hand side, on the other hand, is all red in the data.\n00:10:49 So we have five out of five data points being red.\n00:10:54 This distribution is determined.\n00:11:00 and so we find that the entropy is zero.\n00:11:06 We also compute the fraction of data that went to the left and right.\n00:11:12 So here the left-hand side got 13 eighteenths of the data, and the right-hand side got 5 eighteenths of the data.\n00:11:18 We then calculate the information gain as the weighted difference of entropies.\n00:11:24 So we calculate the probability that we went down the left-hand branch, 13 eighteenths, times the difference in entropies, 0.99 minus 0.77.\n00:11:30 We add to it the probability that we go down the right-hand branch, times the difference in entropies on that branch, so 0.99 minus 0.\n00:11:36 There are several equivalent ways of writing this.\n00:11:42 In particular, this form that I've written here is a more classic representation of the mutual information.\n00:11:48 Here I've written it between the split variable, whether we go left or right, and the class variable, whether it's blue or red.\n00:11:54 But the most common way of writing this is to write it as a matrix.\n00:12:00 representation of this is in this form that I've given here as the weighted average of difference entropies.\n00:12:07 To see why this is useful, consider some visibly less good division of the data.\n00:12:15 Here we put most of the data on the left and only a little bit of data on the right.\n00:12:22 And again, since the data points are the same, the entropy to start with is exactly the same, and it's only the entropy of our two subdivisions that will be different.\n00:12:30 So now the subdivision on the left has 17 out of 18 data points, 7 of them are red and 10 of them are blue, and if we calculate the entropy of that, we find that it's 0.97 bits.\n00:12:37 On the other hand, the right-hand branch over here has only one point, so it's probability 1 18th that will fall in that box.\n00:12:45 And again, it's deterministic, so its entropy is zero.\n00:12:52 And if you calculate the information gain, you find that 17 18th times this distance difference in entropy.\n00:13:00 plus 1 18th times this difference of entropy is significantly smaller than the information gain in the previous slide.\n00:13:07 So that indicates that this is a less desirable split of data.\n00:13:15 The split has told us less about the class variable than a different split would have.\n00:13:22 Mainly we'll use information gain to tell us how to split data for a decision tree, but another common method of measuring impurity is called the Gini index.\n00:13:30 So I feel like I need to mention it.\n00:13:37 It's used in almost exactly the same formulas as I showed for entropy, except instead of measuring impurity via the entropy score, so here I've written H entropy is say minus sum over outcomes, the probability of the outcome, log probability of the outcome.", "start_char_idx": 9166, "end_char_idx": 12894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59503582-ad79-4685-a003-1c1b84ae1ac5": {"__data__": {"id_": "59503582-ad79-4685-a003-1c1b84ae1ac5", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81b9bd71-f7de-42af-b524-7a932c901501", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "fe386147345284bf23631edcc7473fa35a7821d19263743c2b6068b0d6ced6bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2d9ba94-8a2b-42e1-be77-0852437937b9", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "eaabf85348ef0a3e83cc396f8130b1b619f044f6d5327271cea80dbf39ecfc16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0959206a-ee7b-4d33-bf00-5337825e9cae", "node_type": "1", "metadata": {}, "hash": "4357fe00d20d6af9ee4cc712440817daf8735a4de626622cf7b8b11fc389ec4b", "class_name": "RelatedNodeInfo"}}, "text": "00:13:00 plus 1 18th times this difference of entropy is significantly smaller than the information gain in the previous slide.\n00:13:07 So that indicates that this is a less desirable split of data.\n00:13:15 The split has told us less about the class variable than a different split would have.\n00:13:22 Mainly we'll use information gain to tell us how to split data for a decision tree, but another common method of measuring impurity is called the Gini index.\n00:13:30 So I feel like I need to mention it.\n00:13:37 It's used in almost exactly the same formulas as I showed for entropy, except instead of measuring impurity via the entropy score, so here I've written H entropy is say minus sum over outcomes, the probability of the outcome, log probability of the outcome.\n00:13:45 Instead we measure an impurity score called the Gini score, which is related to a variance score.\n00:13:52 So again we sum over all the outcomes.\n00:14:00 but we calculate P times 1 minus P instead of P log P.\n00:14:06 If you plot this, you'll see that it measures impurity in much the same way.\n00:14:13 So, for instance, for a binary variable, the largest value of H entropy is when P is a half.\n00:14:20 If you plot this function, you'll see that its largest value is also when P is a half, but the shapes are slightly different, so they'll choose slightly different splits.\n00:14:26 So, again, the calculation that you go through is much the same.\n00:14:33 If you take the entire selection of data here, 8 red points and 10 blue points, you can calculate the Gini impurity as being 0.247.\n00:14:40 And then in the left-hand split where we have 10 blue and 3 red, you'll find that the Gini impurity is 0.198.\n00:14:46 The right-hand side that's all red, you find that the Gini impurity is 0 again.\n00:14:53 And now the Gini index...\n00:15:00 which measures the reduction in impurity is, again, the weighted difference.\n00:15:08 So, we calculate 13 eighteenths for the left-hand branch times the difference in its Gini impurities, plus 5 eighteenths for the right-hand branch times the difference of its impurities.\n00:15:17 So, entropy and Gini index are useful methods for classification problems.\n00:15:25 We can also do decision trees for regression, and in that case, since we're predicting a real-valued number, and our data have real-valued target values associated with them rather than class variables, those things don't really make sense.\n00:15:34 So, instead, typically, we measure the variance in the set, and we can measure the expected variance reduction.\n00:15:42 So, here, for instance, we have a full data set on the vertical axis is Y, and on the horizontal axis is our single feature X.\n00:15:51 We can measure the variance of the set.\n00:15:59 those data, let's say it's 0.25, and we can consider some split on X that splits them into left-hand and right-hand sections, and we can measure the variance of the left-hand and right-hand sections as, say, 0.1 and 0.2, and the fraction of the data, say, four-tenths of them took the left-hand branch and six- tenths of them would have taken the right-hand branch, and then the variance reduction is just the weighted difference again.\n00:16:11 So four-tenths times the difference in variances plus six-tenths times the difference in variances on the right-hand side.\n00:16:23 So you can think about the pseudocode for making a decision tree as being basically a recursive function, something like when to split the data, decision tree split data, and it takes in a set of data X and its associated target values Y.\n00:16:35 The first thing it does is perhaps decide whether it will be outputting a decision or not, making this a leaf node.\n00:16:47 So it checks some conditions, which we'll discuss further, to decide whether or not this is a leaf node.", "start_char_idx": 12119, "end_char_idx": 15926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0959206a-ee7b-4d33-bf00-5337825e9cae": {"__data__": {"id_": "0959206a-ee7b-4d33-bf00-5337825e9cae", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81b9bd71-f7de-42af-b524-7a932c901501", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "fe386147345284bf23631edcc7473fa35a7821d19263743c2b6068b0d6ced6bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59503582-ad79-4685-a003-1c1b84ae1ac5", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "75421c135fda8343691d6c447bed3729a15e39cb19a24539928be13f8ddb6cd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b566d6c-6e13-4b35-aebb-dcdf7504edb0", "node_type": "1", "metadata": {}, "hash": "1cf8d0f8c5230d9dd2e146416e625c487d95ada8c35d29f02705340b640664a2", "class_name": "RelatedNodeInfo"}}, "text": "00:16:11 So four-tenths times the difference in variances plus six-tenths times the difference in variances on the right-hand side.\n00:16:23 So you can think about the pseudocode for making a decision tree as being basically a recursive function, something like when to split the data, decision tree split data, and it takes in a set of data X and its associated target values Y.\n00:16:35 The first thing it does is perhaps decide whether it will be outputting a decision or not, making this a leaf node.\n00:16:47 So it checks some conditions, which we'll discuss further, to decide whether or not this is a leaf node.\n00:16:59 If it's not, it's an internal node, and we're going to be deciding to split the data into a left-hand and right-hand branch.\n00:17:08 So we check all the possible features, all the possible ways we could split the data.\n00:17:17 So for continuous features, we typically sort all the data, and we compute all the points where a data point and the next data point have different values, and we can split at their midpoint.\n00:17:25 So we think about all these splits.\n00:17:34 We score each of them using some kind of measure of impurity, like the information gain or the variance reduction.\n00:17:42 This tells us we just keep track of whichever feature and split scored the best, and we choose that feature and split to be the decision at that node.\n00:17:51 If we've decided this is an internal node, we then split the data according to that rule that we've just chosen, giving us two subsets, a left-hand subset of the data and a right-hand subset of the data.\n00:18:00 recursively call the same function on the left-hand branch and the right-hand branch, which will decide what their subtrees look like.\n00:18:10 So the only thing that remains that I haven't talked about are the stopping conditions, so the decisions that tell us whether we'll be outputting a prediction or making a rule to split the data further.\n00:18:20 And these are commonly things like checking whether the depth of the tree is greater than some predetermined threshold D, or whether the number of data that we're given is sufficiently large that we feel we could learn a rule on it.\n00:18:30 Or what I mentioned at the beginning, if all the data are indistinguishable, obviously nothing that we split on can help us anymore.\n00:18:40 Or we might say stop if we think that our prediction is sufficiently accurate.\n00:18:50 So if we're only mispredicting one data point, even though we haven't gotten them all correct, we might stop there.\n00:19:00 think that a good way of stopping is to ask whether the information gain or say Gini index is high enough and if that gain isn't high enough to stop.\n00:19:15 It turns out that's not a very good stopping rule for the simple reason that it might be that one split doesn't help us very much but two splits produces a really good rule.\n00:19:30 So here's a simple example of that where we've got an XOR like function and any vertical or horizontal split will still end up with about 50% of the data being blue and red on both sides but at the next level of the tree we can then split those further and we'll end up with a rule that predicts all the training data correctly.\n00:19:45 So typically instead of having a threshold that tells us to stop and when our gain is not large enough we just build the tree much further and then we prune the tree back analyzing whether the levels that we had helped us or not.\n00:20:00 we build all the way to this point, and that way we could tell that this tree actually was successful even though one of the internal nodes had a very low gain.\n00:20:05 So here's a few examples of the growing complexity of decision trees and how they can be controlled with certain parameters.\n00:20:10 So here I'm exploring depth cutoffs.\n00:20:16 Here's a picture of a decision tree that's of depth 1.\n00:20:21 So at depth 1 it's a binary split on one feature.\n00:20:27 So here we've split into predicting on the left-hand and right-hand sides, and since we only have two leaf nodes we predict blue on one side and red on the other.", "start_char_idx": 15308, "end_char_idx": 19415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b566d6c-6e13-4b35-aebb-dcdf7504edb0": {"__data__": {"id_": "7b566d6c-6e13-4b35-aebb-dcdf7504edb0", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81b9bd71-f7de-42af-b524-7a932c901501", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "fe386147345284bf23631edcc7473fa35a7821d19263743c2b6068b0d6ced6bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0959206a-ee7b-4d33-bf00-5337825e9cae", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "a411009dcd267794d7366dc029f2aa30e3edc96b6ee453e25dcfdd2f019d7529", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "525598fe-a218-4f11-bdf1-901fa523417c", "node_type": "1", "metadata": {}, "hash": "ef86e10326ca24f492285b826e09017d7e2524ab403f4e753c5299cc72145765", "class_name": "RelatedNodeInfo"}}, "text": "00:20:00 we build all the way to this point, and that way we could tell that this tree actually was successful even though one of the internal nodes had a very low gain.\n00:20:05 So here's a few examples of the growing complexity of decision trees and how they can be controlled with certain parameters.\n00:20:10 So here I'm exploring depth cutoffs.\n00:20:16 Here's a picture of a decision tree that's of depth 1.\n00:20:21 So at depth 1 it's a binary split on one feature.\n00:20:27 So here we've split into predicting on the left-hand and right-hand sides, and since we only have two leaf nodes we predict blue on one side and red on the other.\n00:20:32 If we go one more depth down, now we've subdivided it.\n00:20:38 See here's exactly the same division we had before.\n00:20:43 In the left-hand region it's subdivided again into now two more regions, and the right-hand region is subdivided into two regions.\n00:20:49 And here we can now make four predictions.\n00:20:54 If we go to depth 3, each of those\n00:21:00 regions is further subdivided in two, and we can end up with a slightly more complex function.\n00:21:08 Depth 4 has up to 2 to the 4th regions being predicted, depth 5 up to 2 to the 5th, and if we are willing to go on without any limit, we end up with this much more complex looking function up here on the top right.\n00:21:17 So, setting a depth parameter can act to control the complexity of the function that can be learned.\n00:21:25 Another threshold that I mentioned was the minimum number of data to be used in a parent node.\n00:21:34 So, I'll use the notation min parent here.\n00:21:42 So, if we set here as min parent grows, we tend to have less complex functions.\n00:21:51 So, for instance, the first one, the full decision tree that I showed up on the right corresponds to min parent that means that I'm willing to split a node if it has any more than one data.\n00:22:00 Over here is minParent3, where I'm willing to split a node only if it has three data or more, or rather more than three data.\n00:22:07 And so you can see that some of the much smaller regions that were making independent decisions are no longer able to support splits.\n00:22:15 minParent5, so only split things that have six or more data points, ends up merging some of those together in a slightly simpler function.\n00:22:22 minParent10, an even simpler function, and so forth.\n00:22:30 MATLAB has a few functions built in to do decision trees.\n00:22:37 The decision tree functions are in the stats toolbox these days, in a function called classRegTree.\n00:22:45 If you have an old version of MATLAB, there used to be a standalone function called TreeFit that can do it.\n00:22:52 And these decision trees can do both regression or classification, and the way they decide which that you want to do is by the type of the variable Y.\n00:23:00 if Y is a double, which is the most common variable type for MATLAB, it assumes that you want to do regression.\n00:23:12 If you want to do classification, you have to convert Y into a logical array for a binary classifier.\n00:23:24 Its default is to use the Gini index, but you can have it use other things like information gain as well.\n00:23:36 It's nice, it can show you a picture of your decision tree with these arrow nodes telling you decision nodes, and what feature it is, and what the threshold was, and branching left and right, you come to another internal node, or at a leaf node, it tells you what the prediction is, and it can output a text form of this, which tells you lists each of the nodes in turn, and tells you which node to go to as you walk through the decision tree.\n00:23:48 So, in summary, decision trees are a common, flexible, functional form for machine learning.\n00:24:00 They work by a set of nested if-then-else statements that at each level pick a single variable or a single feature and a condition on which to split.", "start_char_idx": 18771, "end_char_idx": 22660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "525598fe-a218-4f11-bdf1-901fa523417c": {"__data__": {"id_": "525598fe-a218-4f11-bdf1-901fa523417c", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81b9bd71-f7de-42af-b524-7a932c901501", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "fe386147345284bf23631edcc7473fa35a7821d19263743c2b6068b0d6ced6bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b566d6c-6e13-4b35-aebb-dcdf7504edb0", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}, "hash": "774d4da71b5f25564f7b13b0df2d6a6fce84208c5e3799379f9aa49e608f180e", "class_name": "RelatedNodeInfo"}}, "text": "00:23:24 Its default is to use the Gini index, but you can have it use other things like information gain as well.\n00:23:36 It's nice, it can show you a picture of your decision tree with these arrow nodes telling you decision nodes, and what feature it is, and what the threshold was, and branching left and right, you come to another internal node, or at a leaf node, it tells you what the prediction is, and it can output a text form of this, which tells you lists each of the nodes in turn, and tells you which node to go to as you walk through the decision tree.\n00:23:48 So, in summary, decision trees are a common, flexible, functional form for machine learning.\n00:24:00 They work by a set of nested if-then-else statements that at each level pick a single variable or a single feature and a condition on which to split.\n00:24:08 For continuous variables, usually it's a greater-than or less-than condition.\n00:24:17 At the leaves of this tree, it predicts a value, either a class or a real-valued number for regression.\n00:24:25 In learning decision trees, we typically follow a greedy recursive pattern where we score all possible splits and pick the best split among them, and we score them using some kind of measure of impurity, like the information gain or the Gini index for classification or a variance reduction score for regression.\n00:24:34 The other important thing is what our stopping criteria are, since that determines how complex of a decision tree we'll end up learning.\n00:24:42 The complexity of our function depends on a number of things.\n00:24:51 The main one is depth, since the total number of independent outcomes will be\n00:25:00 be two to the number of depths at most.\n00:25:08 And I mentioned in the previous set that a decision stump is a particular phrase used for a single-level decision tree, meaning we choose a single feature and split it once.\n00:25:17 These are very simple classifiers, even simpler than linear classifiers.", "start_char_idx": 21832, "end_char_idx": 23800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9418fddb-01f5-4bec-893d-75a157c6819a": {"__data__": {"id_": "9418fddb-01f5-4bec-893d-75a157c6819a", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear classifiers (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hAnfnc2dWBs", "Link": "https://www.youtube.com/watch?v=hAnfnc2dWBs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea172faa-1bf1-48bf-8470-6e4ea7dde048", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hAnfnc2dWBs", "Link": "https://www.youtube.com/watch?v=hAnfnc2dWBs"}, "hash": "475e739cce399c56f474e2d734410d5c99cd4576e1c5cc444ca2ae83e3d40e52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2e44479-2d65-43dd-ad9a-6e63d3182a0b", "node_type": "1", "metadata": {}, "hash": "d507e5eebae9e248a70c9765640dffee860761ab9738c0c9e9ce21821f6929e4", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 We'll now discuss the use of linear models for classification.\n00:00:08 As usual, we'll be doing supervised learning, where we have features X used to predict targets Y.\n00:00:17 Our predictions will be called Y-hat, and the model parameters will be called theta.\n00:00:25 When we considered linear regression, we predicted a real-valued target Y using a linear form, theta 0 plus theta 1 X1, and so on.\n00:00:34 In contrast, when we classify, we're required to predict a discrete-valued target Y, and so our outputs must also be discrete-valued.\n00:00:42 Let's consider a problem with only two classes, so a binary classifier, say classes plus 1, positive, and minus 1, negative.\n00:00:51 A simple idea is to just compute the same linear response, so theta 0 plus theta 1 X1,\n00:00:59 and so on, but then convert that output into a discrete class just by taking the sign of the linear response.\n00:01:06 In other words, we predict plus one if the linear computation was positive and predict minus one if it was negative.\n00:01:13 Here's a visualization with only one feature x explicitly drawing the class y.\n00:01:20 So these points are class minus one, these points are class plus one.\n00:01:26 Then a linear function of x, f of x, is thresholded to produce a binary output prediction.\n00:01:33 Historically, linear classifiers are often called perceptrons.\n00:01:40 While mostly use theta to represent the parameters, they're sometimes called weights, so w is another common variable name.\n00:01:46 As with regression, we'll assume there's a constant input feature to simplify our notation.\n00:01:53 In our linear classifier, it computes the...\n00:02:00 weighted sum of the input features, just as in regression, but converts it by thresholding it to produce a binary output.\n00:02:07 The term perceptron derives from a notion that this threshold of weighted inputs could be some kind of oversimplified model of a neuron, so that the neuron would fire, output plus one, if enough of its inputs were sufficiently stimulated by synapses, the weighted features.\n00:02:15 Notationally, we have our d input features, x1 through xd, plus an additional constant feature, x0.\n00:02:22 Together, we'll refer to this as the feature vector.\n00:02:30 We similarly have d plus 1 weights, theta, one for each feature, together called the parameter vector.\n00:02:37 Our linear response is now just the dot product between these vectors, which computes the weighted sum theta 0 times feature 0 plus theta 0 times feature 0.\n00:02:45 Plus theta 1 times feature 1.\n00:02:52 Then the output prediction is produced by.\n00:03:00 threshold such as the sine function, which outputs plus 1 for positive values and minus 1 for negative values.\n00:03:06 A MATLAB implementation of this is fairly trivial.\n00:03:13 We can compute the linear response just with a vector product here.\n00:03:20 Then we could alternatively compute it explicitly with a sum over the indices.\n00:03:26 And then we can convert our linear response into a discrete output just using the sine function.\n00:03:33 An important characterization of classifiers is their decision boundary.\n00:03:39 Since here we switch output values when we transition from negative to positive, the decision boundary is the set of points where the linear response equals 0, so transitioning from negative to positive.\n00:03:46 Solving this linear equation, we can see that the decision boundary is also a linear surface.\n00:03:53 So for two features, the decision boundary will be a line, for three it will be a plane, for more it will be a hyperplane.\n00:03:59 Specifically, it's given by solving for that linear response equal to zero.\n00:04:06 So here's a quick example of computing the decision boundary for a model with two features.\n00:04:13 Suppose our parameter vector is given by 0.5 minus 0.5, zero.\n00:04:19 Clearly for this point, if X2 is positive and X1 is zero, then our response will be negative.\n00:04:26 So our prediction will be class minus one.", "start_char_idx": 0, "end_char_idx": 4004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2e44479-2d65-43dd-ad9a-6e63d3182a0b": {"__data__": {"id_": "d2e44479-2d65-43dd-ad9a-6e63d3182a0b", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear classifiers (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hAnfnc2dWBs", "Link": "https://www.youtube.com/watch?v=hAnfnc2dWBs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea172faa-1bf1-48bf-8470-6e4ea7dde048", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hAnfnc2dWBs", "Link": "https://www.youtube.com/watch?v=hAnfnc2dWBs"}, "hash": "475e739cce399c56f474e2d734410d5c99cd4576e1c5cc444ca2ae83e3d40e52", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9418fddb-01f5-4bec-893d-75a157c6819a", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hAnfnc2dWBs", "Link": "https://www.youtube.com/watch?v=hAnfnc2dWBs"}, "hash": "9a155f95a7af860eaeb5769e0ff32a27cce3fd2da3e690800103d9442acd30bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc45f623-2cc3-4c80-9e99-5a571a8452e0", "node_type": "1", "metadata": {}, "hash": "9eee4e29e3b88b0f080329a3a846024c72d5d2839a59e0841afe2b77d3f53804", "class_name": "RelatedNodeInfo"}}, "text": "00:03:46 Solving this linear equation, we can see that the decision boundary is also a linear surface.\n00:03:53 So for two features, the decision boundary will be a line, for three it will be a plane, for more it will be a hyperplane.\n00:03:59 Specifically, it's given by solving for that linear response equal to zero.\n00:04:06 So here's a quick example of computing the decision boundary for a model with two features.\n00:04:13 Suppose our parameter vector is given by 0.5 minus 0.5, zero.\n00:04:19 Clearly for this point, if X2 is positive and X1 is zero, then our response will be negative.\n00:04:26 So our prediction will be class minus one.\n00:04:33 At this other point, X1 is positive and X2 is zero, so our response will be positive.\n00:04:40 The decision boundary lives somewhere in between these points and corresponds to the solution of this linear equation, theta times X equals zero.\n00:04:46 If we write this out and explicitly solve, we can manipulate until we find that X2 equals some function of X1.\n00:04:53 So it in fact involves a line, and X2 equals X1 is the solution.\n00:05:00 On this side of the decision boundary, X2 is greater than X1, and so in this part we predict class minus 1 because our linear response is negative.\n00:05:08 On this side of the decision boundary, X1 is greater than X2, our linear response is positive, and we output class plus 1.\n00:05:17 A useful concept in classification is the notion of separability of a dataset.\n00:05:25 We say that a dataset is separable by a particular learner if there exists some instance of that learner, so some set of parameter values, that will correctly predict the targets of all the data points.\n00:05:34 For a linear model, the data are linearly separable if there exists a line that divides the data by their class.\n00:05:42 For example, over here, we have two classes, red and blue.\n00:05:51 They're well separated in the two-dimensional feature space, feature 1 and feature 2, and we can easily draw a line that will separate the two classes.\n00:05:59 and thus predict them correctly.\n00:06:05 On this data set, on the other hand, while we still have the same two classes, they're no longer well-separated.\n00:06:11 And we can see that any line that we draw will fail to get all of the data points correct.\n00:06:17 Non-separability just means that the data are not perfectly predictable by the model.\n00:06:23 This often happens when features do not provide enough information to know the target.\n00:06:29 For example, when predicting, say, a credit default, given only the features of, say, age and income, it's quite plausible that for any particular values of age and income, there could be examples, data points, coming from either class.\n00:06:35 So this might indicate that we need more information if we want to do a better job.\n00:06:41 We might want to gather extra features.\n00:06:47 Or that we may need to model a more complex relationship between the features we do have and the class.\n00:06:53 In some cases, the target might not be perfectly predictable given any.\n00:06:59 reasonable amount of information and any model will have some errors.\n00:07:07 So here's another example of two classes, red and blue, and we can see that they are not linearly separable.\n00:07:14 There's no way that we can draw a line that divides the data in two parts such that we don't make any errors.\n00:07:22 But on the other hand, if we were allowed a more complex nonlinear decision boundary, we might be able to separate the data into those two classes.\n00:07:29 So our separability is a function of the class of models that we're allowed to use to do the prediction.\n00:07:37 Here's a classic example with binary valued features to help see some of the limitations of a linear function.\n00:07:44 Suppose we try to use a linear model to represent a binary and function.", "start_char_idx": 3358, "end_char_idx": 7214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc45f623-2cc3-4c80-9e99-5a571a8452e0": {"__data__": {"id_": "cc45f623-2cc3-4c80-9e99-5a571a8452e0", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear classifiers (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hAnfnc2dWBs", "Link": "https://www.youtube.com/watch?v=hAnfnc2dWBs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea172faa-1bf1-48bf-8470-6e4ea7dde048", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hAnfnc2dWBs", "Link": "https://www.youtube.com/watch?v=hAnfnc2dWBs"}, "hash": "475e739cce399c56f474e2d734410d5c99cd4576e1c5cc444ca2ae83e3d40e52", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2e44479-2d65-43dd-ad9a-6e63d3182a0b", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hAnfnc2dWBs", "Link": "https://www.youtube.com/watch?v=hAnfnc2dWBs"}, "hash": "5f549a7af1ea8ad6d8ccb15c6fc2f7db8b79f3e12250c75a9b2125a46aece620", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "710f7a71-1614-472f-ac06-d6bffc79e888", "node_type": "1", "metadata": {}, "hash": "56380b0fa85edb511561459d20838f5a3c74a611a005ff9ed857d66b4023d240", "class_name": "RelatedNodeInfo"}}, "text": "00:06:59 reasonable amount of information and any model will have some errors.\n00:07:07 So here's another example of two classes, red and blue, and we can see that they are not linearly separable.\n00:07:14 There's no way that we can draw a line that divides the data in two parts such that we don't make any errors.\n00:07:22 But on the other hand, if we were allowed a more complex nonlinear decision boundary, we might be able to separate the data into those two classes.\n00:07:29 So our separability is a function of the class of models that we're allowed to use to do the prediction.\n00:07:37 Here's a classic example with binary valued features to help see some of the limitations of a linear function.\n00:07:44 Suppose we try to use a linear model to represent a binary and function.\n00:07:52 So we have two binary features, X1 with value 0 and 1, and X2 with value 0 and 1, and we output class positive if both...\n00:07:59 features take value 1 and we output class negative otherwise.\n00:08:07 If we want to learn this function we can exactly reproduce this output just by using the following linear classifier with a decision boundary here and class positive on the upper side.\n00:08:14 But now if we try to do the same thing for an XOR function where the output class positive when feature 0 and feature 1 match and negative otherwise we can't.\n00:08:22 There's no line we can put through here that will correctly separate these four points.\n00:08:29 So this function is not linearly inseparable and not representable by a perceptron.\n00:08:37 Just as in linear regression we can increase the power of our class of functions by artificially expanding its set of features and hence the corresponding number of parameters.\n00:08:45 For example the following data with a one-dimensional feature X is not linearly separable.\n00:08:52 There's no function of the four\n00:09:00 threshold on BX plus C that will correctly divide these data into red and blue.\n00:09:06 But if we add an additional feature whose value is X squared, we can now plot it in a two-dimensional feature space.\n00:09:13 X on the horizontal and the value X squared on the vertical.\n00:09:19 So now the data lives in a one-dimensional manifold in that 2D feature space.\n00:09:26 And when we look at the data, we see that it now is linearly separable.\n00:09:33 We can draw this line, which divides it into class positive on one side and class negative on the other.\n00:09:39 An equivalent view is that by allowing ourselves a more complex response function, a quadratic function of X, when we apply our threshold operator, we get a more complex class of output functions.\n00:09:46 For example, in the original features, the decision boundary was initially a point.\n00:09:53 The solution of BX plus C equals 0, but in the quadratic.\n00:09:59 The decision boundary has more solutions, and thus a potentially more complex decision function.\n00:10:06 So here we see the quadratic form of ax squared plus b plus c, bx plus c.\n00:10:13 When we threshold that, we get an output with a more complex decision boundary.\n00:10:19 Returning to our binary function example, we can ask what set of features would be sufficient to learn the XOR function.\n00:10:26 If we were to increase the number of features available, for example, to include quadratic polynomials, we would be able to learn a function whose decision boundary corresponds to an ellipse.\n00:10:33 And within this ellipse, predict plus 1 and outside minus 1.\n00:10:39 So this would be sufficient to reproduce an XOR function.\n00:10:46 Note that these features are sufficient, but one could also achieve the same thing using other features.\n00:10:53 Since our classifier is a linear function of the features, the abilities of the learner are highly influenced by the choice of features.\n00:10:59 As an example, consider a discrete feature for poisonous mushroom classification, and we'll have a feature that the surface can be either fibrous, grooved, scaly, or smooth.", "start_char_idx": 6426, "end_char_idx": 10417, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "710f7a71-1614-472f-ac06-d6bffc79e888": {"__data__": {"id_": "710f7a71-1614-472f-ac06-d6bffc79e888", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear classifiers (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hAnfnc2dWBs", "Link": "https://www.youtube.com/watch?v=hAnfnc2dWBs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea172faa-1bf1-48bf-8470-6e4ea7dde048", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hAnfnc2dWBs", "Link": "https://www.youtube.com/watch?v=hAnfnc2dWBs"}, "hash": "475e739cce399c56f474e2d734410d5c99cd4576e1c5cc444ca2ae83e3d40e52", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc45f623-2cc3-4c80-9e99-5a571a8452e0", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hAnfnc2dWBs", "Link": "https://www.youtube.com/watch?v=hAnfnc2dWBs"}, "hash": "495e706f7848f964f61ee1268061387364b54f94cd8a1e9752924dbd17dbe126", "class_name": "RelatedNodeInfo"}}, "text": "00:10:26 If we were to increase the number of features available, for example, to include quadratic polynomials, we would be able to learn a function whose decision boundary corresponds to an ellipse.\n00:10:33 And within this ellipse, predict plus 1 and outside minus 1.\n00:10:39 So this would be sufficient to reproduce an XOR function.\n00:10:46 Note that these features are sufficient, but one could also achieve the same thing using other features.\n00:10:53 Since our classifier is a linear function of the features, the abilities of the learner are highly influenced by the choice of features.\n00:10:59 As an example, consider a discrete feature for poisonous mushroom classification, and we'll have a feature that the surface can be either fibrous, grooved, scaly, or smooth.\n00:11:09 If we represent this information by converting these categories to an integer value, one through four, it will probably not be used very effectively by the classifier.\n00:11:19 There's no reason to believe that the influence of observing a smooth feature should be four times the influence of observing a fibrous feature, and in the same direction.\n00:11:29 So an alternative is to convert this into a one-of-K representation, where we create four features, binary valued, and turn on exactly one depending on the value of that discrete class.\n00:11:39 So if fibrous, we output features 1, 0, 0, 0, and if smooth, we output features 0, 0, 0, 1.\n00:11:49 Of course...\n00:11:59 This introduces more parameters into the model, but it allows a more flexible relationship between the discrete feature and the output class.\n00:12:06 In general, we find that data sets will be increasingly separable given more features, whether those features are measured or constructed.\n00:12:13 In some sense, this is good.\n00:12:19 By using more features, we stand a better chance of learning the true, possibly complex, relationship to the target variable.\n00:12:26 We get a better performance on our training data.\n00:12:33 But in another sense, this means that more features can increase our chance of overfitting.\n00:12:39 As we perform better and better on the training set, that doesn't necessarily mean that we'll perform better on new, unseen test data.\n00:12:46 So we have this classic curve where, as we increase the complexity of our model, moving to the right, we perform better and better on the training data and also perform better on the test data as we go to more complex predictors.\n00:12:53 But at some point...\n00:12:59 Even though our training data keeps getting better, our performance keeps decreasing its error, at some point our test error actually starts to go the other way and perform worse.\n00:13:05 So we need to be careful about increasing our feature set too greatly.\n00:13:11 In summary, a perceptron is simply a binary linear classifier, which converts a linear response, similar to linear regression, into a discrete class value.\n00:13:17 The decision boundary of such a linear classifier is just this linear sub-manifold.\n00:13:23 So if we have one feature, it's a point.\n00:13:29 If we have 2D features, it's a line, and so on.\n00:13:35 We discussed the notion of separability, which depends on both the data set and the model class.\n00:13:41 So a data set is separable for a particular learner if there's some instance of that learner, some setting of the parameters, that can correctly predict all the points.\n00:13:47 We saw that not all functions can be represented with a linear classifier.\n00:13:53 We also saw that by adding features artificially, we could increase the representation.\n00:13:59 representational power of the learner.\n00:14:04 This increases the number of parameters in the model and also increases its ability to separate datasets.\n00:14:09 While this enables us to learn more complex relationships to the target variable, it also increases our risk of overfitting.", "start_char_idx": 9637, "end_char_idx": 13534, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c1cee8b-7db8-4372-bb94-78cf12e59d4e": {"__data__": {"id_": "4c1cee8b-7db8-4372-bb94-78cf12e59d4e", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "5fdf2883968745ea1e71fb808c633a8184397098f25a49232e5fa8f685258252", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29b48206-89f1-45c4-97a3-4b36a0326e56", "node_type": "1", "metadata": {}, "hash": "95e892873646318da21952a83b089a9194552360702303c2bf490578ef0fada2", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Now, let's turn our attention to some methods for learning the parameters for a linear classifier.\n00:00:07 In learning, we're given a collection of training data, or labeled examples that show the desired relationship between the observed features and the target.\n00:00:15 Our goal will be to find a set of parameters that predict well, both in training error, the performance on the training examples, and in the future, on unseen data.\n00:00:22 To do this, we'll define an objective function, J of theta, for example, the classifier accuracy, which is a function of the data set and a specific value of the parameters.\n00:00:30 Then we'll maximize this objective, or equivalently, we'll define a loss function such as the error rate and minimize that.\n00:00:37 This is then simply an optimization procedure over the parameter space defined by theta.\n00:00:45 A natural and commonly selected goal is to measure our performance in terms of the fraction of examples we get wrong, the error rate.\n00:00:52 For any value of theta, we can define the impedance.\n00:01:00 empirical error rate by just running over all the data points and checking whether the true target, yi, equals our prediction, y-hat.\n00:01:08 In the equation, this is represented with a Kronecker delta function, which takes a value 1 if the condition is satisfied and 0 otherwise.\n00:01:17 In MATLAB, this is simple to compute.\n00:01:25 We take our prediction, y-hat, the sign of the linear response, and compute the average of a binary indicator, checking whether the two values are equal.\n00:01:34 The main problem with this approach is that it's then difficult to optimize theta using standard tools, such as gradient descent.\n00:01:42 The empirical error is a discontinuous and locally constant function.\n00:01:51 As the decision boundary moves, for instance, moving slightly left or right here, the error rate stays exactly the same until the decision boundary reaches some data point, at which point the loss will change.\n00:01:59 abruptly discontinuously to a new value.\n00:02:05 So there's no real local information in the error rate to tell us which direction we should be evolving theta.\n00:02:10 A very simple approach we could contemplate is to use our tools from regression.\n00:02:16 Let's define the two classes as plus 1 and minus 1.\n00:02:21 Then we could simply choose theta so that a linear regression theta of x is close to the target values.\n00:02:27 And then take that function and threshold it to produce our classifier.\n00:02:32 In some cases, this may work, but often we'll be unsatisfied with its behavior.\n00:02:38 Consider, for example, adding this extra data point over here.\n00:02:43 In some sense, this point is actually very easy to predict.\n00:02:49 It would be predicted correctly by our previous values of theta.\n00:02:54 But using linear regression, the parameters will be greatly distorted so that our linear response at that point will be closer to its.\n00:03:00 value plus one, even though all we actually care about is that its sign is positive.\n00:03:06 The mismatch between the predictions of regression and classification and of the losses, mean squared error versus error rate, lead to poor selection of the parameter values in this case.\n00:03:13 Here's another historical algorithm called the Perceptron algorithm that will fix some of these issues.\n00:03:20 The Perceptron algorithm is a stochastic gradient descent-like algorithm.\n00:03:26 For each iteration, we run through all the data points, one at a time, and perform an update of the parameter vector for each one.\n00:03:33 For each data point, we compute our prediction, y-hat, and then we take an update to the parameter theta.\n00:03:40 An interesting interpretation of this algorithm comes by comparing it to linear regression.\n00:03:46 The online update is almost exactly that of stochastic gradient descent for linear regression.\n00:03:53 We update the parameter theta by a step alpha times a direction comprised of an error term, the difference between...\n00:04:00 between y and our prediction times the feature vector x.", "start_char_idx": 0, "end_char_idx": 4107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29b48206-89f1-45c4-97a3-4b36a0326e56": {"__data__": {"id_": "29b48206-89f1-45c4-97a3-4b36a0326e56", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "5fdf2883968745ea1e71fb808c633a8184397098f25a49232e5fa8f685258252", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c1cee8b-7db8-4372-bb94-78cf12e59d4e", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "570b47d720f4d77eaee52622909997df5b03e39e1dd1b186fff5e00815541264", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7ca81fd-a58f-40cf-9035-9aff931fdc00", "node_type": "1", "metadata": {}, "hash": "b2c18987f3c79aa3ec8b679f4af526bd8004d231fd97e01413931d11a3d5964c", "class_name": "RelatedNodeInfo"}}, "text": "00:03:13 Here's another historical algorithm called the Perceptron algorithm that will fix some of these issues.\n00:03:20 The Perceptron algorithm is a stochastic gradient descent-like algorithm.\n00:03:26 For each iteration, we run through all the data points, one at a time, and perform an update of the parameter vector for each one.\n00:03:33 For each data point, we compute our prediction, y-hat, and then we take an update to the parameter theta.\n00:03:40 An interesting interpretation of this algorithm comes by comparing it to linear regression.\n00:03:46 The online update is almost exactly that of stochastic gradient descent for linear regression.\n00:03:53 We update the parameter theta by a step alpha times a direction comprised of an error term, the difference between...\n00:04:00 between y and our prediction times the feature vector x.\n00:04:05 To contrast with the linear regression gradient, instead of using the linear response theta times x in this error term, we use the thresholded class output y hat.\n00:04:10 So the points that are more wrong don't get a larger update.\n00:04:16 In fact, this term is just a sign.\n00:04:21 So if our prediction is correct, if the class y equals the class y hat, this update is 0.\n00:04:27 And if the term is incorrect, then that becomes a signed value, some signed constant.\n00:04:32 So if y is plus 1 and y hat is minus 1, this is plus 2.\n00:04:38 And if they're reversed, it becomes minus 2.\n00:04:43 Thus, points that are already right get no update at all.\n00:04:49 Intuitively, you can imagine something like performing a sequence of regressions, each of which pays attention only to the points that it's currently getting wrong.\n00:04:54 This makes the procedure a bit immune to the very.\n00:04:59 data error that we saw just a moment ago.\n00:05:04 As the process gets more points right, it starts to focus exclusively on the data near the boundary, and eventually it will stop if it gets them all right.\n00:05:09 Here's a visualization of this procedure.\n00:05:13 We select a data point here and compute our prediction.\n00:05:18 In this example, we get this predicted point incorrect.\n00:05:23 The true class is minus 1, blue, but we predict plus 1.\n00:05:27 So our error term here is minus 2.\n00:05:32 If we made the opposite mistake, the error term would be plus 2.\n00:05:36 So this tells us we need to update the model to try to get things that are close to this area more correct, and we'll perform this update and shift the model, which shifts the decision boundary.\n00:05:41 In the next step, we choose another point to update, say over here.\n00:05:46 Here, the true class y equals our prediction y hat.\n00:05:50 The point is predicted correctly.\n00:05:55 This error term is\n00:06:00 zero, and our update has no effect on the parameter value.\n00:06:04 We continue until no data point produces an update to theta.\n00:06:09 In other words, all data points are predicted correctly.\n00:06:13 Interestingly, if the data are linearly separable, this will eventually happen for any choice of the step size constant alpha.\n00:06:18 However, in practice, this perceptron algorithm doesn't work very well.\n00:06:23 The data are not separable, its behavior is unstable and unpredictable.\n00:06:27 This means it's rarely used in practice.\n00:06:32 A more typical way to learn theta is to use a surrogate loss function.\n00:06:36 In essence, the problem with optimizing the error rate was that it was non-smooth.\n00:06:41 The problem with using linear regression was that it was too different from the cost that we care about.\n00:06:46 Instead, we can try to design smooth losses that would be more similar to the error rate.\n00:06:50 For example, instead of outputting just the class value, we could also compute a smooth prediction that varies smoothly between minus one and one.", "start_char_idx": 3259, "end_char_idx": 7093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7ca81fd-a58f-40cf-9035-9aff931fdc00": {"__data__": {"id_": "f7ca81fd-a58f-40cf-9035-9aff931fdc00", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "5fdf2883968745ea1e71fb808c633a8184397098f25a49232e5fa8f685258252", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29b48206-89f1-45c4-97a3-4b36a0326e56", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "9ae47ef45de205a9c802abcae6ba75fc4106c0e9cf4a45d7ad9d8c72af1dbcec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47d8a91e-cafd-4ea2-a255-12d43902aa3d", "node_type": "1", "metadata": {}, "hash": "0cc464eb4c0760ae544bf946ed4394b59ad695e24f119c88318383fae2560b5c", "class_name": "RelatedNodeInfo"}}, "text": "00:06:18 However, in practice, this perceptron algorithm doesn't work very well.\n00:06:23 The data are not separable, its behavior is unstable and unpredictable.\n00:06:27 This means it's rarely used in practice.\n00:06:32 A more typical way to learn theta is to use a surrogate loss function.\n00:06:36 In essence, the problem with optimizing the error rate was that it was non-smooth.\n00:06:41 The problem with using linear regression was that it was too different from the cost that we care about.\n00:06:46 Instead, we can try to design smooth losses that would be more similar to the error rate.\n00:06:50 For example, instead of outputting just the class value, we could also compute a smooth prediction that varies smoothly between minus one and one.\n00:06:55 Because such a function is...\n00:07:00 shaped like an S, it's often called a sigmoid function.\n00:07:07 Now, instead of counting the errors, we can measure, for example, the mean squared error between our soft prediction and the true class.\n00:07:15 This has the nice property that if we're very far from the decision boundary, our output is nearly minus 1 or plus 1.\n00:07:22 And so we have small MSE.\n00:07:30 But near the boundary, we transition smoothly between those two values, and the error will smoothly increase.\n00:07:37 We then add up all of these soft errors, shown here in this figure, and use it to assess our prediction quality.\n00:07:45 As we'll see, this will be much easier to optimize, since small variations in the parameters will actually induce a change into the mean squared error, unlike what we found for the error rate.\n00:07:52 But having a loss\n00:08:00 that smoothly increases near the decision boundary is useful for another reason as well.\n00:08:06 Consider these two examples of classifiers on the same data, each of which has zero error.\n00:08:12 So here's one linear boundary that correctly decides all the data points, and here's a different linear boundary that correctly decides all the data points.\n00:08:18 Intuitively, out of these two, we'd prefer this one over on the right.\n00:08:24 Even though it has the same error, it seems safer.\n00:08:30 If we were to perturb the data or move it very slightly, it will still get those points right because it's gotten them right by a more substantial margin.\n00:08:36 A smooth loss function will promote this type of boundary naturally, since usually being close to the decision boundary will lead to a non-zero loss.\n00:08:42 So once we have our smooth loss function, choosing theta just becomes a simple optimization problem.\n00:08:48 Again, we can visualize this in both the feature space and in the parameter space.\n00:08:54 A point in the parameter space corresponds to a particular.\n00:09:00 value of theta, which corresponds to a decision boundary.\n00:09:07 The loss function can be evaluated for that decision boundary and parameter values, producing a value over here.\n00:09:15 Then we can plot the loss function or optimize it over in parameter space.\n00:09:22 As we evolve the parameter theta, we change the decision boundary, we find a new point in parameter space and get a new loss, hopefully smaller than the previous value.\n00:09:30 Eventually, we'll find a minimum of our loss J at some value of theta, and this will correspond to a model or decision boundary that hopefully does a good job of classifying the training data.\n00:09:37 As we saw in linear regression, this is now just a standard optimization problem in the parameter space.\n00:09:45 We can use whatever algorithms we like.\n00:09:52 Gradient and stochastic gradient descent methods are useful and very all-purpose, but we could also substitute whatever...\n00:10:00 whatever alternatives we prefer, like coordinate descent algorithms, stochastic search, genetic algorithms, or others.\n00:10:07 To do gradient descent, we'll need to calculate the gradient of the loss with respect to the parameters.\n00:10:15 Let's work that out quickly.\n00:10:22 Our loss function J here calculates the squared error between our sigmoid nonlinearity and our target Y.", "start_char_idx": 6341, "end_char_idx": 10415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47d8a91e-cafd-4ea2-a255-12d43902aa3d": {"__data__": {"id_": "47d8a91e-cafd-4ea2-a255-12d43902aa3d", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "5fdf2883968745ea1e71fb808c633a8184397098f25a49232e5fa8f685258252", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7ca81fd-a58f-40cf-9035-9aff931fdc00", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "04197f3711eca704e4293b43d907cdde7a1997a605b6bb3d8d7a695bc48c423e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "febc4676-b683-43a7-a209-fe09ef9c44b9", "node_type": "1", "metadata": {}, "hash": "65a1ff42f0d3f0ba027c4bc72cddd71881cb9dc33d79d1f292c475eff891a490", "class_name": "RelatedNodeInfo"}}, "text": "00:09:30 Eventually, we'll find a minimum of our loss J at some value of theta, and this will correspond to a model or decision boundary that hopefully does a good job of classifying the training data.\n00:09:37 As we saw in linear regression, this is now just a standard optimization problem in the parameter space.\n00:09:45 We can use whatever algorithms we like.\n00:09:52 Gradient and stochastic gradient descent methods are useful and very all-purpose, but we could also substitute whatever...\n00:10:00 whatever alternatives we prefer, like coordinate descent algorithms, stochastic search, genetic algorithms, or others.\n00:10:07 To do gradient descent, we'll need to calculate the gradient of the loss with respect to the parameters.\n00:10:15 Let's work that out quickly.\n00:10:22 Our loss function J here calculates the squared error between our sigmoid nonlinearity and our target Y.\n00:10:30 The first entry in the gradient is the derivative with respect to the first parameter, let's say A.\n00:10:37 Just like in regression, the derivative of a squared error is just two times that error itself, so the signed error, error residual.\n00:10:45 Continuing to apply the chain rule, we get the derivative of the interior, since Y is constant, this is just the derivative of the sigma function at that point, times the derivative of its argument, which is just the feature, X1.\n00:10:52 Here this D sigma term is the slope of the sigmoid at this point, theta times X1.\n00:11:00 Interpreting the derivative again, we have one term measuring the amount of error and its sign, and a second term measuring our sensitivity of our prediction to changes in the parameters.\n00:11:10 If for this particular data point, say, feature X1 is small, then changing the parameter A won't help our prediction very much.\n00:11:20 Similarly, if for this particular data point, our output is highly saturated, either very close to the negative value or the positive value, then changing the parameters will also not change the output very much.\n00:11:30 So the derivatives with respect to the other parameters are identical, except that instead of feature X1, this term will be whichever feature corresponds to that parameter, so feature X2 or the constant feature.\n00:11:40 We could choose any saturating function for our sigmoid, but a very common one to pick is the logistic function.\n00:11:50 The logistic function\n00:12:00 is usually defined as the function 1 over 1 plus e to the minus z, which ranges between the values of 0 and 1.\n00:12:06 When z is very large and negative, this term is large and positive.\n00:12:13 It dominates, and this output becomes nearly 0.\n00:12:20 On the other hand, when z is large and positive, the exponential term is nearly 0, and the output becomes 1.\n00:12:26 At z equals 0, these are equal, we get 1 half.\n00:12:33 So if we want to use this to predict, we can either continue to threshold the z value, the linear output theta times x at 0, or we can calculate the sigma of z and threshold it at 1 half.\n00:12:40 The logistic function has a nice property that its derivative is just the product of itself, sigma of z, times 1 minus itself.\n00:12:46 In MATLAB, the implementation is quite simple.\n00:12:53 So here's a logistic sigmoid.\n00:13:00 function implemented and here's its derivative.\n00:13:06 Note that in the perceptron we use classes plus 1 and minus 1.\n00:13:13 If we prefer we can also scale the logistic sigmoid to match.\n00:13:20 So we can define a rescaled sigmoid as say the function rho which is now scaled to be between minus 1 and 1.\n00:13:26 As Z goes toward minus infinity approaches minus 1 for Z large and positive plus 1.\n00:13:33 Its derivative will be nearly identical to sigma's except rescaled by the constant 2.\n00:13:40 Another very common and closely related linear classifier goes by the name of logistic regression.", "start_char_idx": 9525, "end_char_idx": 13396, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "febc4676-b683-43a7-a209-fe09ef9c44b9": {"__data__": {"id_": "febc4676-b683-43a7-a209-fe09ef9c44b9", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "5fdf2883968745ea1e71fb808c633a8184397098f25a49232e5fa8f685258252", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47d8a91e-cafd-4ea2-a255-12d43902aa3d", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "6e9ecf23a6154357e0ff594b2d22078ed8aa2418aba4eddefbd475a5de288361", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f597d9da-7cc4-4d85-9c00-8ee8edad52df", "node_type": "1", "metadata": {}, "hash": "61abacaeae06511bfc5160faa8fe5ee396810da9f258245ba4832a5f67cf889b", "class_name": "RelatedNodeInfo"}}, "text": "00:12:46 In MATLAB, the implementation is quite simple.\n00:12:53 So here's a logistic sigmoid.\n00:13:00 function implemented and here's its derivative.\n00:13:06 Note that in the perceptron we use classes plus 1 and minus 1.\n00:13:13 If we prefer we can also scale the logistic sigmoid to match.\n00:13:20 So we can define a rescaled sigmoid as say the function rho which is now scaled to be between minus 1 and 1.\n00:13:26 As Z goes toward minus infinity approaches minus 1 for Z large and positive plus 1.\n00:13:33 Its derivative will be nearly identical to sigma's except rescaled by the constant 2.\n00:13:40 Another very common and closely related linear classifier goes by the name of logistic regression.\n00:13:46 In logistic regression we again use the logistic function sigma to convert our linear response to a value between 0 and 1 but this time we interpret that output as the probability that our model associates with the true label being positive.\n00:13:53 Then as is common for probability models we can optimize the log likelihood of the data.\n00:14:00 we tend to minimize J, so we'll minimize the negative log likelihood.\n00:14:06 The log likelihood measures the probability of our observations under the current model.\n00:14:12 So if the true label is positive, y equal 1, we have a reward equal to its log probability.\n00:14:18 So log sigma of x, sorry, log sigma of theta x.\n00:14:24 If our model is very sure that y should be positive, this probability is nearly 1 and we get score 0.\n00:14:30 If it's very sure and wrong, this probability is near 0 and we get a score of minus infinity.\n00:14:36 Again, since we usually minimize, we'll negate this.\n00:14:42 And similarly, for when y equals 0, our model now uses 1 minus sigma as the probability of that event.\n00:14:48 As usual, we may want to write this as a single succinct equation.\n00:14:54 We'll write it as a sum of two terms, 1 when y equals 1.\n00:15:00 and 1 when y equals 0.\n00:15:06 So if y equals 1, this first term will be non-zero, and the second one will be 0, since 1 minus y will be 0.\n00:15:12 On the other hand, when the true class y is 0, this first term will be 0, and the second one will be non-zero.\n00:15:18 Now this is just another loss function.\n00:15:24 Again, it's a soft loss.\n00:15:30 When a data point is close to the decision boundary, its cost increases smoothly, so we'll be able to get local gradient information about which way to evolve our parameters.\n00:15:36 In this case, each data point is associated with a loss, which is the log probability that our model assigns to its true value.\n00:15:42 So here's, for instance, this data point.\n00:15:48 It's very nearly predicted exactly correctly, so its loss might be something like log 0.99, something very small.\n00:15:54 As we move toward the decision boundary, the cost will increase, and it's greatest for things that are on the wrong side of the boundary.\n00:16:00 A very nice property of logistic negative log-likelihood loss is that it's convex.\n00:16:06 Convex functions are nicely behaved, and in particular, a convex function has no local optima.\n00:16:13 So any optimum is also a global optimum.\n00:16:19 If the function is strongly convex, it also only has one optimum.\n00:16:26 This is very helpful, since it means that we don't have to worry very much about initialization, and optimization becomes simpler.\n00:16:33 Just as with our mean-squared error-based loss, we can take the derivative with respect to each parameter to compute the gradient vector.\n00:16:39 Taking the derivative of this loss function with respect to, say, parameter A, we again follow the chain rule.\n00:16:46 We get 1 over sigma from the derivative of the log times the derivative of the interior, so d sigma times the derivative of its interior, which is feature 1.", "start_char_idx": 12688, "end_char_idx": 16500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f597d9da-7cc4-4d85-9c00-8ee8edad52df": {"__data__": {"id_": "f597d9da-7cc4-4d85-9c00-8ee8edad52df", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "5fdf2883968745ea1e71fb808c633a8184397098f25a49232e5fa8f685258252", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "febc4676-b683-43a7-a209-fe09ef9c44b9", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "419bd34c6e84728886e365e16db7ce4eaa93365ad3b0f5fa0edffb8b68fa5ec9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f5201a0-675a-4249-98e3-4b7759737873", "node_type": "1", "metadata": {}, "hash": "f1f50a1543ef551e7aa1c576d995d19b99630c03910849574bd6d4561af38410", "class_name": "RelatedNodeInfo"}}, "text": "00:16:06 Convex functions are nicely behaved, and in particular, a convex function has no local optima.\n00:16:13 So any optimum is also a global optimum.\n00:16:19 If the function is strongly convex, it also only has one optimum.\n00:16:26 This is very helpful, since it means that we don't have to worry very much about initialization, and optimization becomes simpler.\n00:16:33 Just as with our mean-squared error-based loss, we can take the derivative with respect to each parameter to compute the gradient vector.\n00:16:39 Taking the derivative of this loss function with respect to, say, parameter A, we again follow the chain rule.\n00:16:46 We get 1 over sigma from the derivative of the log times the derivative of the interior, so d sigma times the derivative of its interior, which is feature 1.\n00:16:53 There's another corresponding term for this class.\n00:16:59 equals zero term.\n00:17:06 Since d sigma here for the logistic function is just sigma times 1 minus sigma, we can simplify the expression a bit further.\n00:17:13 Again, for different parameters, all that will change is the feature involved.\n00:17:19 So we can compute the gradient vector by using this scalar and the feature vector.\n00:17:26 We've now seen two methods of optimizing the parameters, and both follow the same general theme.\n00:17:33 While we may be interested in doing well under the error rate, also sometimes called the counting loss or 0-1 loss, that counts up the number of wrong predictions we make, this error rate is hard to optimize because it's not smooth.\n00:17:40 A simple way to visualize this loss is to draw a cost as a function of the linear response z.\n00:17:46 So this axis is the z-axis.\n00:17:53 Assuming the true label is positive.\n00:18:00 For the 0-1 loss, when z is positive, we have 0 loss because we've predicted correctly.\n00:18:07 As soon as z becomes negative, our cost jumps to 1.\n00:18:15 The negative features of this choice can easily be seen here.\n00:18:22 It's discontinuous, and even for mispredicted points, say when the loss is over here, the derivative is still 0, so we have no local information about how we should evolve the decision boundary.\n00:18:30 For these reasons, we instead optimize the surrogate loss, a loss function that's similar to what we want but has nicer properties.\n00:18:37 The first of these that we saw was the logistic mean-squared error loss, where the loss increased smoothly from 0 if we were heavily in the positive region, predicting correctly, increasing as we approach the decision boundary, and finally saturating to some large value.\n00:18:45 Here I've scaled this to make a point.\n00:18:52 With proper scaling, this loss is always greater than the 0-1 loss.\n00:19:00 and so it's an upper bound on the true loss.\n00:19:07 It's common to try to minimize upper bounds on difficult losses, since at least it means that if we manage to obtain a small value of our surrogate loss, that must imply a similarly small value of the true loss.\n00:19:15 Our other surrogate loss was the logistic negative log likelihood.\n00:19:22 If we plot its value, we find that also, for very positive values, we get loss zero, and as we increase toward the decision boundary, we again increase smoothly.\n00:19:30 It also bounds the zero-one loss with an appropriate scaling.\n00:19:37 As z becomes very large and negative, it approaches a constant slope.\n00:19:45 So compared to the logistic mean squared error, a loss like this, the logistic negative log likelihood, will tend to penalize very wrong examples much more than something like the mean squared error where the loss saturates.\n00:19:52 We can also see from its shape that it's a convex loss.\n00:20:00 So, the sum or average of these shapes will also be convex.\n00:20:07 In contrast, the logistic MSE is a non-convex loss.\n00:20:15 There are many other possible choices of surrogate losses, and we'll see some in later modules.\n00:20:22 In this section, we turn to learning the parameters of a Perceptron model.", "start_char_idx": 15698, "end_char_idx": 19706, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f5201a0-675a-4249-98e3-4b7759737873": {"__data__": {"id_": "0f5201a0-675a-4249-98e3-4b7759737873", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b42695f-4068-4da2-a2b3-de44867b9bb2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "5fdf2883968745ea1e71fb808c633a8184397098f25a49232e5fa8f685258252", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f597d9da-7cc4-4d85-9c00-8ee8edad52df", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}, "hash": "ff2ff66bdffe179052d10f127393dc47f32c40245535b9eed8a14ae660cd6ec4", "class_name": "RelatedNodeInfo"}}, "text": "00:19:30 It also bounds the zero-one loss with an appropriate scaling.\n00:19:37 As z becomes very large and negative, it approaches a constant slope.\n00:19:45 So compared to the logistic mean squared error, a loss like this, the logistic negative log likelihood, will tend to penalize very wrong examples much more than something like the mean squared error where the loss saturates.\n00:19:52 We can also see from its shape that it's a convex loss.\n00:20:00 So, the sum or average of these shapes will also be convex.\n00:20:07 In contrast, the logistic MSE is a non-convex loss.\n00:20:15 There are many other possible choices of surrogate losses, and we'll see some in later modules.\n00:20:22 In this section, we turn to learning the parameters of a Perceptron model.\n00:20:30 While we're most likely interested in minimizing something like our error rate, we saw that to train the model, we may want to turn to regression-like techniques and surrogate losses.\n00:20:37 These included using a sigmoid nonlinearity and measuring mean squared error, and also interpreting its output as a probability and using logistic regression.\n00:20:45 Either approach gives us a smooth loss that's easier to optimize with standard techniques like gradient descent.\n00:20:52 We described several algorithms, including the historic Perceptron algorithm and the use of standard gradient descent techniques.\n00:21:00 to derive the gradient for them using either of the surrogate losses that we discussed.", "start_char_idx": 18939, "end_char_idx": 20425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eeae53ca-af32-4d06-9d59-3fe6d83a7314": {"__data__": {"id_": "eeae53ca-af32-4d06-9d59-3fe6d83a7314", "embedding": null, "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7994668a-0e4f-4d5d-8d53-d7ba6586a3cc", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "hash": "c2620047d43692f96c7f1e805f4813c0549f41a398e51f88e93322d4541054f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e9b6eed-cafd-4bed-bef3-cb2deba8e295", "node_type": "1", "metadata": {}, "hash": "a69cdeb9d6724487519d3caca0d2d3c3be2e1693c3913f99428084d2d78b9d25", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 We've seen several examples of how the complexity of our model affects performance, but without creating a clear definition of complexity.\n00:00:08 One quantitative definition that captures much of what we mean by complexity is a quantity called the VC dimension.\n00:00:17 Note that many of these slides are based on Andrew Moore's slides from CMU.\n00:00:25 When we talk about the complexity of a learner, we usually mean its representational power, both its ability to learn a wide variety of input-output relationships, and at an extreme, its ability to memorize or overfit to the data.\n00:00:34 Different learners define different classes of possible behavior.\n00:00:42 For example, our standard perceptron learner is a classifier here in two dimensions, features X1 and X2, with a coefficient for each feature, theta 1, theta 2, and a constant theta 0.\n00:00:51 The types of functions it can learn are predictions, in which the two classes of predictions, so the blue prediction, and the red prediction, can be used to predict the behavior of the \n00:00:59 predictions and the red predictions are separated by an arbitrary line.\n00:01:07 But choosing a different form for the classifier would choose a different set of possible functions.\n00:01:15 For example, suppose we chose to use a class of learners that were thresholding a linear equation, but this time we did not include a constant feature.\n00:01:22 So we have only two parameters, theta 1 times feature 1 and theta 2 times feature 2.\n00:01:30 Then it's easy to see that the decision boundary of this classifier must pass through the origin.\n00:01:37 So this defines a more restrictive class of functions where we divide the space into blue predictions and red predictions through a line, but that line is now restricted.\n00:01:45 Another different functional form might be, for example, to take the sign of the norm of x minus some constant theta 0.\n00:01:52 If we think about what kinds of functions this form can produce...\n00:02:00 we find that if the value of the point x is farther from the origin than theta 0, then this function will predict class plus 1.\n00:02:10 If it's closer to the origin than theta 0, then predict minus 1.\n00:02:20 So we can think of this set of all possible functions learnable by this particular classifier as any function where the negative class lives in some sphere centered around the origin.\n00:02:30 The usual tradeoff we expect from any kind of model is that the more representational power our class of functions has, the more chance it will include the real feature-to-target relationship, but also the more chance it may overfit to random variations in the data.\n00:02:40 If we provide less power to the learner, we won't overfit, but we may not find the best possible function.\n00:02:50 It's not easy to try to quantify this notion of representational power, but it's not easy to quantify this notion of representational power.\n00:03:00 But one very useful formulation is the VC or Vapnik-Chervenenko's dimension of the learner.\n00:03:07 First, some preliminaries.\n00:03:15 Let's assume that our training and our future test data are all independent and identically distributed from some distribution P of X.\n00:03:22 We'll define the risk as the long-term test error, so the expected error rate of our predictions over that distribution P.\n00:03:30 The empirical risk we'll define as the training error, so it's the risk on our finite collection of M data points, which is the average error rate.\n00:03:37 The relationship, as we've seen, between the risk and the empirical risk depends on whether we're overfitting.\n00:03:45 If the model is very simple and we happen to be in the underfitting regime, their performance will be quite similar.\n00:03:52 But if our model becomes more and more complex,\n00:04:00 it becomes too complex, the test error might actually be far worse than just the training error would suggest.\n00:04:08 We'll actually define the VC dimension in a minute, but first let's see what the VC dimension tells us about the error.\n00:04:17 Suppose we know the VC dimension of our learner and call it h.", "start_char_idx": 0, "end_char_idx": 4138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e9b6eed-cafd-4bed-bef3-cb2deba8e295": {"__data__": {"id_": "0e9b6eed-cafd-4bed-bef3-cb2deba8e295", "embedding": null, "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7994668a-0e4f-4d5d-8d53-d7ba6586a3cc", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "hash": "c2620047d43692f96c7f1e805f4813c0549f41a398e51f88e93322d4541054f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eeae53ca-af32-4d06-9d59-3fe6d83a7314", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "hash": "0e7c05b4feab75d6a2f5764995ee12a3dc2df9d1393429c17a484700aac19c6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "331c113b-be24-437e-b21c-fed3276897f7", "node_type": "1", "metadata": {}, "hash": "dd05c666f650c307b49cfed806a35ff72487d2f89dc341c2a37d5c4e1030fa77", "class_name": "RelatedNodeInfo"}}, "text": "00:03:30 The empirical risk we'll define as the training error, so it's the risk on our finite collection of M data points, which is the average error rate.\n00:03:37 The relationship, as we've seen, between the risk and the empirical risk depends on whether we're overfitting.\n00:03:45 If the model is very simple and we happen to be in the underfitting regime, their performance will be quite similar.\n00:03:52 But if our model becomes more and more complex,\n00:04:00 it becomes too complex, the test error might actually be far worse than just the training error would suggest.\n00:04:08 We'll actually define the VC dimension in a minute, but first let's see what the VC dimension tells us about the error.\n00:04:17 Suppose we know the VC dimension of our learner and call it h.\n00:04:25 Then we can show that with high probability 1 minus eta, where eta is a small number, the test error will be upper bounded by our training error plus an additional term that depends on the VC dimension h, the number of data points m, and also on that probability eta.\n00:04:34 Notice that if h is very small or m is very large, this additional bounding term will be small.\n00:04:42 So low VC dimension compared to the number of data will suggest that the training and test error will be quite similar.\n00:04:51 This is in some sense an amazing result.\n00:04:59 result, we can actually make a strong statement about what our test performance will be on data we haven't seen using only a property of the model, the VC dimension, and also the assumption that our data are IID from the same distribution.\n00:05:09 So once we understand this, let's now start to define the VC dimension.\n00:05:19 We'll need one more concept first.\n00:05:29 We say a classifier f of x can shatter a collection of points x1 to xh if for every possible target label for those h points, our model f of x can achieve zero training error on that collection of labeled data.\n00:05:39 In other words, however we assign the labels, there's some value of the parameters of the model that can achieve that labeling.\n00:05:49 So for example, let's consider the linear classifier on two features, a standard perceptron given by theta.\n00:05:59 Theta 0 plus Theta 1 times feature 1 plus Theta 2 times feature 2.\n00:06:07 Given the following two positions of data points X1 and X2, can we shatter them with this learner?\n00:06:14 Yes, we can, which we can verify just by going through all possible labelings.\n00:06:22 So one point blue, one point red, the reverse, one point red, one point blue, both red and both blue.\n00:06:29 For the first pattern, we just choose Theta such that the decision boundary passes between the two points, and the negative side is over here to the left.\n00:06:37 For the reverse labeling, we just negate all the parameters.\n00:06:44 That will make the left side of that line positive and the other side negative.\n00:06:52 For the all red or all blue, we just choose lines that have them both on the same side, and so on.\n00:06:59 two points, and a different learner.\n00:07:05 So here we'll take the sine of the radius minus theta.\n00:07:11 In this case, no.\n00:07:17 Again, let's just run through the patterns.\n00:07:23 So there are four possible patterns for these two points.\n00:07:29 And several of them can be predicted.\n00:07:35 For instance, if both data points are blue, we just make theta large enough so that they're both within that radius.\n00:07:41 If they're both red, we take theta very small so that they're outside of the radius.\n00:07:47 If the inner point is blue, then we make the radius such that it falls between the two points, and we can predict them correctly.\n00:07:53 But the last case, where the nearer of the two points is red and the further one is blue, there's no circle that we can create such that this classifier will predict plus 1 in the interior region and minus 1 in the outer region, on the farther away point.", "start_char_idx": 3358, "end_char_idx": 7289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "331c113b-be24-437e-b21c-fed3276897f7": {"__data__": {"id_": "331c113b-be24-437e-b21c-fed3276897f7", "embedding": null, "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7994668a-0e4f-4d5d-8d53-d7ba6586a3cc", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "hash": "c2620047d43692f96c7f1e805f4813c0549f41a398e51f88e93322d4541054f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e9b6eed-cafd-4bed-bef3-cb2deba8e295", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "hash": "be068ceac1fa6f21cbd21b8497de9882e90eed1c149a4c1e4c4ce7401620ca66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e67e2ce0-127b-4b8a-8f30-feaadd7db1b0", "node_type": "1", "metadata": {}, "hash": "6f785099c34064ee33a48c751f87f738bc7b01785c28782b0867376f08677dbd", "class_name": "RelatedNodeInfo"}}, "text": "00:07:17 Again, let's just run through the patterns.\n00:07:23 So there are four possible patterns for these two points.\n00:07:29 And several of them can be predicted.\n00:07:35 For instance, if both data points are blue, we just make theta large enough so that they're both within that radius.\n00:07:41 If they're both red, we take theta very small so that they're outside of the radius.\n00:07:47 If the inner point is blue, then we make the radius such that it falls between the two points, and we can predict them correctly.\n00:07:53 But the last case, where the nearer of the two points is red and the further one is blue, there's no circle that we can create such that this classifier will predict plus 1 in the interior region and minus 1 in the outer region, on the farther away point.\n00:07:59 So, this particular classifier cannot shatter these two points.\n00:08:07 The VC dimension, then, is defined as the maximum number of points that can be arranged so that the classifier f of x can shatter them.\n00:08:14 I think it's helpful to think of this process as a kind of game.\n00:08:22 So we fix the definition of our learner, f of x, beforehand, and then we alternate between two players.\n00:08:29 So Player 1 gets to choose the locations of the feature space of the points x1 to xh.\n00:08:37 Then Player 2 chooses a label for each of them in an attempt to make them as hard to reproduce as possible.\n00:08:45 Player 1 then tries to pick a value for the parameters of the learner that will actually reproduce that target labeling.\n00:08:52 If Player 1 succeeds, then the points can be shattered.\n00:09:00 is at least H.\n00:09:06 If player 1 fails, then the VC dimension must be less than H.\n00:09:13 Mathematically, we have the definition that there exists a collection of H points X1 to XH, such that for all possible labelings Y1 to YH, there exists a setting of the parameters so that all the training labels can be produced by the classifier.\n00:09:19 Okay, so with that definition in hand, another example.\n00:09:26 Using the same model as before that predicts plus 1 outside of radius square root of theta, what is the VC dimension of this learner?\n00:09:33 Well, we can prove that it's 1.\n00:09:39 So we will do this just exhaustively, just by looking at the two cases.\n00:09:46 If we have one point, we can certainly select a parameter to predict either of the two patterns.\n00:09:53 If the data point is colored\n00:09:59 blue, we select the radius large.\n00:10:06 If the data point is colored red, we select the radius small.\n00:10:13 But from our previous example, we found that we could not shatter these two points.\n00:10:19 It was a case that we could not reproduce the labeling.\n00:10:26 So it remains to be shown that one cannot shatter any two points, but since in general one point will always be closer to the origin than the other, the argument holds.\n00:10:33 Note that you might think we could make both points equidistant from the origin, but then that would mean that both points would always be predicted with the same label.\n00:10:39 And since player one determines the locations of those points and wants to be able to predict arbitrary patterns, in general she will not position the points in such a way.\n00:10:46 Okay, so another example with the Perceptron model now on two features, three parameters.\n00:10:53 Again, checking all possible cases,\n00:10:59 is it's easy to see that three points can be shattered.\n00:11:05 Either all three points are the same color, in which case it's easy, or two of them are one color, and one is a different color, and that's illustrated in diagram.\n00:11:10 We place a line between them and color them correctly.\n00:11:16 What about four points?\n00:11:21 Can we shatter, say, these four points?\n00:11:27 It turns out we cannot.", "start_char_idx": 6499, "end_char_idx": 10298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e67e2ce0-127b-4b8a-8f30-feaadd7db1b0": {"__data__": {"id_": "e67e2ce0-127b-4b8a-8f30-feaadd7db1b0", "embedding": null, "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7994668a-0e4f-4d5d-8d53-d7ba6586a3cc", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "hash": "c2620047d43692f96c7f1e805f4813c0549f41a398e51f88e93322d4541054f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "331c113b-be24-437e-b21c-fed3276897f7", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "hash": "4969905a956a6272b067cd11967dba7b419c06483b0cde1703b565e96e58e7fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb718235-0b11-4f04-a710-9767beed3cf7", "node_type": "1", "metadata": {}, "hash": "679c0cee74bfd403dd2d5ed761d4168d02aae04756a2844fb6076d81468d0357", "class_name": "RelatedNodeInfo"}}, "text": "00:10:39 And since player one determines the locations of those points and wants to be able to predict arbitrary patterns, in general she will not position the points in such a way.\n00:10:46 Okay, so another example with the Perceptron model now on two features, three parameters.\n00:10:53 Again, checking all possible cases,\n00:10:59 is it's easy to see that three points can be shattered.\n00:11:05 Either all three points are the same color, in which case it's easy, or two of them are one color, and one is a different color, and that's illustrated in diagram.\n00:11:10 We place a line between them and color them correctly.\n00:11:16 What about four points?\n00:11:21 Can we shatter, say, these four points?\n00:11:27 It turns out we cannot.\n00:11:32 A quick argument to see this is geometric.\n00:11:38 For the following pattern, looks basically like an XOR pattern that we saw could not be learned by a linear classifier before.\n00:11:43 We can try to predict these two points, with these colored red and these colored blue.\n00:11:49 In that case, we must have the decision boundary passed between this red point and this blue point, and also between this red point and this blue point.\n00:11:54 But, in order to get both red points correct, that line cannot pass.\n00:11:59 through the line connecting the two reds.\n00:12:06 So since there's no line that passes between this area without splitting the reds, at least one point must be incorrectly predicted.\n00:12:13 It turns out that for the perceptron, we can make a very general statement.\n00:12:19 In d dimensions, with a constant term, the VC dimension is exactly d plus 1.\n00:12:26 Note that this is easy to remember because this is also the number of parameters of the model.\n00:12:33 One for the constant plus D for each of the features.\n00:12:39 And as we know, adding more parameters tends to make a more complex classifier.\n00:12:46 In general, the VC dimension may not exactly equal the number of parameters.\n00:12:53 The VC dimension is an existential measure of the ability of the learner to memorize arbitrary patterns in the data, which is much more difficult to analyze than just a simple.\n00:12:59 parameter count, but one can construct a somewhat convoluted examples of cases where the parameter account is very different from the VC dimension.\n00:13:11 For example, you can create a classifier with a lot of parameters, but designing those parameters so that most of them don't have any real effect, then counting the number of parameters will overestimate the complexity of the actual learner.\n00:13:23 It's harder to do the opposite, but you can compact all the variability in the learner's behavior into one parameter, and then you can get a very complex function that appears to only have one parameter.\n00:13:35 Luckily, legions of machine learning researchers have given the VC dimension a lot of thought, and for many of the learners that you might want to think about, you can simply look up in the literature what the VC dimension is.\n00:13:47 So now let's consider how to use the VC dimension and its associated test error bound in practice.\n00:13:59 So recall we had a fundamental problem of model selection between models with varying degrees of complexity.\n00:14:09 If we slowly ratchet up the representational power from simple to more complex, for example, maybe a constant predictor, a linear predictor, a quadratic predictor, and so on, our optimized training performance can only get better as the model increases.\n00:14:19 So large error for simple models, small error for more complex models.\n00:14:29 But if we actually estimate the test performance, either using a validation set or a cross-validation procedure, we find that although the error might decrease at first, at some point the error will start to increase and will degrade.\n00:14:39 And so what we should do is try to choose the model with the best estimated test performance.\n00:14:49 We can use the VC dimension risk bound in a similar manner.\n00:14:59 number of training data points, as the complexity of the model increases, the training error will get smaller.", "start_char_idx": 9556, "end_char_idx": 13676, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb718235-0b11-4f04-a710-9767beed3cf7": {"__data__": {"id_": "eb718235-0b11-4f04-a710-9767beed3cf7", "embedding": null, "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7994668a-0e4f-4d5d-8d53-d7ba6586a3cc", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "hash": "c2620047d43692f96c7f1e805f4813c0549f41a398e51f88e93322d4541054f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e67e2ce0-127b-4b8a-8f30-feaadd7db1b0", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}, "hash": "fb58b24b2f50825977c63bccabd3e534bbc72b27b7e62b3deba6b3351c1283f6", "class_name": "RelatedNodeInfo"}}, "text": "00:14:09 If we slowly ratchet up the representational power from simple to more complex, for example, maybe a constant predictor, a linear predictor, a quadratic predictor, and so on, our optimized training performance can only get better as the model increases.\n00:14:19 So large error for simple models, small error for more complex models.\n00:14:29 But if we actually estimate the test performance, either using a validation set or a cross-validation procedure, we find that although the error might decrease at first, at some point the error will start to increase and will degrade.\n00:14:39 And so what we should do is try to choose the model with the best estimated test performance.\n00:14:49 We can use the VC dimension risk bound in a similar manner.\n00:14:59 number of training data points, as the complexity of the model increases, the training error will get smaller.\n00:15:08 But the second term in the upper bound we saw that depends on the VC dimension will increase because the VC dimension will increase.\n00:15:17 So when you look at the sum of these two, which is the upper bound on our test risk, we find that the sum will decrease for a bit and then increase again as complexity grows.\n00:15:25 So one possible model selection procedure is to choose the model that minimizes our VC upper bound on risk.\n00:15:34 This technique is called structural risk minimization, and it's an alternative to trying to estimate the test error through some validation process.\n00:15:42 Notice that as the number of data M increase, the bound term will get smaller.\n00:15:51 So assuming that the training error stays the same, as you might expect, the more data you have, the more able you are to support higher and higher complexity.\n00:15:59 models.\n00:16:08 This general theme of selecting the right complexity of model using a penalized score on training data is common to a number of different techniques.\n00:16:17 In probabilistic models, our training score is usually the log-likelihood of the data instead of an error rate.\n00:16:25 But similarly, techniques like the Akkakei information criterion and the Bayesian information criterion use an extra penalty term to try to penalize for high-complexity models.\n00:16:34 Then, even though the more complex model will always have a higher likelihood than the simpler model, it may not be enough higher to compensate for the penalty which will increase with complexity.\n00:16:42 AIC and BIC differ mainly in the degree to which they penalize the complexity measured in the number of parameters.\n00:16:51 And there are also other probabilistic methods of doing model selection that are commonly chosen.\n00:16:59 like using marginal likelihood comparisons, and those implicitly penalized for complexity instead of explicitly adding a penalty term.\n00:17:10 In practice, my feeling is that many of these methods are very conservative.\n00:17:20 Structural risk minimization is considered very conservative and may really not be as effective as just simply using a holdout set or a cross-validation set.\n00:17:30 However, the concept of VC dimension is certainly very useful in understanding the role of complexity in learning and the degree to which a more flexible class of learners can lead to overfitting and how.", "start_char_idx": 12798, "end_char_idx": 16066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fec9bd48-de84-4037-a5db-1a1c7f9123dd": {"__data__": {"id_": "fec9bd48-de84-4037-a5db-1a1c7f9123dd", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (1) Linear SVMs, primal form.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=IOetFPgsMUc", "Link": "https://www.youtube.com/watch?v=IOetFPgsMUc"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b36b93ab-3813-4bdc-a4b0-bbfd558bccf5", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (1) Linear SVMs, primal form.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=IOetFPgsMUc", "Link": "https://www.youtube.com/watch?v=IOetFPgsMUc"}, "hash": "6bff6e4d034551063fe2890c001a8b22513b9862453eb84346bde7297af7c67c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8f8a750-e266-4933-9267-129ee24223df", "node_type": "1", "metadata": {}, "hash": "640ab3a217b72c85e5d7cb8fdb568d01d5b16abdaf92a0d52a6aa8753d1d76d5", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 One of the most popular types of linear classifier is the support vector machine.\n00:00:04 Suppose we're training a linear classifier for a set of data.\n00:00:09 There may be a number of classifiers that all separate the data, in other words, have no training error.\n00:00:13 How can we choose among these?\n00:00:18 If we look at these two examples, we see that although the one on the left correctly classifies all the data, it does not look safe.\n00:00:23 Small perturbations in the data or the decision boundary could cause an incorrect prediction.\n00:00:27 So we may not trust it on test data, which would differ slightly from our training dataset.\n00:00:32 The one on the right, by contrast, looks more stable.\n00:00:36 There's a margin around the decision boundary in which no data are located.\n00:00:41 We already saw a bit of this with surrogate losses.\n00:00:46 Using a smooth loss, we'll prefer to move data away from the boundary.\n00:00:50 But in some sense, this was an accidental side effect.\n00:00:55 If a margin is what's desirable, how can we...\n00:00:59 create an optimization that explicitly tries to maximize it.\n00:01:07 To optimize the margin of our classifier, we'll have to relate it directly to the parameter vector.\n00:01:14 Here, instead of our usual theta parameters, I'll usually use w to indicate the weights associated with each feature, one through n, and to denote the constant term by b without listing the constant feature.\n00:01:22 It'll become clear shortly why it's useful to treat the constant term separately.\n00:01:29 To define the margin, we'll start by assuming that our parameters get all the data correct, and we'll enforce this point later.\n00:01:37 Another thing to note, for any setting of the parameters, the decision boundary is invariant to rescaling.\n00:01:44 For example, if this region is defined by the linear response being positive, then multiplying the parameters by 10 doesn't change the decision.\n00:01:52 For this reason, let's\n00:01:59 define the margin using an additional property that is not scale invariant.\n00:02:06 We'll define that not only are the positive class a region with positive response, they're in a region with positive response at least plus 1.\n00:02:13 Similarly, the negative data are in a region with response at most minus 1.\n00:02:19 Since these are isosurfaces of a linear response, they're also hyperplanes, just like and parallel to the decision boundary.\n00:02:26 Then we can define this margin explicitly in terms of these hyperplanes.\n00:02:33 Since no data are inside them, we'll define the margin as the distance between them.\n00:02:39 I've drawn this with data on the hyperplanes, but since we're going to maximize the margin, if there are no data on the plus and minus 1 hyperplanes, we'll simply end up changing the parameters to expand the margin until this is true.\n00:02:46 So you can also think of the margin as twice the distance from the decision boundary to the nearest training point.\n00:02:53 The points on the margin are referred to as the support vectors.\n00:02:59 Now, we can compute the margin geometrically in terms of the parameters.\n00:03:08 The first thing to note is that the vector, defined by the weights w, the coefficients and the features, is perpendicular to the decision boundary.\n00:03:17 To see this quickly, just note for any two points x and x' that are both on the boundary, the linear response is zero.\n00:03:25 So rearranging these equations, we see that w and the vector x'-x, which lies along the boundary, are perpendicular.\n00:03:34 Now let's pick any point x- that's in the part of the space where the negative data are allowed to lie, specifically on the minus one hyperplane.\n00:03:42 This is any point, not necessarily a data point.\n00:03:51 Take x-plus to be a point as close as possible to x-minus in the plus one space.\n00:04:00 orthogonal to these planes, the vector from x-minus to x-plus is some scalar multiple r of the vector w.", "start_char_idx": 0, "end_char_idx": 3981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8f8a750-e266-4933-9267-129ee24223df": {"__data__": {"id_": "d8f8a750-e266-4933-9267-129ee24223df", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (1) Linear SVMs, primal form.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=IOetFPgsMUc", "Link": "https://www.youtube.com/watch?v=IOetFPgsMUc"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b36b93ab-3813-4bdc-a4b0-bbfd558bccf5", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (1) Linear SVMs, primal form.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=IOetFPgsMUc", "Link": "https://www.youtube.com/watch?v=IOetFPgsMUc"}, "hash": "6bff6e4d034551063fe2890c001a8b22513b9862453eb84346bde7297af7c67c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fec9bd48-de84-4037-a5db-1a1c7f9123dd", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (1) Linear SVMs, primal form.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=IOetFPgsMUc", "Link": "https://www.youtube.com/watch?v=IOetFPgsMUc"}, "hash": "bde7eb6cbe6be85ecc34c2fafd4c4e5e155a8e21bd217b0f282b54bfbea5afea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19a53875-5b16-43fb-98c7-5918680dbf33", "node_type": "1", "metadata": {}, "hash": "febf75ca9754bb295c941363cb1fbc8e111a4276ba38d6e8d36ebee7282feec6", "class_name": "RelatedNodeInfo"}}, "text": "00:03:17 To see this quickly, just note for any two points x and x' that are both on the boundary, the linear response is zero.\n00:03:25 So rearranging these equations, we see that w and the vector x'-x, which lies along the boundary, are perpendicular.\n00:03:34 Now let's pick any point x- that's in the part of the space where the negative data are allowed to lie, specifically on the minus one hyperplane.\n00:03:42 This is any point, not necessarily a data point.\n00:03:51 Take x-plus to be a point as close as possible to x-minus in the plus one space.\n00:04:00 orthogonal to these planes, the vector from x-minus to x-plus is some scalar multiple r of the vector w.\n00:04:12 We also know that, since x-minus is on the negative hyperplane, and x-plus is on the positive hyperplane, their linear responses are minus 1 and plus 1, respectively.\n00:04:24 Some simple geometry will complete the analysis.\n00:04:36 We can compute the value of this scalar r by plugging our definition x-plus equals x-minus plus r times w into the positive hyperplane equation, giving this, then rearranging and plugging in the negative hyperplane equation here, giving the value of r.\n00:04:48 And the margin is just the distance from x-minus to x-plus.\n00:05:00 In other words, the length of r times w.\n00:05:07 Evaluating this is just 2 times the length of the vector w.\n00:05:15 So, for any parameters w and b satisfying our assumptions, in other words, all the positive data in the plus 1 region, all the negative data in the minus 1 region, the margin of w can be defined as 2 over its length.\n00:05:22 So, now let's go about actually optimizing the parameters.\n00:05:30 This is a constrained optimization problem.\n00:05:37 We want to maximize our margin equation, subject to the constraints that all our data lie in the specified region, in other words, on the correct side of their respective margin.\n00:05:45 Finding the value of w that maximizes 1 over w's length, we can equivalently find the value of w that minimizes w's length.\n00:05:52 The vector w with smallest length will also be the w that has the largest inverse length.\n00:06:00 Similarly, minimizing squared length instead doesn't change the location of the minimizer.\n00:06:06 So we'll reform our maximization as a minimization of the sum of Wj squared.\n00:06:13 To enforce the data constraints, we have one constraint per data point.\n00:06:20 If a point i is positive, then we need that the linear response be greater than plus one.\n00:06:26 If a point i is negative, we need for the linear response to be less than minus one.\n00:06:33 Framed this way, the margin problem is an example of a classic optimization problem called a quadratic program.\n00:06:40 We're minimizing a quadratic function of the parameters, the sum of W squared, subject to a collection of m linear constraints on the parameters, one for each data point that it lies in the correct region.\n00:06:46 Framing it this way makes it easy to apply a suite of optimization techniques and algorithms designed for quadratic programs.\n00:06:53 For reference, we call this formula.\n00:07:00 our max margin classifier, the primal problem, a point we'll contrast with later.\n00:07:07 It will also be convenient to compact these constraints into a single phrase that will work for both positive and negative data.\n00:07:15 Here, the target y is either plus 1 or minus 1.\n00:07:22 If it's plus 1, the linear response should also be positive and greater than 1, so the product will be also.\n00:07:30 If y is minus 1, then the linear response should be negative and large, so the product will, again, be greater than 1.\n00:07:37 In other words, our linear response should be larger than 1 in magnitude and match the sign of y.\n00:07:45 Let's see a simple example, small enough that we can plot both the feature and parameter spaces.", "start_char_idx": 3311, "end_char_idx": 7154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19a53875-5b16-43fb-98c7-5918680dbf33": {"__data__": {"id_": "19a53875-5b16-43fb-98c7-5918680dbf33", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (1) Linear SVMs, primal form.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=IOetFPgsMUc", "Link": "https://www.youtube.com/watch?v=IOetFPgsMUc"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b36b93ab-3813-4bdc-a4b0-bbfd558bccf5", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (1) Linear SVMs, primal form.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=IOetFPgsMUc", "Link": "https://www.youtube.com/watch?v=IOetFPgsMUc"}, "hash": "6bff6e4d034551063fe2890c001a8b22513b9862453eb84346bde7297af7c67c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8f8a750-e266-4933-9267-129ee24223df", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (1) Linear SVMs, primal form.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=IOetFPgsMUc", "Link": "https://www.youtube.com/watch?v=IOetFPgsMUc"}, "hash": "7089cda2f03ed23982a71f8dcaae5e01add6eb3513b2d443dc4aa87842ef416a", "class_name": "RelatedNodeInfo"}}, "text": "00:07:00 our max margin classifier, the primal problem, a point we'll contrast with later.\n00:07:07 It will also be convenient to compact these constraints into a single phrase that will work for both positive and negative data.\n00:07:15 Here, the target y is either plus 1 or minus 1.\n00:07:22 If it's plus 1, the linear response should also be positive and greater than 1, so the product will be also.\n00:07:30 If y is minus 1, then the linear response should be negative and large, so the product will, again, be greater than 1.\n00:07:37 In other words, our linear response should be larger than 1 in magnitude and match the sign of y.\n00:07:45 Let's see a simple example, small enough that we can plot both the feature and parameter spaces.\n00:07:52 Let's say we have one scalar feature x and three data points from the negative class at x equals minus 3 and minus 1 and the positive class at x equals minus 3.\n00:08:00 x equals 2.\n00:08:06 We can see that these data are easily separable, in fact, there are many linear classifiers that will separate them, with different weights a and constant or biased terms b.\n00:08:13 For example, here's one, here's another, here's a third.\n00:08:20 In fact, any classifier that transitions sign, crosses zero, between the blue and red points will predict all three points correctly.\n00:08:26 So let's find the classifier with largest margin.\n00:08:33 We can write our margin constraints, one for each data point, and visualize them in the two-dimensional parameter space, A and B.\n00:08:40 Rearranging these constraints, we have that the point at minus 3 constrains our parameter B to lie below this line.\n00:08:46 Our second point, at minus 1, to lie below this line.\n00:08:53 And our third point...\n00:09:00 constraint at 2 to lie above this line.\n00:09:10 So the set of parameters that satisfy our margin constraints is shown in shaded green.\n00:09:20 Notice that, as one would expect, the constraint here for x equals minus 3 is superseded by the constraint here for x equals minus 1.\n00:09:30 Any point within this shaded region corresponds to a valid perceptron with a margin.\n00:09:40 For example, setting A equals 1, B equals 0, this point here, corresponds to a linear classifier in the feature space, here, with a margin, meaning the region between response minus 1 and plus 1, shown between these dashed vertical lines.\n00:09:50 The SVM problem is now to minimize the non-constant weight magnitude.\n00:10:00 It's just the parameter a.\n00:10:07 The closest point in our constraint set to the a equals 0 line is this vertex here, with a equals 4 3rds and b equals minus 1 3rd.\n00:10:15 Notice that two constraints, here and here, are tight in the sense that our parameters lie on the constraint.\n00:10:23 This means that our solution would change if those constraints were not present.\n00:10:31 These are exactly the constraints due to points that lie on the margin.\n00:10:38 For example, if the negative example here were not present, the solution would change to widen the margin.\n00:10:46 In the next lecture, we'll see more about optimizing the support vector machine, including its dual form and a formulation for non-separable data.", "start_char_idx": 6410, "end_char_idx": 9602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c107ad4-d38a-4023-826e-e4edd65f7002": {"__data__": {"id_": "9c107ad4-d38a-4023-826e-e4edd65f7002", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (2) Dual & soft-margin forms.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=1aQLEzeGJC8", "Link": "https://www.youtube.com/watch?v=1aQLEzeGJC8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab07d92b-bcb6-405e-b831-d099703e2bdc", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (2) Dual & soft-margin forms.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=1aQLEzeGJC8", "Link": "https://www.youtube.com/watch?v=1aQLEzeGJC8"}, "hash": "1aad39722e82eedc09957e217ba147ce4f8ed226dc1644436032ddb2e8e62467", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd04d83f-084a-431d-a00a-5ad7437ebc61", "node_type": "1", "metadata": {}, "hash": "9b220630a34779bbaa343c77af1d0a16e84c24de39448e6914808e650f344f3e", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 We saw previously that support vector machines could be formulated as a constrained optimization, specifically a quadratic program.\n00:00:06 In this section, we'll consider this optimization, examine its dual form, and look at the soft margin version of an SVM.\n00:00:13 First, recall our constrained optimization for the SVM, minimizing the weights subject to constraints that enforce a correctness margin on each data point.\n00:00:20 Let's think about how we would actually optimize this constrained system.\n00:00:26 Even finding a point that satisfies these constraints is not trivial.\n00:00:33 The constraints can only be satisfied if we find a perceptron with zero error.\n00:00:40 Even assuming that one exists, just finding one such example was the entire purpose of the perceptron algorithm.\n00:00:46 To better solve this constrained optimization, we'll use Lagrangian optimization.\n00:00:53 Let's first rewrite our constraint slightly so that it's a non-positive inequality.\n00:00:59 Now, let's generically refer to our cost function here as f of theta and to the constraint for data point i as gi of theta, where theta just means all the parameters, both w and b.\n00:01:10 We can now introduce a Lagrange multiplier, alpha sub i, for each constraint g sub i.\n00:01:20 This will be used to enforce the constraint.\n00:01:30 The Lagrangian is given by a joint optimization over the original parameter's theta and the Lagrange multiplier's alpha, whereas before, the theta are to be minimized, but the alpha are to be maximized.\n00:01:40 We also have a very simple constraint on the parameter's alpha, that they be non-negative, so it's easy to initialize a value of theta and alpha that satisfy these constraints.\n00:01:50 We can then proceed to optimize theta and alpha together by, for example...\n00:02:00 gradient steps.\n00:02:06 Now consider what happens when we optimize over alpha i for any fixed theta.\n00:02:13 If the constraint gi is satisfied, then g of theta is negative and the largest value we can attain in this optimization is 0 by setting alpha i equal to 0.\n00:02:20 But if the constraint is not satisfied and gi is positive, alpha will increase.\n00:02:26 Then theta will have to change to decrease g for that constraint.\n00:02:33 In fact, any optimum of the original problem will be a saddle point to the Lagrangian and vice versa.\n00:02:39 This converts a set of constraints that were difficult to satisfy into a nearly unconstrained, easily satisfied constraint problem over more variables.\n00:02:46 Another point that will be useful in a moment is the result of the so-called KKT complementary slackness condition.\n00:02:53 It's easy to see that if we're at a saddle point, then either alpha...\n00:02:59 cannot increase the Lagrangian because its derivative is 0, so g i equals 0, or because it's itself constrained, so alpha i equals 0.\n00:03:07 So for any non-zero alpha i, meaning a slack constraint on the alpha i, it must be that the constraint on g is tight.\n00:03:14 So, we can simply optimize our Lagrangian over both the parameters w and b, and the Lagrange multiplier's alpha.\n00:03:22 The geometric consequence of complementary slackness is that the alpha i are only non-zero for points where the margin constraint is tight, in other words, points on the boundary of the margin.\n00:03:29 These points are called the support vectors.\n00:03:37 Notice now that we fix alpha.\n00:03:44 Then, we can solve directly for w and b in terms of alpha.\n00:03:52 This is now an unconstrained...\n00:03:59 quadratic function, this is quite simple.\n00:04:06 Taking the derivative and setting it equal to zero gives the optimal w is a linear combination of the data, a sum over i of alpha i y i times x i.\n00:04:13 Notice that since alpha i is zero for non-support vector data points, the max margin boundary solution w star depends only on the support vectors.\n00:04:19 We can also solve for b by plugging in the margin hyperplane equations for any support vector.", "start_char_idx": 0, "end_char_idx": 3994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd04d83f-084a-431d-a00a-5ad7437ebc61": {"__data__": {"id_": "bd04d83f-084a-431d-a00a-5ad7437ebc61", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (2) Dual & soft-margin forms.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=1aQLEzeGJC8", "Link": "https://www.youtube.com/watch?v=1aQLEzeGJC8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab07d92b-bcb6-405e-b831-d099703e2bdc", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (2) Dual & soft-margin forms.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=1aQLEzeGJC8", "Link": "https://www.youtube.com/watch?v=1aQLEzeGJC8"}, "hash": "1aad39722e82eedc09957e217ba147ce4f8ed226dc1644436032ddb2e8e62467", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c107ad4-d38a-4023-826e-e4edd65f7002", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (2) Dual & soft-margin forms.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=1aQLEzeGJC8", "Link": "https://www.youtube.com/watch?v=1aQLEzeGJC8"}, "hash": "96ad324cee7442872dcf5cd5b1ee6d46f4334aaf257c879b61aa4833d0b1c76d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64768b67-abfe-4b47-90b7-7cca631471a5", "node_type": "1", "metadata": {}, "hash": "f4161b0949dee22663019051d83836b6f8e3f5948a0dd540ea779f1de5bbff28", "class_name": "RelatedNodeInfo"}}, "text": "00:03:29 These points are called the support vectors.\n00:03:37 Notice now that we fix alpha.\n00:03:44 Then, we can solve directly for w and b in terms of alpha.\n00:03:52 This is now an unconstrained...\n00:03:59 quadratic function, this is quite simple.\n00:04:06 Taking the derivative and setting it equal to zero gives the optimal w is a linear combination of the data, a sum over i of alpha i y i times x i.\n00:04:13 Notice that since alpha i is zero for non-support vector data points, the max margin boundary solution w star depends only on the support vectors.\n00:04:19 We can also solve for b by plugging in the margin hyperplane equations for any support vector.\n00:04:26 So, for example, for this point here, we know that the linear response is plus one, and since we know the value of w, we can just solve for b.\n00:04:33 It's customary to average over a number of these points for numerical stability.\n00:04:40 So, for example, in this formula, we average over all the support vectors and SV of them.\n00:04:46 Finally, if we know the optimal value of w in terms of alphas, we can just plug it in to get an optimization solely over alpha.\n00:04:53 The resulting problem is called the Lagrangian dual of the original.\n00:05:00 problem.\n00:05:07 Plugging in the equation for W star and rearranging, we find that the dual is given here as a maximization over positive alpha.\n00:05:15 We also need to enforce the stationary condition on B that the derivative with respect to B is zero.\n00:05:22 Since the original equation was linear in B, this actually becomes a constraint here.\n00:05:30 Notice that this is now a quadratic function in alpha with a single linear constraint, so it's another quadratic program.\n00:05:37 This quadratic program is over m variables, one for each data point, the alpha i, with m simple inequality constraints, alphas are positive, and one linear equality constraint.\n00:05:45 The Lagrangian dual is always a lower bound on the original primal problem, our minimization over theta.\n00:05:52 Quadratic programs like this have a property called strong duality, which\n00:06:00 says that the value of this maximization over alpha will be the same as the primal problem.\n00:06:06 And as we saw, there's a simple transformation from any solution, alpha star, to a solution, w star, given by the equations in the previous slide.\n00:06:12 This dual form is mainly useful when m, the number of data points, is much smaller than n, the number of features.\n00:06:18 Notice that our optimization is now over alpha, which is length m.\n00:06:24 Evaluating the objective here is then O of m squared.\n00:06:30 And optimizing it is usually between quadratic and cubic and m, depending on the solver used, the optimization tolerances, and so on.\n00:06:36 While this situation may not sound common, it'll become important shortly.\n00:06:42 Before that, let's deal with the problem of non-separable data.\n00:06:48 We often find ourselves with data that are not linearly separable.\n00:06:54 So the margin constraints cannot be satisfied.\n00:07:00 for any value of W and B.\n00:07:06 The large margin principle for separable data suggests we should choose a model with small magnitude parameters.\n00:07:13 However, if we're forced to have non-zero error, we should trade this off with the error that results from our predictions.\n00:07:20 How can we do this?\n00:07:26 One solution is to allow some of our data points to violate the margin constraints, so a soft margin, but assign them a cost, for example the distance by which they violated the constraint scaled by some factor r.\n00:07:33 If r is chosen to be very large, we'll pay a lot of attention to making sure no data violate the margin if possible.\n00:07:40 On the other hand, if r is small, we'll try to maximize the margin for most data but allow some of them to violate it.\n00:07:46 We do so by adding so-called slack variables, epsilon i, one for each data point.", "start_char_idx": 3326, "end_char_idx": 7263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64768b67-abfe-4b47-90b7-7cca631471a5": {"__data__": {"id_": "64768b67-abfe-4b47-90b7-7cca631471a5", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (2) Dual & soft-margin forms.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=1aQLEzeGJC8", "Link": "https://www.youtube.com/watch?v=1aQLEzeGJC8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab07d92b-bcb6-405e-b831-d099703e2bdc", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (2) Dual & soft-margin forms.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=1aQLEzeGJC8", "Link": "https://www.youtube.com/watch?v=1aQLEzeGJC8"}, "hash": "1aad39722e82eedc09957e217ba147ce4f8ed226dc1644436032ddb2e8e62467", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd04d83f-084a-431d-a00a-5ad7437ebc61", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (2) Dual & soft-margin forms.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=1aQLEzeGJC8", "Link": "https://www.youtube.com/watch?v=1aQLEzeGJC8"}, "hash": "64cc24fd1fbaead9f262d51e4a74c0cff62d71982cd139f1dada05b82ad7ff62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbf64994-6b87-40dd-aa98-60fdbbc9fb92", "node_type": "1", "metadata": {}, "hash": "14c3596079eb46b2298a7cbf05c630d9fc541d12280611c2e00aa42e63f93001", "class_name": "RelatedNodeInfo"}}, "text": "00:07:13 However, if we're forced to have non-zero error, we should trade this off with the error that results from our predictions.\n00:07:20 How can we do this?\n00:07:26 One solution is to allow some of our data points to violate the margin constraints, so a soft margin, but assign them a cost, for example the distance by which they violated the constraint scaled by some factor r.\n00:07:33 If r is chosen to be very large, we'll pay a lot of attention to making sure no data violate the margin if possible.\n00:07:40 On the other hand, if r is small, we'll try to maximize the margin for most data but allow some of them to violate it.\n00:07:46 We do so by adding so-called slack variables, epsilon i, one for each data point.\n00:07:53 Epsilon i measures the amount the data point i violates the margin constraint here.\n00:08:00 It is always non-negative, zero if the constraint is satisfied, and then we add a penalty r times epsilon to our objective function, balancing the margin term, w squared, with the amount of slack.\n00:08:08 Notice that this new formulation remains a quadratic program.\n00:08:17 It's a quadratic objective in w and epsilon, subject to linear constraints.\n00:08:25 But let's look for a moment at this optimization.\n00:08:34 In fact, the constraints are far less difficult to satisfy during optimization, since now for any weight vector w, we can always choose a value of epsilon that minimizes its term and satisfies these constraints.\n00:08:42 This means that first, we can always initialize a solution, w and epsilon and b, to something that satisfies the constraints, even if it doesn't minimize the objective.\n00:08:51 Second, the optimal value of epsilon...\n00:09:00 epsilon given w is easy to select.\n00:09:08 If data point i does not satisfy the margin, this with epsilon equal to zero, then the smallest value of epsilon that enforces this inequality to be true will be to set the two sides equal.\n00:09:17 Let's choose that optimal value of epsilon for a given w, and then we can optimize the resulting cost as a function of just w directly.\n00:09:25 What we find is that the cost j is only non-zero for data points that do not satisfy the margin constraint.\n00:09:34 And for those points, it grows linearly with their distance to the margin.\n00:09:42 Sketching this in the same form as our usual surrogate loss pictures, we see that for a positive data point, y equals plus one, if the linear response is already greater than plus one, it has no cost.\n00:09:51 On the other hand, if it's less than plus one, the cost will increase linearly with its distance from the margin.\n00:10:00 This surrogate loss is called the hinge loss for its hinge-like shape.\n00:10:08 Its analytic form, shown here, is piecewise linear.\n00:10:17 It's either 0 or a positive cost that increases away from the margin.\n00:10:25 Our overall optimization, then, is the margin term, sum of w squared, plus r times the hinge loss.\n00:10:34 In other words, a balance between the margin term and a term having to do with slack variables.\n00:10:42 If we just divide this whole thing by r, we get that the optimal parameters minimize the sum of a data term, the hinge loss, plus 1 over r times an L2-like regularization term on the weights.\n00:10:51 This form should now be very familiar as a standard linear classifier.\n00:10:59 optimization from before, the only difference being the choice of the hinge loss as the surrogate loss, and not regularizing the constant coefficient B, which is itself another common thing to do in regularization.\n00:11:11 We can then optimize this form in whatever manner we like, such as our standard stochastic gradient algorithm from the linear classifier.\n00:11:23 If we take the dual of the soft margin quadratic program, we again obtain a quadratic program similar to before, over only the Lagrange multiplier's alpha, with just one minor modification.\n00:11:35 The alphas are now bounded from above as well by R.", "start_char_idx": 6534, "end_char_idx": 10492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbf64994-6b87-40dd-aa98-60fdbbc9fb92": {"__data__": {"id_": "dbf64994-6b87-40dd-aa98-60fdbbc9fb92", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (2) Dual & soft-margin forms.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=1aQLEzeGJC8", "Link": "https://www.youtube.com/watch?v=1aQLEzeGJC8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab07d92b-bcb6-405e-b831-d099703e2bdc", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (2) Dual & soft-margin forms.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=1aQLEzeGJC8", "Link": "https://www.youtube.com/watch?v=1aQLEzeGJC8"}, "hash": "1aad39722e82eedc09957e217ba147ce4f8ed226dc1644436032ddb2e8e62467", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64768b67-abfe-4b47-90b7-7cca631471a5", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (2) Dual & soft-margin forms.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=1aQLEzeGJC8", "Link": "https://www.youtube.com/watch?v=1aQLEzeGJC8"}, "hash": "2e88144a1c29e39d66df31f30ff3d39489a9c44462c1834d2ae335cbeab2e47d", "class_name": "RelatedNodeInfo"}}, "text": "00:10:42 If we just divide this whole thing by r, we get that the optimal parameters minimize the sum of a data term, the hinge loss, plus 1 over r times an L2-like regularization term on the weights.\n00:10:51 This form should now be very familiar as a standard linear classifier.\n00:10:59 optimization from before, the only difference being the choice of the hinge loss as the surrogate loss, and not regularizing the constant coefficient B, which is itself another common thing to do in regularization.\n00:11:11 We can then optimize this form in whatever manner we like, such as our standard stochastic gradient algorithm from the linear classifier.\n00:11:23 If we take the dual of the soft margin quadratic program, we again obtain a quadratic program similar to before, over only the Lagrange multiplier's alpha, with just one minor modification.\n00:11:35 The alphas are now bounded from above as well by R.\n00:11:47 Intuitively, this says that if a data point violates the margin constraint, the Lagrange multiplier alpha i for that data point will increase until it's at most R times the violated distance, i.e. R times epsilon.\n00:11:59 Complementary slackness now tells us that the alphas are non-zero only on data that are either at the margin or on the wrong side.\n00:12:08 In other words, for positive data, say, any points where the linear response is less than or equal to plus one.\n00:12:17 As I mentioned, the dual form can be very important when we have many more features than data points.\n00:12:25 In other words, n is much greater than m.\n00:12:34 A key thing to notice about the dual form of the SVM is the quadratic program involves the features x only through their dot products.\n00:12:42 In other words, the coefficient of the ij interaction term between alpha i and alpha j is the inner product of the data point xi and xj.\n00:12:51 Let's call this inner product K sub ij as the ij-th entry in some matrix K, sometimes called the Gram matrix.\n00:12:59 We can think of this quantity as measuring the similarity of two data points, here through their dot product.\n00:13:08 If the vectors are in the same direction, this takes its maximal value, if they're orthogonal it's zero, and if they're in opposite directions, you get negative values.\n00:13:17 This point will be useful in a moment.\n00:13:25 Interestingly, prediction also only involves dot products.\n00:13:34 Using our solution for W star from before, we find that our prediction y for a new test point x is a linear combination of the dot products of x with the support vectors xi, the points where alpha i is non-zero.\n00:13:42 As a side note here, evaluating b is a bit more complicated, but as before, any support vector with slack on alpha, so not equal to zero or r, will have a tight margin constraint, and can be used to solve for b.\n00:13:51 Typically, b is kept updated while alpha is being solved.\n00:13:59 see more about how we can use this dot product fact in the next lecture, which discusses the kernel trick.", "start_char_idx": 9581, "end_char_idx": 12585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d3ea0b0-1025-4fa4-9738-82d1d6c64ae7": {"__data__": {"id_": "7d3ea0b0-1025-4fa4-9738-82d1d6c64ae7", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (3) Kernels.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OmTu0fqUsQk", "Link": "https://www.youtube.com/watch?v=OmTu0fqUsQk"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df21632a-c358-487b-8023-0ec7e7add3dd", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (3) Kernels.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OmTu0fqUsQk", "Link": "https://www.youtube.com/watch?v=OmTu0fqUsQk"}, "hash": "2880d7b1eb94f88bc8ed5d017008b77cd93075f740121eebd5e9c8578fb1a17f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3496f612-a1bc-4b22-96f7-9d314615e8af", "node_type": "1", "metadata": {}, "hash": "cbf1d93eb0c5e73261c05f1b44a080bbc888462e2bc864a9067d775a02edace6", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 We've now seen both the primal and dual forms of the support vector machine classifier.\n00:00:10 In this part of the lecture, we'll look at one of the ideas that makes support vector machines so popular, called the kernel trick.\n00:00:20 Up until now, we've looked at linear SVMs, meaning that the classifier was effectively a simple perceptron, in other words, linear weights W on the input features, resulting in a linear decision boundary.\n00:00:30 We developed the Lagrangian optimization form, which led us to an equivalent dual formation of the learning process, in which the objective function depended on the matrix of dot products between each pair of data points, xi and xj, called the Gram matrix.\n00:00:40 This value can be viewed as measuring a form of angular similarity between xi and xj, in the sense that it's zero if xi and xj are orthogonal directions, and large if they're in the same direction.\n00:00:50 Now, recall what we did for linear...\n00:01:00 classifiers when we wanted to increase their representational complexity.\n00:01:08 If our data were not linearly separable in the original feature, x1, we could add additional deterministic features, for example, going from one feature x1 to two features x1 and x2, equaling x1 squared, so that the data all lie on this curve.\n00:01:17 Then in our new higher dimensional space, the data were more likely to be linearly separable.\n00:01:25 Let's see how this affects the dual form of the SVM.\n00:01:34 Now using a feature transform phi, we transform xi and xj to find their new feature vectors.\n00:01:42 Then the dual form involves a dot product between these transformed vectors.\n00:01:51 As an illustrative example, let's consider the polynomial features given here, so 1, x1, x2, x1 squared, x2 squared.\n00:01:59 squared, cross products, and so on.\n00:02:10 We'll see the purpose of these scaling root 2 terms shortly.\n00:02:20 So for the dual form, we need to compute the inner product of two expanded feature vectors, phi of xi and phi of xj.\n00:02:30 Let's denote xi and xj as just a and b for convenience.\n00:02:40 And listing out our features for both points, we can compute them here.\n00:02:50 And we find that the dot product is the sum 1 from the product of 1's, plus the sum of 2 times a1 b1 here, plus 2 a2 b2 here, and so on, forming this term, plus a1 squared b1 squared here, a2 squared b2 squared, forming this term, plus 2 a1 a2 b1 b2, and so on for all pairs here.\n00:03:00 If we then manipulate this sum algebraically, we find that it's equivalent to a much simpler computation.\n00:03:10 1 plus the sum of ajbj squared.\n00:03:20 Or, take the dot product of the original feature vectors a and b, add 1, and then square the whole thing.\n00:03:30 This is straightforward to verify with a bit of effort, and a similar relationship holds for higher degrees as well.\n00:03:40 Denoting this value k of ab, instead of actually taking our higher dimensional feature transform, and then computing the inner products of those vectors, we could instead just take this very simple nonlinear function of similarity, also called a kernel, and compute it in time linear in only the original number of features of a and b, regardless of how many features our expansion phi has.\n00:03:50 So, for example, for quadratic features,\n00:04:00 Phi of a and phi of b have O of n squared features, and computing their dot product takes O of n squared time, but computing it this way instead takes only O of n computations.\n00:04:08 In fact, it turns out that this concept can be made quite general.\n00:04:17 Any non-linear function k of x, x prime, which satisfies a particular condition called Mercer's condition, can be viewed as corresponding to the dot product between transformed vectors phi of x, for some transformation phi.\n00:04:25 Then the similarity function k is referred to as a kernel or a Mercer kernel.", "start_char_idx": 0, "end_char_idx": 3892, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3496f612-a1bc-4b22-96f7-9d314615e8af": {"__data__": {"id_": "3496f612-a1bc-4b22-96f7-9d314615e8af", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (3) Kernels.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OmTu0fqUsQk", "Link": "https://www.youtube.com/watch?v=OmTu0fqUsQk"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df21632a-c358-487b-8023-0ec7e7add3dd", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (3) Kernels.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OmTu0fqUsQk", "Link": "https://www.youtube.com/watch?v=OmTu0fqUsQk"}, "hash": "2880d7b1eb94f88bc8ed5d017008b77cd93075f740121eebd5e9c8578fb1a17f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d3ea0b0-1025-4fa4-9738-82d1d6c64ae7", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (3) Kernels.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OmTu0fqUsQk", "Link": "https://www.youtube.com/watch?v=OmTu0fqUsQk"}, "hash": "7956b37bb8a3b201a4e8eed6ca3f260c764285bf0fa4dd7850d153bbad366a96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45577703-9eba-4055-a6d3-b8a99e089027", "node_type": "1", "metadata": {}, "hash": "7c5e14cc789c1ddbea91de66de0e0b0779f835b4629ce8eb9507fb770c6617a2", "class_name": "RelatedNodeInfo"}}, "text": "00:03:50 So, for example, for quadratic features,\n00:04:00 Phi of a and phi of b have O of n squared features, and computing their dot product takes O of n squared time, but computing it this way instead takes only O of n computations.\n00:04:08 In fact, it turns out that this concept can be made quite general.\n00:04:17 Any non-linear function k of x, x prime, which satisfies a particular condition called Mercer's condition, can be viewed as corresponding to the dot product between transformed vectors phi of x, for some transformation phi.\n00:04:25 Then the similarity function k is referred to as a kernel or a Mercer kernel.\n00:04:34 As a side note, Mercer's condition is effectively asserting that the gram matrix k has a positive semi-definite structure for any possible data sets.\n00:04:42 For polynomial features, we saw there was a direct correspondence between a particular k, which we showed, and a particular feature vector phi.\n00:04:51 However, for an arbitrary...\n00:04:59 function k, it may be quite hard to find exactly what feature vector phi corresponds to that k.\n00:05:06 In fact, many useful kernel functions correspond to infinite dimensional phi vectors.\n00:05:13 So, using such a kernel is equivalent to a linear classifier with an infinite number of constructed features.\n00:05:20 Yet, it's not computationally harder, as long as k itself is easy to compute.\n00:05:26 Then, we can compute the gram matrix in O of m squared time and space, and solve the resulting quadratic program in quadratic to cubic time.\n00:05:33 This is really an extreme of the n much much greater than m, number of features much much greater than the number of data domain.\n00:05:40 Our computation no longer depends on the number of features n, and so making n go to infinity is not a problem.\n00:05:46 There are a number of common kernel choices.\n00:05:53 Here, I'll show a few examples and plot an example of the kernel function value for a single scalar input feature x.\n00:06:00 to show how the kernel measures similarity in the original space.\n00:06:05 We saw the polynomial kernel.\n00:06:10 For degree d, it corresponds to a particular degree d expansion of the features.\n00:06:16 Looking at the kernel value here for d equals 3, we can see that data are in the direction of x from the origin get larger values.\n00:06:21 So here I have data point x equals 1.\n00:06:27 This is the similarity to that point.\n00:06:32 Data that are in the same direction as x, this way, get larger values.\n00:06:38 And the further they are in that direction, the more positive the values are.\n00:06:43 Perhaps the most common kernel function is the radial basis function, or Gaussian similarity kernel.\n00:06:49 This kernel results in high values near the point x, falling off as a Gaussian with some spread, sigma, as we move away from that point.\n00:06:54 Here, the parameter sigma can be used to control over and underfitting.\n00:06:59 chosen very large, all data will look similar to the kernel.\n00:07:08 For sigma small, only a few points will look similar to any particular test point.\n00:07:17 This is in addition to the value R in the dual optimization we discussed before, which effectively controls the severeness of the penalty for misclassified points.\n00:07:25 One can also use hyperbolic tangent-like similarity functions which transition from 0 to 1 as they move in the direction of the point x, so that data that are sufficiently in the direction of x, however far in that direction, all get equivalently high similarity values.\n00:07:34 Kernels are a major strength of support vector machines and systems with small to moderate numbers of data.\n00:07:42 While radial basis functions are probably the most common choice, a powerful feature of support vector machines is that we can design similarity functions based on the task at hand.\n00:07:51 So, for example, we can develop\n00:07:59 similarity measures for, say, string-like data, such as text or DNA sequences, that work by performing string matching and measuring a sequence-like similarity that includes the potential for insertions, deletions, and so on.", "start_char_idx": 3261, "end_char_idx": 7379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45577703-9eba-4055-a6d3-b8a99e089027": {"__data__": {"id_": "45577703-9eba-4055-a6d3-b8a99e089027", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (3) Kernels.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OmTu0fqUsQk", "Link": "https://www.youtube.com/watch?v=OmTu0fqUsQk"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df21632a-c358-487b-8023-0ec7e7add3dd", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (3) Kernels.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OmTu0fqUsQk", "Link": "https://www.youtube.com/watch?v=OmTu0fqUsQk"}, "hash": "2880d7b1eb94f88bc8ed5d017008b77cd93075f740121eebd5e9c8578fb1a17f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3496f612-a1bc-4b22-96f7-9d314615e8af", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (3) Kernels.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OmTu0fqUsQk", "Link": "https://www.youtube.com/watch?v=OmTu0fqUsQk"}, "hash": "45939ea65a155a57fca710b7daddcf4055cbf12e1a7098915f2013a6e2d1a2a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a958742-7e86-474a-b13d-f0f00d11a708", "node_type": "1", "metadata": {}, "hash": "d9fd683606c3b23dfa34d5716724f93d3d061b37b2846d0d345dcab85e965998", "class_name": "RelatedNodeInfo"}}, "text": "00:07:25 One can also use hyperbolic tangent-like similarity functions which transition from 0 to 1 as they move in the direction of the point x, so that data that are sufficiently in the direction of x, however far in that direction, all get equivalently high similarity values.\n00:07:34 Kernels are a major strength of support vector machines and systems with small to moderate numbers of data.\n00:07:42 While radial basis functions are probably the most common choice, a powerful feature of support vector machines is that we can design similarity functions based on the task at hand.\n00:07:51 So, for example, we can develop\n00:07:59 similarity measures for, say, string-like data, such as text or DNA sequences, that work by performing string matching and measuring a sequence-like similarity that includes the potential for insertions, deletions, and so on.\n00:08:08 In practice, such functions may not actually be Mercer kernels.\n00:08:17 So they may not correspond to any particular feature transform, but often still seem to perform very well empirically.\n00:08:25 For extremely large data sets, however, kernel forms of support vector machines are less common, and linear SVMs with explicit features are more typical.\n00:08:34 For these data sets, the O of M squared cost of working in a dual may be too much, and the primal form, optimizing the hinge loss directly, using, for example, stochastic gradient descent, is preferred.\n00:08:42 A quick comment on the memory behavior of linear and kernel SVMs as well.\n00:08:51 For linear SVMs, we typically remember the actual linear parameters, in other words, the weight.\n00:08:59 and the bias term, so we have n plus 1 parameters, no matter how many data we have.\n00:09:09 Alternatively, as we saw, we could simply store the support vectors, the data points that contribute to the decision boundary, since, as we show, the decision is also computable using the dot products between the test point and the support vectors.\n00:09:19 For kernelized SVMs, on the other hand, the kernel function may correspond to a high or even infinite dimensional feature transform.\n00:09:29 We have no choice but to store the actual support vector data and compute their similarity to new test points using the kernel function.\n00:09:39 Thus, the kernelized SVM is usually more of an instance-based learner, which memorizes certain instances of the training data to make a decision, so think like nearest neighbors, rather than a parametric one that saves some fixed length vector of parameters, like a standard perceptron.\n00:09:49 In fact, the kernel trick can be applied in many linear systems.\n00:09:59 leading to powerful nonlinear prediction methods.\n00:10:08 Here, I'll show how to use the kernel trick in least squares regression to increase its representational power.\n00:10:17 First, recall the standard L2 regularized linear regression solution theta equals yx x transpose x plus alpha i inverse where alpha i is the regularization term.\n00:10:25 We'll manipulate this form to show how it can be expressed in terms of the gram matrix K which allows us to substitute an arbitrary kernel function in the computation of the elements of K.\n00:10:34 Note that non-zero regularization is critical to this trick since for complex kernels, the effective number of features n may be much larger than m.\n00:10:42 So, alpha is critical in making the solution well-posed.\n00:10:51 First, we'll rearrange by moving the inverse over to the left-hand side.\n00:10:59 Now, distribute the product, move theta x transpose x back to the right-hand side, and factor out the matrix x.\n00:11:08 Let's define an intermediate vector r.\n00:11:17 This is just the residual vector, r minus our prediction, theta x transpose, scaled by 1 over alpha.\n00:11:25 Just by definition, theta equals r times x, from here.\n00:11:34 Also by definition, alpha r equals the residual, y minus theta x transpose.\n00:11:42 Plugging in theta equals r x to this equation, we get alpha r equals y minus r x transpose x.", "start_char_idx": 6516, "end_char_idx": 10539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a958742-7e86-474a-b13d-f0f00d11a708": {"__data__": {"id_": "9a958742-7e86-474a-b13d-f0f00d11a708", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (3) Kernels.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OmTu0fqUsQk", "Link": "https://www.youtube.com/watch?v=OmTu0fqUsQk"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df21632a-c358-487b-8023-0ec7e7add3dd", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (3) Kernels.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OmTu0fqUsQk", "Link": "https://www.youtube.com/watch?v=OmTu0fqUsQk"}, "hash": "2880d7b1eb94f88bc8ed5d017008b77cd93075f740121eebd5e9c8578fb1a17f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45577703-9eba-4055-a6d3-b8a99e089027", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (3) Kernels.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OmTu0fqUsQk", "Link": "https://www.youtube.com/watch?v=OmTu0fqUsQk"}, "hash": "48477cc52ec2bb82c9a52ed2965627345c5f1314d68d2758e89c155f7066518e", "class_name": "RelatedNodeInfo"}}, "text": "00:10:42 So, alpha is critical in making the solution well-posed.\n00:10:51 First, we'll rearrange by moving the inverse over to the left-hand side.\n00:10:59 Now, distribute the product, move theta x transpose x back to the right-hand side, and factor out the matrix x.\n00:11:08 Let's define an intermediate vector r.\n00:11:17 This is just the residual vector, r minus our prediction, theta x transpose, scaled by 1 over alpha.\n00:11:25 Just by definition, theta equals r times x, from here.\n00:11:34 Also by definition, alpha r equals the residual, y minus theta x transpose.\n00:11:42 Plugging in theta equals r x to this equation, we get alpha r equals y minus r x transpose x.\n00:11:51 We can then rearrange and solve for...\n00:11:59 R, which gives a form similar to before, but it's now in terms of X X transpose, the M by M gram matrix of dot product similarities between each pair of data points.\n00:12:11 So denoting X X transpose by K, K is an M by M matrix made up of ijth entry is Xi dot Xj. Then our prediction at a new test point, say X tilled, is just the dot product between theta and X tilled.\n00:12:23 We know that theta equals R X.\n00:12:35 So that's R X times X tilled, which is a sum over data points J of RJ times the dot product of data point J with X tilled or a sum of RK times the similarity between X, J and X tilled in this dot product form.\n00:12:47 If we replace K with a nonlinear similar\n00:12:59 similarity kernel, we can solve for r in the same way, so r is a vector with one entry per data point, and then use it and the training data to predict a new point using this form.\n00:13:09 As one side note, unlike the alpha before, here r will not be sparse, it will be non-zero for all the data.\n00:13:19 Another common support vector regression method uses a hinge-like regression loss that's 0 up to some amount of error, and this allows some of the r values to be 0 as well.\n00:13:29 Another point worth mentioning is the similarity of this prediction rule to methods like locally-weighted regression.\n00:13:39 In summary, support vector machines are a variant of linear classifier that adheres to the large margin principle, that we should prefer a classifier that's as far from the data as possible.\n00:13:49 In linearly separable\n00:13:59 data, we saw how to formulate this as a standard quadratic program, we saw how to optimize it using its Lagrangian, and we saw the form of the dual quadratic program, which involves only dot product similarity between each pair of data points.\n00:14:09 The soft margin variant for non separable data includes a penalty for margin violations or slack term.\n00:14:19 We saw that this is equivalent to using a simple surrogate loss called the hinge loss and applying a form of L2 regularization.\n00:14:29 Again, the optimization has a dual form that involves only pairwise similarity between the data.\n00:14:39 Finally, we saw how we could exploit these dual forms to create classifiers that work implicitly in very high dimensional feature spaces by defining nonlinear similarity kernels that measure the dot product in those spaces.\n00:14:49 This makes our classifier independent of the implicit feature dimension, but computationally quadratic to cubic in the amount of training\n00:14:59 data, so it's most useful for small to moderate datasets.", "start_char_idx": 9861, "end_char_idx": 13179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02a8e608-a26f-47b3-a592-8c954e86ee29": {"__data__": {"id_": "02a8e608-a26f-47b3-a592-8c954e86ee29", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Neural Networks (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=bH6VnezBZfI", "Link": "https://www.youtube.com/watch?v=bH6VnezBZfI"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba613405-9475-4938-ae68-036aa597a1c3", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Neural Networks (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=bH6VnezBZfI", "Link": "https://www.youtube.com/watch?v=bH6VnezBZfI"}, "hash": "cf1c561d172ba9d35cde96b27b8527a7eba5b471d2e85aa3542f3b6118fbfb6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee1a19b4-8c93-4840-a75e-2312c9f57e69", "node_type": "1", "metadata": {}, "hash": "d88c219609204304123bbf344eec284ea3b62df7462c56dfd518d796c2367eb9", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Next, let's talk about an extension of the basic linear perceptron model called multi-layer perceptrons, which also commonly go by the name of neural networks.\n00:00:10 If you recall, a perceptron simply refers to a binary linear classifier, which partitions the space into parts using linear function features.\n00:00:20 Its decision boundary is a linear n-1 dimensional hyperplane, so for example, for two features, the decision boundary is just a line.\n00:00:30 Practically speaking, we associate a set of parameters, or weights, here I'll use w for weights, one with each feature, and one with a constant feature.\n00:00:40 We compute a linear response, w0 plus w1 x1 plus w2 x2 and so on, and then binarize the output by thresholding, for example, taking a sine of our linear response.\n00:00:50 So for illustration, if we had only one feature, x1, our output functions would look like\n00:01:00 like this, dividing a one-dimensional feature space into two parts, negative and positive, with a single transition point.\n00:01:07 If we would like to design a more complex classifier, in other words one with a more complex decision boundary, one way to accomplish this is to augment our original feature set by using new transformed features, just as we did in linear regression.\n00:01:15 Here I'll illustrate with a single scalar feature x, then a basic linear perceptron thresholds a linear function of x, ax plus b.\n00:01:22 The decision boundary, where the linear function changes from negative to positive, is the solution of this linear function, a single point.\n00:01:30 We could then augment our features with a new transformation of x.\n00:01:37 A common choice, for example, might be polynomial features, like we used in linear regression.\n00:01:45 For this example, with a scalar x, we can add an additional feature x squared.\n00:01:52 The decision boundary, where the linear function changes from negative to positive, is the solution of this linear function, a single point.\n00:02:00 would then be the set of solutions to ax squared plus bx plus c equals 0, which could be two points instead of only one.\n00:02:06 For example, on these data, we could separate the data and achieve zero training error by using a cubic function, something like this.\n00:02:13 Then, thresholding at 0 would produce positive and negative regions in the desired areas.\n00:02:20 In general, more artificially constructed features will yield a more complex decision boundary.\n00:02:26 But we're certainly not restricted to only use polynomial features.\n00:02:33 We can choose whatever kinds of features we'd like.\n00:02:39 In fact, let's consider a different class of features obtained by thresholding our original feature at different points.\n00:02:46 These nonlinear transforms can actually be just as useful as polynomials.\n00:02:53 Suppose, again, we have only one input feature, x, and we create three new binary.\n00:02:59 valued features by thresholding x at various points.\n00:03:05 So f1 is positive only if x is greater than some value.\n00:03:10 f2 is positive if x is greater than some other value.\n00:03:16 f3 is positive if x is greater than a third value, and so on.\n00:03:21 By creating a linear combination of these new features, such as f1 minus f2 plus f3, we can actually trace out exactly the same shape of classifier that we did with the cubic features from before.\n00:03:27 So here, f1 becomes positive.\n00:03:32 Then when f2 becomes positive, we go back down.\n00:03:38 Then when f3 becomes positive again, we go back up.\n00:03:43 What's particularly interesting about this view is that these simple step function features are just perceptrons themselves.\n00:03:49 By using them as input features to a second perceptron, we've created a stack of two linear classifiers whose output can be more complex than a single standard perceptron.\n00:03:54 This multi-layer perceptron is illustrated here.\n00:04:00 We'll use the term input features to refer to the original data measurements.\n00:04:07 Then there may be one or more hidden layers that compute linear combinations of the previous layer and threshold them.", "start_char_idx": 0, "end_char_idx": 4127, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee1a19b4-8c93-4840-a75e-2312c9f57e69": {"__data__": {"id_": "ee1a19b4-8c93-4840-a75e-2312c9f57e69", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Neural Networks (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=bH6VnezBZfI", "Link": "https://www.youtube.com/watch?v=bH6VnezBZfI"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba613405-9475-4938-ae68-036aa597a1c3", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Neural Networks (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=bH6VnezBZfI", "Link": "https://www.youtube.com/watch?v=bH6VnezBZfI"}, "hash": "cf1c561d172ba9d35cde96b27b8527a7eba5b471d2e85aa3542f3b6118fbfb6e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02a8e608-a26f-47b3-a592-8c954e86ee29", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Neural Networks (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=bH6VnezBZfI", "Link": "https://www.youtube.com/watch?v=bH6VnezBZfI"}, "hash": "a5713984d4e4e099921733ea9058b9fde8c5b9d71f75e4b34bbe2283479bce2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27fe44aa-0e3b-47e7-bf07-0f04f9f0258e", "node_type": "1", "metadata": {}, "hash": "9e1f7a7b9ed86739bd2632feabf130ed2d6e0f593f2f79c5c0d4559c8645c209", "class_name": "RelatedNodeInfo"}}, "text": "00:03:27 So here, f1 becomes positive.\n00:03:32 Then when f2 becomes positive, we go back down.\n00:03:38 Then when f3 becomes positive again, we go back up.\n00:03:43 What's particularly interesting about this view is that these simple step function features are just perceptrons themselves.\n00:03:49 By using them as input features to a second perceptron, we've created a stack of two linear classifiers whose output can be more complex than a single standard perceptron.\n00:03:54 This multi-layer perceptron is illustrated here.\n00:04:00 We'll use the term input features to refer to the original data measurements.\n00:04:07 Then there may be one or more hidden layers that compute linear combinations of the previous layer and threshold them.\n00:04:15 And then the final layer is referred to as the output layer, which produces our actual class prediction.\n00:04:22 Moreover, what will make this framework particularly appealing will be our ability to train the entire system, in effect, updating the definition of our hidden layer features so as to improve that performance of the later output perceptron layers.\n00:04:30 For the same reason we did in the perceptron, we'll replace these hard threshold operators with something smooth and differentiable, like a logistic function or a hyperbolic tangent sigmoid function.\n00:04:37 The parameters of this model encompass all the weights at any level.\n00:04:45 Notationally, each node at a hidden or output layer is a perceptron, with a collection of weights defining the linear response at that node.\n00:04:52 Because there are multiple nodes, we'll use a double subscript.\n00:05:00 with Wij indicating the weight for node i associated with feature j at the previous layer.\n00:05:07 So, for example, weight 1, 1 is the weight for node 1 associated with feature 1 at the previous layer.\n00:05:15 Weight 1, 0 is the weight for node 1 associated with the constant feature, and so on.\n00:05:22 We'll also need superscripts to distinguish between layers when necessary.\n00:05:30 So, for example, W superscript 1 refers to the first layer of weights, which we can assemble as a matrix like so.\n00:05:37 W superscript 2 refers to the second layer of weights, and so on.\n00:05:45 We can use this kind of functional structure for classification problems or for regression.\n00:05:52 For regression, since we want a real-valued output that might not be bounded, we usually discard the saturating nonlinearity activation function on the output nodes.\n00:06:00 Viewed from this perspective, a multilayer perceptron is an overall function made up of small, simple building blocks, each element of which is just a perceptron function.\n00:06:06 As we build layers upon layers, our representational complexity increases.\n00:06:13 With only one layer, for example, we just get a standard perceptron with a linear decision boundary.\n00:06:20 With a two-layer model, these outputs become our input features to the next layer.\n00:06:26 So the features are simple, they're soft thresholds of linear combinations of the input, but then by combining them, we can get more complex outputs, like the ones we saw before.\n00:06:33 If we add a third layer on top of that, now these complex outputs become the new features for the next layer, leading to even more potentially complex outputs, and so on.\n00:06:40 Internet cutting-edge research focuses on so-called deep networks, with many layers.\n00:06:46 Training these deep networks can be very challenging in and of itself.\n00:06:53 We will\n00:07:00 focus on it too much here, but it's a very hot topic right now.\n00:07:08 It's easy enough to show that given enough hidden nodes, even a two-layer network can actually represent any output function arbitrarily closely.\n00:07:17 To see this, you can just think of each hidden node firing or activating, turning on, in a particular region of space.\n00:07:25 So in 1D, for example, say when x becomes greater than a threshold.\n00:07:34 Then differences between these hidden values will just select out regions of space between ci and ci plus 1.", "start_char_idx": 3383, "end_char_idx": 7450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27fe44aa-0e3b-47e7-bf07-0f04f9f0258e": {"__data__": {"id_": "27fe44aa-0e3b-47e7-bf07-0f04f9f0258e", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Neural Networks (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=bH6VnezBZfI", "Link": "https://www.youtube.com/watch?v=bH6VnezBZfI"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba613405-9475-4938-ae68-036aa597a1c3", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Neural Networks (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=bH6VnezBZfI", "Link": "https://www.youtube.com/watch?v=bH6VnezBZfI"}, "hash": "cf1c561d172ba9d35cde96b27b8527a7eba5b471d2e85aa3542f3b6118fbfb6e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee1a19b4-8c93-4840-a75e-2312c9f57e69", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Neural Networks (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=bH6VnezBZfI", "Link": "https://www.youtube.com/watch?v=bH6VnezBZfI"}, "hash": "fd355ec8d7d2e9a23dbe5cd91d21308b37dc413a89d2c415e0ab28e5e27015e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f75a080-1144-4618-b51b-cc74e0b550f1", "node_type": "1", "metadata": {}, "hash": "6593e1bc71601acf44ecb0f07c29a1091f8421662c0923faaccf08b95dd7a0f4", "class_name": "RelatedNodeInfo"}}, "text": "00:06:40 Internet cutting-edge research focuses on so-called deep networks, with many layers.\n00:06:46 Training these deep networks can be very challenging in and of itself.\n00:06:53 We will\n00:07:00 focus on it too much here, but it's a very hot topic right now.\n00:07:08 It's easy enough to show that given enough hidden nodes, even a two-layer network can actually represent any output function arbitrarily closely.\n00:07:17 To see this, you can just think of each hidden node firing or activating, turning on, in a particular region of space.\n00:07:25 So in 1D, for example, say when x becomes greater than a threshold.\n00:07:34 Then differences between these hidden values will just select out regions of space between ci and ci plus 1.\n00:07:42 So for example, I can select out this region as being h1 minus h2.\n00:07:51 To approximate any function, we can just choose weights that will reproduce those values, effectively discretizing the function into those small regions.\n00:08:00 While multilayer perceptrons, I think, is a fairly descriptive term for these learners, they're also commonly called neural networks.\n00:08:07 Again, like with the perceptron, the simple sum of inputs with a nonlinear activation function can be thought of as a pretty trivial model of a neuron.\n00:08:15 When inputs from the previous layer are activated, if you squint really hard, it's a little bit like dendrites, the neurons sensing the activity of neighboring neurons, with the weights of those determining the strength of the connection.\n00:08:22 And when the observed activity gets strong enough, then the node will itself turn on or activate, a bit like a neuron firing.\n00:08:30 Hence, this is an alternative name, neural networks.\n00:08:37 The nonlinear function used at each node is called the activation function.\n00:08:45 And so far, I've been mostly using the same logistic function that we found useful in training a linear classifier before.\n00:08:52 But there are many other possible choices.\n00:09:00 that people use in practice.\n00:09:08 For example, the hyperbolic tangent curve here is also shaped like a sigmoid S, but instead of saturating at 0 and 1 like the logistic, it saturates at minus 1 and plus 1, with the middle crossing being at z equals 0.\n00:09:17 So, this is a bit like the rescale logistic function we discussed with perceptrons.\n00:09:25 A strictly positive function like the standard logistic can be a bit awkward in multilayer models, since if the outlayer's outputs are all positive, then the next layer's transition point being at 0, it can put a lot of pressure on the bias term to compensate, and so this can lead to slow learning.\n00:09:34 So, sometimes hyperbolic tangents are preferred for that reason.\n00:09:42 Gaussian or radial basis responses like this one are sometimes also used.\n00:09:51 Even a simple linear response like this could be used, usually for the output of the regression model, so that the last layer\n00:10:00 looks like a linear regression with the previous layer's features with no non-linearity attached.\n00:10:07 Moreover, different activation functions can actually be used at different layers.\n00:10:15 Usually it's pretty homogeneous, except for the output layer, which might be different.\n00:10:22 But sometimes in deep models, the activation functions might alternate or change from layer to layer.\n00:10:30 We'll focus only on feed-forward models, in which the information and the computation flows from left to right in our drawings.\n00:10:37 From the input, the observed feature values X, through some hidden layers to the output or prediction.\n00:10:45 So, for example, in this two-layer model, the input features X are used to compute the responses of the first linear layer.\n00:10:52 So here we compute a linear response T, which is a weighted linear combination of the input features X, which we can represent as a matrix with one row for each hidden node,\n00:11:00 nonlinearity sig.\n00:11:06 The values at that hidden node then become inputs for the next layer in exactly the same way.\n00:11:13 So, S becomes the linear response of the next node.", "start_char_idx": 6709, "end_char_idx": 10829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f75a080-1144-4618-b51b-cc74e0b550f1": {"__data__": {"id_": "2f75a080-1144-4618-b51b-cc74e0b550f1", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Neural Networks (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=bH6VnezBZfI", "Link": "https://www.youtube.com/watch?v=bH6VnezBZfI"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba613405-9475-4938-ae68-036aa597a1c3", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Neural Networks (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=bH6VnezBZfI", "Link": "https://www.youtube.com/watch?v=bH6VnezBZfI"}, "hash": "cf1c561d172ba9d35cde96b27b8527a7eba5b471d2e85aa3542f3b6118fbfb6e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27fe44aa-0e3b-47e7-bf07-0f04f9f0258e", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Neural Networks (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=bH6VnezBZfI", "Link": "https://www.youtube.com/watch?v=bH6VnezBZfI"}, "hash": "a0dbe4c3411dbad5f15e53230d07aec96def7371382c7b644ae506d41cbb5704", "class_name": "RelatedNodeInfo"}}, "text": "00:10:30 We'll focus only on feed-forward models, in which the information and the computation flows from left to right in our drawings.\n00:10:37 From the input, the observed feature values X, through some hidden layers to the output or prediction.\n00:10:45 So, for example, in this two-layer model, the input features X are used to compute the responses of the first linear layer.\n00:10:52 So here we compute a linear response T, which is a weighted linear combination of the input features X, which we can represent as a matrix with one row for each hidden node,\n00:11:00 nonlinearity sig.\n00:11:06 The values at that hidden node then become inputs for the next layer in exactly the same way.\n00:11:13 So, S becomes the linear response of the next node.\n00:11:20 Passing it through an activation function gives you the output response.\n00:11:26 Notice that at each level, the nodes' responses at that level, so each node in H1, for example, are independent of one another given the previous layer.\n00:11:33 So, it's trivial to compute their values, so the matrix product, W1 times X1, in parallel if we want.\n00:11:39 A contrast to feedforward networks are so-called recurrent neural networks, in which the output of one layer may actually be fed back into some earlier layer, creating a complex system with many self-dependencies.\n00:11:46 Such systems are considerably harder to analyze and to train than feedforward models, and we won't focus on them here.\n00:11:53 One point to mention here, in many of my drawings, we have multiple output nodes.\n00:11:59 So, for example, here there are two.\n00:12:07 In regression, for example, we may want to predict a vector-valued target Y.\n00:12:14 Predicting the vector jointly, like in this neural network, allows the two coordinates predictions to share their parameters, and specifically to share the features that are developed by the earlier layers of the network.\n00:12:22 This parameter sharing can decrease the complexity of the model and reduces the potential for overfitting.\n00:12:29 It's also extremely useful in classification for non-binary targets.\n00:12:37 Multiple output nodes can be used to predict a binary vector instead of a single binary value.\n00:12:44 This can be used for simple multi-class prediction problems by encoding a non-binary class using a 1 of k binary vector, in which the value of Y determines which unique entry of the vector will be 1, and the others are 0.\n00:12:52 So perhaps if Y can take on values 0, 1, 2, and so on, if we observe Y equals 2, we turn on the entry of the vector associated with Y equals 2.\n00:12:59 It can also be useful in so-called structured prediction problems to predict an arbitrary binary vector.\n00:13:10 These are used in systems like image tagging, where an image might be tagged with multiple labels depending on its content, but the prediction of those labels is usually coupled.\n00:13:20 For simplicity, often such multi-output classifiers are actually trained as if they were a nonlinear regression task to predict a binary target vector, in a similar way to our logistic mean squared error method of training a linear classifier.\n00:13:31 A classification decision can then be obtained by, for example, just selecting the most activated output node.\n00:13:41 In the next section, we'll discuss training of neural networks using gradient descent.", "start_char_idx": 10074, "end_char_idx": 13442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10ad2bfa-9f83-44fe-a81d-e2353aa6ae38": {"__data__": {"id_": "10ad2bfa-9f83-44fe-a81d-e2353aa6ae38", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Neural Networks (2) Backpropagation.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6RUwfKNdaV0", "Link": "https://www.youtube.com/watch?v=6RUwfKNdaV0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "847416df-5686-4eaf-8108-2a52e706903d", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Neural Networks (2) Backpropagation.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6RUwfKNdaV0", "Link": "https://www.youtube.com/watch?v=6RUwfKNdaV0"}, "hash": "c8948f66fa28f7d8fd216a059e30844417c050c1a998f2db93ee56c0f3fcab19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfb9428d-3e32-4477-99fe-ca200021d2b1", "node_type": "1", "metadata": {}, "hash": "07be3aa9b3c8200fae360b3e48eff36d129fb475ffaf00ef5d81e6fe510ae1cd", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 In these slides, we'll discuss training neural networks using gradient descent, which goes by the special name of backpropagation in the neural network literature.\n00:00:06 Recall our neural network model.\n00:00:13 We observe some features x in the input layer, which are used to compute the value of a collection of hidden nodes.\n00:00:20 These hidden nodes then become inputs for the next layer, and so on, to a possibly vector-valued output layer y hat.\n00:00:26 We'll measure the error in our prediction via a mean-squared error between the prediction y hat and the observed target vector y.\n00:00:33 We could easily change this if we want.\n00:00:40 Our training problem, then, is to select values for the weights at all layers to minimize the mean-squared error.\n00:00:46 This is an extension of our single-layer perceptron model, in which we used a logistic sigmoid nonlinearity.\n00:00:53 This ensures that our optimization will be smooth and differentiable, allowing us to optimize the weights using either batch or stochastic...\n00:00:59 online gradient descent.\n00:01:05 The backpropagation algorithm simply applies gradient descent to the neural network cascade of weights.\n00:01:10 It's fundamentally simply an application of the chain rule calculus over and over.\n00:01:16 We'll write some notation here.\n00:01:21 The loss function is the sum of squared errors over the output vector, which we'll index with k.\n00:01:27 The output layer is a smooth nonlinearity sigma of a linear response sk.\n00:01:32 So sk is a weighted linear combination of the hidden nodes.\n00:01:38 And finally, the hidden nodes are an activation function of a linear response t, which is a weighted combination of the input features.\n00:01:43 Now we can take the derivative of the loss j with respect to one of the parameters.\n00:01:49 Let's say a weight in the second layer, denoted w2, is equal to w1, where w1 is the input feature, and w2 is the output feature, and w1 is the activation function of w1.\n00:01:54 So we can take the derivative of the loss j with respect to one of the parameters, w1, with respect to one of the parameters, w2, with respect to one of the parameters, w1.\n00:01:59 sub kj, the weight of the second layer between hidden node j and output node k.\n00:02:15 Following the chain rule on the lost j, we get, here's the definition, we get the sum over output nodes, the derivative of y minus y hat squared is just y minus y hat, times the derivative of the interior, which is minus the derivative of y hat, so that's dy hat.\n00:02:30 When we look at the definition of y hat, it's sigma of sk, so we get the slope of sigma at point sk times the derivative of sk, and sk is just linear in the weights, so we get the hidden value hj.\n00:02:45 Notably, so far, this is exactly the same derivation that we had for logistic mean squared error regression, since a single layer, given the values of h, is just a perceptron with a logistic...\n00:03:00 sigmoid on the output.\n00:03:12 Now let's take the derivative with respect to a different weight, say one at the first layer, W1 sub ji, so the weight between input node xi and hidden node hj.\n00:03:24 Following the chain rule, we take the derivative of j, we get a bunch of y minus y hat terms times the derivative of the y hats.\n00:03:36 As before, the derivative of y hat is just the slope sigma prime at value sk times the derivative of sk, but now sk depends on weight the weights in the first layer through the hidden node value h, so we get Wkj times the derivative of hj.\n00:03:48 Finally, hj depends on the weights at the slope\n00:04:00 slope of sigma at point tj times the derivative of the interior, which is just xi.\n00:04:15 We can notice that in the second computation, we use this term, y minus y hat times the slope of sigma at sk, but we already computed this value when we were computing the derivatives of layer 2.", "start_char_idx": 0, "end_char_idx": 3895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfb9428d-3e32-4477-99fe-ca200021d2b1": {"__data__": {"id_": "cfb9428d-3e32-4477-99fe-ca200021d2b1", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Neural Networks (2) Backpropagation.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6RUwfKNdaV0", "Link": "https://www.youtube.com/watch?v=6RUwfKNdaV0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "847416df-5686-4eaf-8108-2a52e706903d", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Neural Networks (2) Backpropagation.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6RUwfKNdaV0", "Link": "https://www.youtube.com/watch?v=6RUwfKNdaV0"}, "hash": "c8948f66fa28f7d8fd216a059e30844417c050c1a998f2db93ee56c0f3fcab19", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10ad2bfa-9f83-44fe-a81d-e2353aa6ae38", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Neural Networks (2) Backpropagation.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6RUwfKNdaV0", "Link": "https://www.youtube.com/watch?v=6RUwfKNdaV0"}, "hash": "90dbfc2bdb14f2d2dfc218c117fb44d8253b9527151013c75ca555132229babb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29793935-f7ab-4e57-bf87-c7be91afdbb1", "node_type": "1", "metadata": {}, "hash": "31cd37fb92ae4af327ceddcae4a78fe65f6c909e400c2bd5cccb8fcc5da209ec", "class_name": "RelatedNodeInfo"}}, "text": "00:03:24 Following the chain rule, we take the derivative of j, we get a bunch of y minus y hat terms times the derivative of the y hats.\n00:03:36 As before, the derivative of y hat is just the slope sigma prime at value sk times the derivative of sk, but now sk depends on weight the weights in the first layer through the hidden node value h, so we get Wkj times the derivative of hj.\n00:03:48 Finally, hj depends on the weights at the slope\n00:04:00 slope of sigma at point tj times the derivative of the interior, which is just xi.\n00:04:15 We can notice that in the second computation, we use this term, y minus y hat times the slope of sigma at sk, but we already computed this value when we were computing the derivatives of layer 2.\n00:04:30 So we can reuse it here, we'll just call it beta at layer 2 sub k, propagating the derivative values backward to the previous layer.\n00:04:45 If we had another, even earlier layer, it would involve terms that included the sum over k of y minus y hat sigma prime sk wk sigma prime of tj, and we could save those values and propagate them backward.\n00:05:00 again to an earlier layer.\n00:05:07 This gives gradient descent its popular name backpropagation.\n00:05:15 In code, this is usually implemented as an easy recursion, as we compute the beta terms and derivatives recursively starting with the last layer and moving backwards to the first.\n00:05:22 So let's suppose there are N3 output nodes, N2 hidden nodes, and N1 input nodes.\n00:05:30 Then the beta values at layer 2, B2, are just Y hat times the derivative of sigma at value S.\n00:05:37 So this is a vector indexed by node K, so it's length N3.\n00:05:45 We can then use this to compute the gradient of the weights at layer 2.\n00:05:52 This is a matrix, so this is an outer product between B2 and H.\n00:06:00 So, it's a matrix of size N3 by N2, which is the same size as the weight matrix of the second layer.\n00:06:07 We can use the betas at layer 2 to also compute the betas at layer 1 as beta 2 times weight 2.\n00:06:15 So, that sums over K and size N3, that's the output node index.\n00:06:22 And then multiplying by the slope of the activation, d sigma, at value T.\n00:06:30 So, that sums over K giving a vector of length N2, and then it's an element-wise product by another vector of length N2.\n00:06:37 So, we get beta being the size of the number of hidden nodes.\n00:06:45 And finally, the gradient of the weights at the earlier layer is then beta 1 times the input features X.\n00:06:52 So, that'll be a matrix that's the same size as the weight.\n00:07:00 matrix, so it's N2 times N1 plus 1.\n00:07:10 Actually, the H matrix here should have also had the constant feature, so that this gradient should also have been N3 by N2 plus 1.\n00:07:20 Overall, this gives us the gradient of the entire system, which is just the gradient of the weights at the second and first layer together.\n00:07:30 Using this gradient, we can do gradient descent or stochastic gradient descent just by repeatedly updating our parameter vectors and recomputing the gradient.\n00:07:40 Just as a simple example of what a neural network regression model might look like, here's a two-layer model trained on some data from the UCI repository, the motorcycle data set.\n00:07:50 So the model consists of one hidden layer with 10 nodes with logistic sigma activation functions, followed by an output layer.\n00:08:00 with a linear activation so that we get a regression.\n00:08:08 After training, the hidden nodes activations are plotted down here at the bottom.\n00:08:17 And you can see that they act to segment the original feature space X into different regions with the hidden nodes effectively acting like step function detectors.", "start_char_idx": 3155, "end_char_idx": 6870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29793935-f7ab-4e57-bf87-c7be91afdbb1": {"__data__": {"id_": "29793935-f7ab-4e57-bf87-c7be91afdbb1", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Neural Networks (2) Backpropagation.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6RUwfKNdaV0", "Link": "https://www.youtube.com/watch?v=6RUwfKNdaV0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "847416df-5686-4eaf-8108-2a52e706903d", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Neural Networks (2) Backpropagation.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6RUwfKNdaV0", "Link": "https://www.youtube.com/watch?v=6RUwfKNdaV0"}, "hash": "c8948f66fa28f7d8fd216a059e30844417c050c1a998f2db93ee56c0f3fcab19", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfb9428d-3e32-4477-99fe-ca200021d2b1", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Neural Networks (2) Backpropagation.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6RUwfKNdaV0", "Link": "https://www.youtube.com/watch?v=6RUwfKNdaV0"}, "hash": "259d2f83e349a69b3b63a3682cdb90d952280a437f21c2f6700a25bbc6a29895", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a601ba43-885c-4927-b04f-68311704d0c4", "node_type": "1", "metadata": {}, "hash": "fad028cded034ddee26857452a22a77e7abe859063b31abd0397b0907773c47b", "class_name": "RelatedNodeInfo"}}, "text": "00:07:30 Using this gradient, we can do gradient descent or stochastic gradient descent just by repeatedly updating our parameter vectors and recomputing the gradient.\n00:07:40 Just as a simple example of what a neural network regression model might look like, here's a two-layer model trained on some data from the UCI repository, the motorcycle data set.\n00:07:50 So the model consists of one hidden layer with 10 nodes with logistic sigma activation functions, followed by an output layer.\n00:08:00 with a linear activation so that we get a regression.\n00:08:08 After training, the hidden nodes activations are plotted down here at the bottom.\n00:08:17 And you can see that they act to segment the original feature space X into different regions with the hidden nodes effectively acting like step function detectors.\n00:08:25 So then, a linear combination of these hidden node values can produce this output function, which we can see is hiding nonlinear in the original feature space X.\n00:08:34 Here's another simple example that shows the actual optimization process during gradient descent for a classifier using two features of the Iris dataset.\n00:08:42 So we're optimizing the mean squared error of an output vector, which is a 1 of k representation of the three classes, using stochastic gradient descent.\n00:08:51 And the values of the optimization, the mean squared error...\n00:08:59 are shown in blue.\n00:09:05 The gradient descent takes steps changing theta, which decreases that overall MSE with every step.\n00:09:11 At the same time, we can take the output and threshold it and use that as a classification decision and evaluate the error rate of that classification decision.\n00:09:17 So, this is shown in green.\n00:09:23 You can see that these aren't perfectly related, right?\n00:09:29 The mean squared error is acting as a surrogate for the harder-to-optimize classification loss.\n00:09:35 But typically, improving the MSE of the prediction also improves the error rate of the classification.\n00:09:41 So, the trained classification boundaries are shown over here.\n00:09:47 You can see they're somewhat nonlinear versions of the original features.\n00:09:53 In practice, neural networks provide complex, highly flexible input-output behavior for a variety of classification tasks.\n00:09:59 by Professor Jeff Hinton of a Deep Belief Network.\n00:10:09 More precisely, I think this demo is actually a Deep Boltzmann machine, which he's made available as an online demonstration.\n00:10:19 The task here is to classify handwritten digits from a U.S. Postal Service data set, USPS, identifying which of the 10 digits is written in each patch of image.\n00:10:29 So the input features X are a 16 by 16 pixel image, so 784 pixels, and the network has four layers.\n00:10:39 So shown are the response values, the post activation function values of the hidden layer 1, the second hidden layer, the third hidden layer, and the output layer.\n00:10:49 The first hidden layer is 500 nodes, the second one is the same size, the third one is larger at 2,000 nodes, and then\n00:10:59 And the output layer is using a 1 of k representation for our 10 digits, so it's a vector of length 10.\n00:11:09 So in this input, we input a handwritten 2 as an image, compute the activation function at the first, then second, then third layers, then the output layer, and choose the largest response as our decision.\n00:11:19 Here it outputs, predicts 0, 0, 1, 0, 0, or which is a 2, since the 1 is in the position of the 2 digit.\n00:11:29 If we input a different image patch, we can recompute the output at each layers and the output.\n00:11:39 Here we input a 6, and after passing it through the network, the prediction correctly predicts a 1 in the 6 position.\n00:11:49 Interestingly, the actual model being used here is capable...\n00:11:59 of being interpreted as a probability distribution as well.\n00:12:09 So the demo program actually will allow you to fix an output configuration, say 0, and then do a process which approximately samples from the values of the rest of the system.", "start_char_idx": 6051, "end_char_idx": 10124, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a601ba43-885c-4927-b04f-68311704d0c4": {"__data__": {"id_": "a601ba43-885c-4927-b04f-68311704d0c4", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Neural Networks (2) Backpropagation.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6RUwfKNdaV0", "Link": "https://www.youtube.com/watch?v=6RUwfKNdaV0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "847416df-5686-4eaf-8108-2a52e706903d", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Neural Networks (2) Backpropagation.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6RUwfKNdaV0", "Link": "https://www.youtube.com/watch?v=6RUwfKNdaV0"}, "hash": "c8948f66fa28f7d8fd216a059e30844417c050c1a998f2db93ee56c0f3fcab19", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29793935-f7ab-4e57-bf87-c7be91afdbb1", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Neural Networks (2) Backpropagation.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6RUwfKNdaV0", "Link": "https://www.youtube.com/watch?v=6RUwfKNdaV0"}, "hash": "c326676b1f724c38c49281103c2a6c9eda5965056fcad4ff83400a0ef0d5c1dd", "class_name": "RelatedNodeInfo"}}, "text": "00:11:19 Here it outputs, predicts 0, 0, 1, 0, 0, or which is a 2, since the 1 is in the position of the 2 digit.\n00:11:29 If we input a different image patch, we can recompute the output at each layers and the output.\n00:11:39 Here we input a 6, and after passing it through the network, the prediction correctly predicts a 1 in the 6 position.\n00:11:49 Interestingly, the actual model being used here is capable...\n00:11:59 of being interpreted as a probability distribution as well.\n00:12:09 So the demo program actually will allow you to fix an output configuration, say 0, and then do a process which approximately samples from the values of the rest of the system.\n00:12:19 Here that lets you draw a sample from the input vector which the model thinks might have generated this output.\n00:12:29 And here it's drawn an awkward but recognizable synthetic 0, which suggests that the resulting probability distribution defined by this model does have some idea of what makes, say, a 0 in appearance different from other possible values.\n00:12:39 So this particular variety of neural network, Boltzmann machine, is actually interpretable as a generative model over the features as well.\n00:12:49 If you're interested in neural networks, there's quite a lot more to learn and do than we cover in this class.\n00:12:59 This is a very active area of research these days.\n00:13:06 To try things out, I would probably suggest the Deep Learning Toolbox, which is available on GitHub as open source code.\n00:13:13 It's written for MATLAB and Octave, and it's a good starting place for exploring and testing out some of the methods that are popular right now.\n00:13:19 There are also other open source toolkits available.\n00:13:26 Notably, there's one that's supplied with MATLAB, but it requires the purchase of the Neural Network Toolbox and your license.\n00:13:33 Or there's an older code base called NetLab, which was widely used at one point, but hasn't been updated for a while.\n00:13:39 In summary, neural networks, or multilayer perceptrons, provide an extremely flexible class of functions.\n00:13:46 They're built up from a cascade of several layers, each of which is a collection of simple linear responses with a nonlinear activation function, essentially a perceptron model.\n00:13:53 The hidden layers of the model can be interpreted as designing good features.\n00:13:59 the original output that can then be used by the output layer perceptron classifier.\n00:14:08 When more than one layer is used, the resulting neural network becomes a general function approximator.\n00:14:16 In other words, it's capable of approximating any function given enough nodes, or in other words, tunable features.\n00:14:24 We can use this functional framework either for classification or for regression.\n00:14:32 Because each layer is smooth and differentiable, we can train the overall system using gradient descent methods, which we derived using the chain rule.\n00:14:40 Furthermore, because this computation can be ordered from last layer to first, reusing some of the computation at the previous layer, gradient descent in neural network models is usually called backpropagation.", "start_char_idx": 9454, "end_char_idx": 12623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91d648f3-73ff-4350-9ad8-6b56bc5e4913": {"__data__": {"id_": "91d648f3-73ff-4350-9ad8-6b56bc5e4913", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Yvn3--rIdZg", "Link": "https://www.youtube.com/watch?v=Yvn3--rIdZg"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1b1131c-be94-4192-bc82-af8e247fa58c", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Yvn3--rIdZg", "Link": "https://www.youtube.com/watch?v=Yvn3--rIdZg"}, "hash": "a5a4f55768f626086f44a0dc6476c3a6a802aa59faa5897eb871db98b512dc77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65206850-f9ff-4992-af30-2965d3f30b20", "node_type": "1", "metadata": {}, "hash": "f79285206953fc73a3cbd013a86185372fb296f3dff1eeeddf889bc8fd673923", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Ensembl methods in machine learning are a kind of meta-algorithm that makes use of other machine learning algorithms as a component to learn a collection of predictors.\n00:00:08 Ensembl methods take the attitude that more predictors can be better than any single one, and by learning many we can do a better job.\n00:00:17 An Ensembl combines many predictors, often through some weighted average or weighted combination, and these might be of the same kind of learner or of different types.\n00:00:25 So Ensembls might be used to combine many different kinds of predictors, like a neural network, a support vector machine, and the decision tree, if we didn't know what the best kind of predictor or what type of predictor would be best for this problem.\n00:00:34 They can also be used to compute collections of the same type of learner.\n00:00:42 We'll see several examples, like bagging and boosting, that use a single kind of learner, but many instances to try to do a better job.\n00:00:51 One view of Ensembl learning is related to a psychological phenomenon\n00:00:59 known as the wisdom of the crowds.\n00:01:10 Most of us have probably seen the show Who Wants to Be a Millionaire, where a series of contestants are asked multiple choice questions testing their knowledge of trivia.\n00:01:20 In that show, there are several ways of getting assistance, one of which is phoning a predetermined friend who is presumably someone very good or very smart, someone who knows a lot of trivia, and the other where you poll the audience, which is asking a collection of 100 people who had nothing better to do than show up to the game show.\n00:01:30 Yet, if you actually watch the show, you'll see a strange phenomenon.\n00:01:40 The phone a friend is often wrong, but asking the audience is almost never wrong.\n00:01:50 Somehow, a collection of 100 non-experts turns out to be a far better predictor of the correct answer than the prediction of the one predetermined expert.\n00:02:00 that the contestant was able to select.\n00:02:05 Fundamentally, this is what ensembles try to do.\n00:02:10 They take a collection of OK predictors and try to combine them into something far more powerful.\n00:02:16 The simplest kind of ensemble is simply a committee or unweighted average.\n00:02:21 If we're doing regression, we'd do something like an unweighted average of the predictions.\n00:02:27 If we're doing classification, we might do a majority vote.\n00:02:32 This is probably the simplest way combining a collection of predictions.\n00:02:38 More generally, you could think about using weighted averages.\n00:02:43 For example, we might believe that some of our predictors are better or more accurate than others, and we'd like to give them higher weight than the lower quality performances.\n00:02:49 So a simple example of this, suppose we had a collection of predictors F1, F2, F3, and so on, and they produced predictions Y1, Y2, Y hat 1, Y hat 2, and so on.\n00:02:54 We might try to combine these by giving them each some.\n00:03:00 weight, alpha i, and if the classes are, say, plus and minus 1, we could just add up the weighted sum, weighted signed sum of our predictions.\n00:03:12 And if a greater weight preferred plus 1, this would be positive, and if a greater weight preferred minus 1, it would be negative, so we'd take the sign of that to be our prediction.\n00:03:24 Even more generally, we could take these predictions as being features of some other predictor.\n00:03:36 So we simply learn our predictors, F1 through Fk, as best we can, and then we could just combine them using some entirely new predictor, FeF ensemble, that takes the output of those initial predictors and tries to use them to produce a better predictor, y hat.", "start_char_idx": 0, "end_char_idx": 3735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65206850-f9ff-4992-af30-2965d3f30b20": {"__data__": {"id_": "65206850-f9ff-4992-af30-2965d3f30b20", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Yvn3--rIdZg", "Link": "https://www.youtube.com/watch?v=Yvn3--rIdZg"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1b1131c-be94-4192-bc82-af8e247fa58c", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Yvn3--rIdZg", "Link": "https://www.youtube.com/watch?v=Yvn3--rIdZg"}, "hash": "a5a4f55768f626086f44a0dc6476c3a6a802aa59faa5897eb871db98b512dc77", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91d648f3-73ff-4350-9ad8-6b56bc5e4913", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Ensembles (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Yvn3--rIdZg", "Link": "https://www.youtube.com/watch?v=Yvn3--rIdZg"}, "hash": "4552d96b0878af7854ab3f2d7b36b21cff8bc96f1fe0419f7e5712b7e70194b3", "class_name": "RelatedNodeInfo"}}, "text": "00:02:54 We might try to combine these by giving them each some.\n00:03:00 weight, alpha i, and if the classes are, say, plus and minus 1, we could just add up the weighted sum, weighted signed sum of our predictions.\n00:03:12 And if a greater weight preferred plus 1, this would be positive, and if a greater weight preferred minus 1, it would be negative, so we'd take the sign of that to be our prediction.\n00:03:24 Even more generally, we could take these predictions as being features of some other predictor.\n00:03:36 So we simply learn our predictors, F1 through Fk, as best we can, and then we could just combine them using some entirely new predictor, FeF ensemble, that takes the output of those initial predictors and tries to use them to produce a better predictor, y hat.\n00:03:48 So this is quite similar to the idea in multilayer perceptrons, where we have one layer of predictors, let's say perceptrons, F1 through Fk, producing features for a...\n00:04:00 another layer of prediction, except that unlike multi-layer perceptrons where all the layers are going to be trained simultaneously, here we're going to just train the individual predictors once and then we'll try to combine them using a post-processing prediction step.\n00:04:15 In the case that the Y-hats are binary, like plus minus one, and our ensemble predictor Fe is linear, this will arrive at a weighted vote, but it would give you perhaps a way to train the weights of that weighted vote in a principled way.\n00:04:30 I should mention that if you are planning on training the ensemble weights or the ensemble predictor Fe, you should probably either use validation data or some separated out data, since if F1 through Fk have used the same data as Fe, Fe will simply choose the one that overfits the data the most.\n00:04:45 So for instance, if one of the FIs is...\n00:05:00 nearest neighbor predictor, it'll get all of the training data correct, and then FE will decide to trust it entirely if it's trained on the same data.\n00:05:08 So, it should be trained on some validation data.\n00:05:17 Even more generally, the concept of mixtures of experts produces ensembles of predictors or ensembles of models that make the weights perhaps depend on X.\n00:05:25 So, mixture of experts have some weight, alpha i, that might be a function of the feature X, and that weight indicates their expertise.\n00:05:34 And depending on the work, some mixtures of experts use a weighted average, some just use the majority, the largest value as the most confident expert.\n00:05:42 And what this will do is provide a model that is a mixture of other models.\n00:05:51 So, for instance, here we have some training data, and we've fit a model consisting of three linear predictors.\n00:06:00 Each of which is capable of doing a good job of predicting, but only on some narrow regime of x.\n00:06:10 So here we would learn perhaps a function alpha i of x that would tell us which region we're in, and which of our experts we should use, and learn the experts to fit that region.\n00:06:21 So these models are usually trained in a simultaneous way, where we're simultaneously trying to figure out what region the model should be an expert in, and also what the job of that expert should be.\n00:06:31 So this will connect to mixture models when we see that in unsupervised learning.\n00:06:42 For the near future, though, we'll concentrate mostly on simple weights, simple scalar weights on the experts, and weighted averages or weighted votes.", "start_char_idx": 2952, "end_char_idx": 6448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6abb9c2b-add7-466a-b907-b02bba4487e3": {"__data__": {"id_": "6abb9c2b-add7-466a-b907-b02bba4487e3", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1376bd02-1d92-4d41-b9d9-92483544b484", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "hash": "18d41d7cfb94709b55e99d34521889c09fdb6b68f847652a0caaebd57c0cca76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "636b11df-74cf-4d00-bbfb-25b4e7eac215", "node_type": "1", "metadata": {}, "hash": "ff05948fa9346d47759bc351296f5bad5f691e30229dd30b7af40dc8e7df6f1e", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Our first ensemble technique is called bagging, and is a classic technique for generating lots of predictors and combining them together in a simple way to do a better job.\n00:00:07 Ensemble methods try to use a collection of predictors to do a better job than any single predictor would alone.\n00:00:15 Our first instance of this is called bagging, which stands for bootstrap aggregation.\n00:00:22 Bagging is a technique for learning many classifiers, each using only portions of the data, and then combining them through a model averaging technique.\n00:00:30 The idea behind this is to reduce overfitting of a class of models.\n00:00:37 So recall that when we overfit, we would memorize the data set, and we would get a far lower training error on that training data set than we would see on, say, a new validation or test data.\n00:00:45 So we did techniques like cross-validation to try to test or check to see whether we had overfit.\n00:00:52 In cross-validation, we would take our data set\n00:01:00 and we would repeatedly split it up into one part, which would train a model with a given complexity level, and another part which would check to see whether we had overfit.\n00:01:12 So this was a way to use the entire training set, but still do some kind of verification of whether or not we were beginning to overfit on the data.\n00:01:24 So the idea behind bagging is to do a similar kind of data splitting or resampling technique, but instead of using them to check to see whether we overfit, we instead try to combine them so that we produce a better classifier or predictor.\n00:01:36 Bagging relies on a classical statistical technique called the bootstrap, which creates a random new subset of data by sampling from a given data set.\n00:01:48 So the idea is that we have a particular data set, and we'd like to generate a similar data set by sampling from it with replacement.\n00:02:00 confidence values, or confidence intervals of estimates, and understand what the variation due to the particular realization of the data set is.\n00:02:10 To do it, we take a collection of N data points, and we can generate a new imaginary data set of size N prime by simply drawing those N prime data points from our collection using replacement.\n00:02:20 There are variants of this that use without replacement, but we won't really focus on those.\n00:02:30 Bagging, then, uses Bootstrap as a subroutine of its learners.\n00:02:40 So it creates K learners, each of which is created independently.\n00:02:50 We take the original training set, and each learner generates its own training data set, so data set sub I, by drawing N prime data points from the original full training set using replacement.\n00:03:00 a classifier on these data.\n00:03:07 Now these data will were drawn with replacement so some of them will be repeated and other data points will be left out.\n00:03:15 We just learn all of these in parallel and then to test we predict on all K of the predictors and we have them do an unweighted combination.\n00:03:22 So a classifier would do a majority vote and a regressor would do say an unweighted average.\n00:03:30 So this technique is used to reduce the complexity or reduce overfitting and you can see why it might reduce complexity to some degree.\n00:03:37 It has some complexity control.\n00:03:45 It's harder for any particular bag classifier to memorize the data since each one is seeing its own individually generated training data set and that training data set was sampled from the original one so it's left some points out.\n00:03:52 So those points won't be seen by that particular classifier and even if it really is a\n00:04:00 overfitting type of classifier, even if it memorizes its training set, it will be missing some data from the original training data, and so it won't be able to memorize the full training set.\n00:04:10 So it should control complexity to some degree because every element of it will be unable to fully memorize the data.\n00:04:20 So I should say that this doesn't work on predictors that are linear functions of the input data, since the average of a linear function will just be another linear function, so it doesn't really do very much.", "start_char_idx": 0, "end_char_idx": 4192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "636b11df-74cf-4d00-bbfb-25b4e7eac215": {"__data__": {"id_": "636b11df-74cf-4d00-bbfb-25b4e7eac215", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1376bd02-1d92-4d41-b9d9-92483544b484", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "hash": "18d41d7cfb94709b55e99d34521889c09fdb6b68f847652a0caaebd57c0cca76", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6abb9c2b-add7-466a-b907-b02bba4487e3", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "hash": "459b838ce3f15494a019eb5295e2fb71c925aa259242c90bd448e6b6daea9ced", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b56a6812-1b91-4a46-9acd-2c3f7f90ff33", "node_type": "1", "metadata": {}, "hash": "f4e78a29e5d9f1e2c3f7185e58686fd1bdfe13b3fcb852c5d3d0fe8e93551aeb", "class_name": "RelatedNodeInfo"}}, "text": "00:03:45 It's harder for any particular bag classifier to memorize the data since each one is seeing its own individually generated training data set and that training data set was sampled from the original one so it's left some points out.\n00:03:52 So those points won't be seen by that particular classifier and even if it really is a\n00:04:00 overfitting type of classifier, even if it memorizes its training set, it will be missing some data from the original training data, and so it won't be able to memorize the full training set.\n00:04:10 So it should control complexity to some degree because every element of it will be unable to fully memorize the data.\n00:04:20 So I should say that this doesn't work on predictors that are linear functions of the input data, since the average of a linear function will just be another linear function, so it doesn't really do very much.\n00:04:30 It does work with anything that has non-linearity, so things like perceptrons, where we have a linear function but it's thresholded, so the actual prediction is not a linear function of the inputs, it's fine.\n00:04:40 Bagging is fundamentally trading off on the bias-variance balance that we find in under and overfitting.\n00:04:50 The idea behind bias and variance was that...\n00:05:00 The two effects that we were worried about in our training were, one, bias, which represents our inability to represent the true function in the class of functions that we're willing to tolerate.\n00:05:10 So bias tends to be something that pulls us toward a particular function or class of functions, regardless of the data.\n00:05:20 And this might be something simple like only looking at a certain class of functions like linear or quadratic functions, or it might be something softer like a strong regularization, which will force us toward models that tend to have zeros in the parameters or have small parameter values.\n00:05:30 On the other hand, when we didn't invoke any such bias, we ended up with models that were quite complex.\n00:05:40 They could fit very complex functions, but they had high variance.\n00:05:50 So they were extremely dependent on the exact data that they\n00:06:00 were trained on.\n00:06:08 So, in particular, if we had, let's say, a hundred training examples and we trained one of these very complex models, it would be able to fit whatever function explained those data, but it would be quite likely that if we drew a hundred new examples from the same concept, that the model we trained on the new data would be very different than the first model.\n00:06:17 And so, any particular realization of the data set produces a high degree of variation in the predictor.\n00:06:25 So, the model itself has high variance.\n00:06:34 Bagging works on the high-variance end of the spectrum by taking a collection of low-bias, high-variance models and averaging them together.\n00:06:42 So, the averaging works to reduce variance, but without significantly increasing the bias.\n00:06:51 So, it simply tries to move this curve downward.\n00:06:59 Here's an example of bootstrap and of bagging using decision trees on the iris data.\n00:07:11 So here, up in the upper right, is the actual iris data with three classes, blue, green, and red, being plotted, and a decision tree with full depth and no complexity control and the function that it learns on those data being plotted as well.\n00:07:23 So the bootstrap procedure takes these data and it draws a new set of data, in this case of the same size, using replacement to try to simulate some equally likely data set that we could have seen in a different universe.\n00:07:35 So here's that data set.\n00:07:47 It's on a slightly different scale, so it's a little hard to compare, but you can see if you stare that a few data points are missing, in particular, this red.\n00:07:59 point here has vanished from this data set.\n00:08:06 It's particularly hard to tell.\n00:08:13 Oh, and this red point here also has vanished.\n00:08:19 It's particularly hard to tell data that have been sampled twice since those points are right on top of each other, but they're present as well.", "start_char_idx": 3309, "end_char_idx": 7425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b56a6812-1b91-4a46-9acd-2c3f7f90ff33": {"__data__": {"id_": "b56a6812-1b91-4a46-9acd-2c3f7f90ff33", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1376bd02-1d92-4d41-b9d9-92483544b484", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "hash": "18d41d7cfb94709b55e99d34521889c09fdb6b68f847652a0caaebd57c0cca76", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "636b11df-74cf-4d00-bbfb-25b4e7eac215", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "hash": "5456baf5b654c9bfea09afc39988ecc111dbe3b55c64b5ea304681c573ba7211", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cc89a71-8e1c-44fd-89a1-ce324ef9e448", "node_type": "1", "metadata": {}, "hash": "6a304482228b074837af2fba5770dcd55b3869e84d7c5adfcb5aba50774a878f", "class_name": "RelatedNodeInfo"}}, "text": "00:07:23 So the bootstrap procedure takes these data and it draws a new set of data, in this case of the same size, using replacement to try to simulate some equally likely data set that we could have seen in a different universe.\n00:07:35 So here's that data set.\n00:07:47 It's on a slightly different scale, so it's a little hard to compare, but you can see if you stare that a few data points are missing, in particular, this red.\n00:07:59 point here has vanished from this data set.\n00:08:06 It's particularly hard to tell.\n00:08:13 Oh, and this red point here also has vanished.\n00:08:19 It's particularly hard to tell data that have been sampled twice since those points are right on top of each other, but they're present as well.\n00:08:26 So this is a new data set of the same size simulated from the original data set, and being slightly different, it learns a slightly different decision tree model.\n00:08:33 Here's yet another data set sampled in exactly the same way.\n00:08:39 Again, being slightly different, it learns a slightly different decision tree model.\n00:08:46 Here's a third, again slightly different, a fourth, and a fifth.\n00:08:53 The randomness created in the bootstrap data generation process produces five equally likely models.\n00:08:59 in some sense that we could have gotten from the same data had we just been, say, differently lucky.\n00:09:06 So now we take that collection and combine it into an overall average with an unweighted sum.\n00:09:13 So in this case, we're classifying, so I mean we take a majority vote.\n00:09:19 So if the majority of classifiers picks a particular class, we predict that one.\n00:09:26 This reduces the memorization effect, since not every predictor can see each data point.\n00:09:33 In particular, a region will only be predicted to a class if it has enough support among all of the data points that in most of the decision trees we learn, that region is still assigned to the class.\n00:09:39 So this will lower the complexity of the overall predictor and typically lead us to better generalization performance than the single predictor we learned at first.\n00:09:46 So here's an example where we've learned the five trees shown on the previous slide and averaged them together.\n00:09:53 And you can see already region.\n00:09:59 regions like this that were assigned to green by the influence of a single data point have now been overwhelmed by some of the other predictors that did not perhaps have that data point in it.\n00:10:11 By 25 trees, we're starting to see what I think a human would start to assign.\n00:10:23 So blue is taking a more oblong region in the upper left, red in the right-hand side and green in the lower center, and we're starting to see that shape emerge from our collection of trees.\n00:10:35 By 100 trees, that shape is generally prevalent, and many of the artifacts that we saw in the single decision tree have vanished, so that now we find that green has a relatively sensible looking support in the center, and blue and red have also taken on fairly sensible looking supports as well.\n00:10:47 Practically speaking, bagging is an easy procedure.\n00:10:59 to implement.\n00:11:08 So here's MATLAB code just showing how easy.\n00:11:17 The first thing we do is we create a container for our set of classifiers, in this case a cell array.\n00:11:25 Cell arrays in MATLAB can represent collections of arbitrary types of objects, although in this case we'll probably just be using the same type of classifier.\n00:11:34 We then iterate over the number of classifiers we'd like to learn and we do a procedure to generate our bootstrap data sample.\n00:11:42 So if we'd like to generate unused data points valued between 1 and n with replacement, we just generate unused random numbers between 1 and n, and so we can do that just by uniformly generating random numbers, multiplying them by n, and then rounding them up using the ceiling function.\n00:11:51 We select out those data points that'll contain several values that are the same, so we'll get some repetition in our data just by extracting data.", "start_char_idx": 6688, "end_char_idx": 10773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cc89a71-8e1c-44fd-89a1-ce324ef9e448": {"__data__": {"id_": "3cc89a71-8e1c-44fd-89a1-ce324ef9e448", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1376bd02-1d92-4d41-b9d9-92483544b484", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "hash": "18d41d7cfb94709b55e99d34521889c09fdb6b68f847652a0caaebd57c0cca76", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b56a6812-1b91-4a46-9acd-2c3f7f90ff33", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "hash": "9885e1f5b6de0b2805e77314660a6cd418d2f8ee66948733b2bc5b17268eba88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b85dfca6-5620-4e73-baa9-fbc07b6d888f", "node_type": "1", "metadata": {}, "hash": "2229aedca1bbc2914d01d7767fcfa7bb3141756393d6b9c7c4ae5210df048c57", "class_name": "RelatedNodeInfo"}}, "text": "00:11:17 The first thing we do is we create a container for our set of classifiers, in this case a cell array.\n00:11:25 Cell arrays in MATLAB can represent collections of arbitrary types of objects, although in this case we'll probably just be using the same type of classifier.\n00:11:34 We then iterate over the number of classifiers we'd like to learn and we do a procedure to generate our bootstrap data sample.\n00:11:42 So if we'd like to generate unused data points valued between 1 and n with replacement, we just generate unused random numbers between 1 and n, and so we can do that just by uniformly generating random numbers, multiplying them by n, and then rounding them up using the ceiling function.\n00:11:51 We select out those data points that'll contain several values that are the same, so we'll get some repetition in our data just by extracting data.\n00:11:59 data set I to be X of those indices, and we extract out the targets as well.\n00:12:08 And then we call our classifier function, so this is some black box function that trains a classifier on the new bootstrap data set XI with targets YI.\n00:12:17 We store the result in our set of classifiers.\n00:12:25 When we go to test, we just create a collection of predictions, so we need the number of data that we're testing on times the number of classifiers that we've trained.\n00:12:34 We predict on that vector of test data for each classifier in turn, and then at the end we do a procedure to do a majority vote, so this is a simple one-line majority vote between outputs one and two.\n00:12:42 If you have a multiclass classifier, you need to do a slightly more complex procedure.\n00:12:51 A particularly common application of bagging is the use of them using decision trees, and an algorithm called random porous.\n00:12:59 So one problem that arises when you apply bagging to decision trees is that if you have a large enough amount of data, often the decision trees that you learn aren't very different.\n00:13:08 So if you have enough data, for example, the root node, even if you resample those data with replacement, will look generally the same.\n00:13:17 And so the first variable selected, the first feature, and the first split point will be fairly similar.\n00:13:25 And so the class of trees that you learn in your bagging procedure is not really different enough.\n00:13:34 It's not really diverse enough to help.\n00:13:42 The solution is to introduce some extra variability in the learner and inject some extra randomness into this procedure.\n00:13:51 And so the way random forests do that is at every step of the decision tree training process, instead of searching over all features, we just search over some subset of features.\n00:13:59 So, this is going to create diversity among our trees, because at any particular node, in one tree versus another, the same set of features will not be available.\n00:14:09 And so, in one tree, the best feature among its allowed set will be different than the best feature among a different tree's allowed set.\n00:14:19 That'll produce a larger variability in our collection of trees, and then we just average over them in the same way.\n00:14:29 This is, again, extremely simple to implement, so in our decision tree recursive data splitting function, instead of searching over every possible features in every possible split, we first generate a random subset of the features of some size, and search only over that set and all splits.\n00:14:39 Otherwise, the procedure works exactly the same way as before.\n00:14:49 In summary, bagging is a classic ensemble technique for...\n00:14:59 better predictors than any single predictor.\n00:15:07 It stands for bootstrap aggregation and it's a technique that tries to reduce the complexity of a model class.\n00:15:14 So you choose a model class that's very prone to overfitting and apply bagging to provide a collection of learners in that class that are less complex and less prone to overfitting.\n00:15:22 In practice, it's quite simple to implement.\n00:15:29 We just re-sample the data once for each learner.", "start_char_idx": 9905, "end_char_idx": 13981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b85dfca6-5620-4e73-baa9-fbc07b6d888f": {"__data__": {"id_": "b85dfca6-5620-4e73-baa9-fbc07b6d888f", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1376bd02-1d92-4d41-b9d9-92483544b484", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "hash": "18d41d7cfb94709b55e99d34521889c09fdb6b68f847652a0caaebd57c0cca76", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cc89a71-8e1c-44fd-89a1-ce324ef9e448", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}, "hash": "f31be2da83a2c0a75172f8a2a3536bde01d5f42c2e6cadf916ef50f3c9f22602", "class_name": "RelatedNodeInfo"}}, "text": "00:14:39 Otherwise, the procedure works exactly the same way as before.\n00:14:49 In summary, bagging is a classic ensemble technique for...\n00:14:59 better predictors than any single predictor.\n00:15:07 It stands for bootstrap aggregation and it's a technique that tries to reduce the complexity of a model class.\n00:15:14 So you choose a model class that's very prone to overfitting and apply bagging to provide a collection of learners in that class that are less complex and less prone to overfitting.\n00:15:22 In practice, it's quite simple to implement.\n00:15:29 We just re-sample the data once for each learner.\n00:15:37 Each learner is trained on an individual re-sampling, and we create a predictor that might overfit on that sample, but then by averaging them together produces something that's robust to small variations in the data.\n00:15:44 It essentially plays on the bias-variance tradeoff, choosing something that's prone to overfitting and thus has low bias, but reducing its variance through model averaging.\n00:15:52 The price of this, of course, is the computational cost, which if we learn K bagged predictors.\n00:15:59 our prediction time computation becomes k times larger than it would have been before.", "start_char_idx": 13364, "end_char_idx": 14590, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8091657c-ac4d-4c16-818f-84580dab4ebd": {"__data__": {"id_": "8091657c-ac4d-4c16-818f-84580dab4ebd", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (3) Gradient Boosting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sRktKszFmSk", "Link": "https://www.youtube.com/watch?v=sRktKszFmSk"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcf19e6f-6bf6-4a72-b5f3-42ec9228d639", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (3) Gradient Boosting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sRktKszFmSk", "Link": "https://www.youtube.com/watch?v=sRktKszFmSk"}, "hash": "e0f2153cf30e8d0c752b1c9b9a219b05894ab5e0e1a6e4dcfe1b19aac3356c3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67f3c380-8ea5-42bf-8248-8c0982dd0211", "node_type": "1", "metadata": {}, "hash": "36018306b259afc96ec28d082f41e5cc9bc3c6e3c1a1e4073c465384a8fe7577", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Boosting is another ensemble technique for creating collections of predictors, and gradient boosting is a technique for producing regression models consisting of collections of regressors.\n00:00:12 An ensemble is a collection of predictors whose predictions are combined, usually by some sort of weighted average or vote, in order to provide an overall prediction that takes its guidance from the collection itself.\n00:00:24 Boosting is an ensemble technique in which learners are learned sequentially, with early learners fitting simple models to the data, and then analyzing the data for errors.\n00:00:36 Those errors identify problems or particular instances of the data that are the difficult or hard-to-fit examples, and later models focus primarily on those examples, trying to get them right.\n00:00:48 In the end, all the models are given weight, and the set is combined into some\n00:01:00 overall predictors.\n00:01:05 So boosting is a method of converting a sequence of weak learners into a very complex predictor.\n00:01:10 It's a way of increasing the complexity of a particular model.\n00:01:15 Initial learners tend to be very simple and then the weighted combination can grow more and more complex as learners are added.\n00:01:20 Gradient boosting is an instantiation of this idea for regression.\n00:01:25 The idea is to repeatedly follow this procedure.\n00:01:30 We learn a simple regression predictor of our data.\n00:01:35 Then we compute the error residual, the amount the error per data point in our predictions.\n00:01:40 And then we learn a new model to try to predict this error residual.\n00:01:45 So here's an example with red data on the left indicating the motorcycle data set.\n00:01:50 So it has a fairly complex shape with regard to the feature time.\n00:01:55 And here we fit an extreme\n00:02:00 simple predictor.\n00:02:06 So this is a one-layer decision tree used for regression.\n00:02:13 So it partitions the space into two parts, thresholding at some particular value, predicting a constant value on the left and a constant value on the right.\n00:02:20 Obviously this model is too simple to fit the shape of the data, and so there are a lot of errors.\n00:02:26 We make errors over here where the prediction is too low, errors here where the prediction is too high, and so forth.\n00:02:33 And if we compute that error on a data-by-data basis, the error residual, it becomes a function plotted over here in the green on the right.\n00:02:39 Then that function can further be predicted by the model, by a new model.\n00:02:46 Again fitting a model from our simple class of predictors, we get a new prediction of these error residuals.\n00:02:53 So here we've predicted a region where the error residuals were quite high and fairly constant, and then partitioned into\n00:02:59 two parts, and on the right-hand region we've predicted a slightly negative value, producing what becomes the best one-layer decision tree regressor for the error residuals.\n00:03:11 Adding these two predictors together gives a new function, which is slightly more complex than a single-layer decision tree.\n00:03:23 So here in the far left-hand side, we see that the combination of the two predictors has done a better job than the first predictor alone, then it drops down and then it goes back up.\n00:03:35 So now we have a function that has two transition points and fits the data better than the original single-layer decision tree did.\n00:03:47 Again, we take this function and we can compute its error residual, meaning the error for each data point, and we can plot that over here on the left.\n00:03:59 in green.\n00:04:06 So now we have a new error residual function, and we can again try to fit that using a new model.\n00:04:13 We fit another single-layer decision tree, and we produce the following model, slightly negative over here on the left, slightly positive over here on the right, the best single-layer decision stump regressor for this error residual.\n00:04:19 Adding this into the set will produce a slightly more complex function, regressor, being the sum of now three decision stumps, and thus possibly having three transition points, and so on.\n00:04:26 So here's the sequence of models that we might learn, and their error residuals.", "start_char_idx": 0, "end_char_idx": 4261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67f3c380-8ea5-42bf-8248-8c0982dd0211": {"__data__": {"id_": "67f3c380-8ea5-42bf-8248-8c0982dd0211", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (3) Gradient Boosting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sRktKszFmSk", "Link": "https://www.youtube.com/watch?v=sRktKszFmSk"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcf19e6f-6bf6-4a72-b5f3-42ec9228d639", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (3) Gradient Boosting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sRktKszFmSk", "Link": "https://www.youtube.com/watch?v=sRktKszFmSk"}, "hash": "e0f2153cf30e8d0c752b1c9b9a219b05894ab5e0e1a6e4dcfe1b19aac3356c3a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8091657c-ac4d-4c16-818f-84580dab4ebd", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Ensembles (3) Gradient Boosting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sRktKszFmSk", "Link": "https://www.youtube.com/watch?v=sRktKszFmSk"}, "hash": "66851e67dbc28429b470358c10c3c19101404c6a9b3daeb1a6a56ca01b4bde0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "948d7d48-3fe7-4199-9a8d-386661131ac8", "node_type": "1", "metadata": {}, "hash": "55a71a5852084ac52102f04fad8a9b6c629da6698e1743c0291910f0ad541e46", "class_name": "RelatedNodeInfo"}}, "text": "00:03:47 Again, we take this function and we can compute its error residual, meaning the error for each data point, and we can plot that over here on the left.\n00:03:59 in green.\n00:04:06 So now we have a new error residual function, and we can again try to fit that using a new model.\n00:04:13 We fit another single-layer decision tree, and we produce the following model, slightly negative over here on the left, slightly positive over here on the right, the best single-layer decision stump regressor for this error residual.\n00:04:19 Adding this into the set will produce a slightly more complex function, regressor, being the sum of now three decision stumps, and thus possibly having three transition points, and so on.\n00:04:26 So here's the sequence of models that we might learn, and their error residuals.\n00:04:33 Over here is the first decision stump regressor.\n00:04:40 It's error residual.\n00:04:46 Obviously it's too simple of a model to do a very good job of fitting this function.\n00:04:53 Once we fit the error residual, we add its model in, and we get this model, which is now slightly negative.\n00:05:00 slightly more complex, slightly better able to fit the data.\n00:05:06 Again, computing its error residual, we get this green function down here.\n00:05:12 Fitting it to a single-layer decision tree regressor, we get another function.\n00:05:18 Adding it in, we get a slightly more complex shape, a new error residual.\n00:05:24 Fit it to a simple model, add it in, get a slightly more complex shape.\n00:05:30 As we proceed, we move from the left where we have a single decision stump to the right where we have the sum of five decision stumps, and our function is getting slightly more complicated at each step.\n00:05:36 While its accuracy is getting slightly better to the training data at each step as well.\n00:05:42 This algorithm is an instance of gradient boosting.\n00:05:48 It's called gradient boosting because it's related to a gradient ascent sort of procedure.\n00:05:54 First, we make a set of predictions.\n00:06:00 one for each data point.\n00:06:05 So y hat i is our prediction for data point i.\n00:06:10 We can calculate the error in our predictions.\n00:06:16 Let's call it j.\n00:06:21 And j just relates the accuracy of y hat in modeling y.\n00:06:27 For mean squared error, which is what my previous example was focused on, this cost function is the sum over the data points of y i minus y hat i squared.\n00:06:32 And so now we can try to think about adjusting our prediction y hat to try to reduce this error.\n00:06:38 So the gradient of j with respect to the predictions y hat is just solved by taking the derivative with respect to all the predictions.\n00:06:43 And a gradient ascent on the predictions would look like adjusting y hat to be its old value plus some step size alpha times the gradient f.\n00:06:49 So f i is the gradient of j.\n00:06:54 And if we take the derivative of this mean squared error function, we find that it's.\n00:06:59 take the derivative of the mean squared error function with respect to the prediction y-hat-i, we find that it is simply two times the difference y-i minus y-hat-i.\n00:07:08 And this is precisely the error residual.\n00:07:17 So the example I gave before, where we're computing an estimate y-hat, we're computing a model that we fit to its error residual.\n00:07:25 You can think of that as fitting a model to this gradient function.\n00:07:34 And then we take a step, alpha, by adding it together.\n00:07:42 So each learner is estimating this gradient del J, and a gradient descent on the squared error takes a sequence of steps to try to reduce J.\n00:07:51 So we'll estimate the gradient and take a step of size alpha.\n00:07:59 in reducing that gradient by adding in the new predictor of the gradient to our y hat.\n00:08:06 So here's pseudocode for the procedure, again specialized to mean squared error.", "start_char_idx": 3446, "end_char_idx": 7330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "948d7d48-3fe7-4199-9a8d-386661131ac8": {"__data__": {"id_": "948d7d48-3fe7-4199-9a8d-386661131ac8", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (3) Gradient Boosting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sRktKszFmSk", "Link": "https://www.youtube.com/watch?v=sRktKszFmSk"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcf19e6f-6bf6-4a72-b5f3-42ec9228d639", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (3) Gradient Boosting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sRktKszFmSk", "Link": "https://www.youtube.com/watch?v=sRktKszFmSk"}, "hash": "e0f2153cf30e8d0c752b1c9b9a219b05894ab5e0e1a6e4dcfe1b19aac3356c3a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67f3c380-8ea5-42bf-8248-8c0982dd0211", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Ensembles (3) Gradient Boosting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sRktKszFmSk", "Link": "https://www.youtube.com/watch?v=sRktKszFmSk"}, "hash": "4cef15ab16ea9d3d5295ee7f21a23daf6d1101afff004e250144567778d34131", "class_name": "RelatedNodeInfo"}}, "text": "00:07:08 And this is precisely the error residual.\n00:07:17 So the example I gave before, where we're computing an estimate y-hat, we're computing a model that we fit to its error residual.\n00:07:25 You can think of that as fitting a model to this gradient function.\n00:07:34 And then we take a step, alpha, by adding it together.\n00:07:42 So each learner is estimating this gradient del J, and a gradient descent on the squared error takes a sequence of steps to try to reduce J.\n00:07:51 So we'll estimate the gradient and take a step of size alpha.\n00:07:59 in reducing that gradient by adding in the new predictor of the gradient to our y hat.\n00:08:06 So here's pseudocode for the procedure, again specialized to mean squared error.\n00:08:13 Usually we start off with the extremely simple predictor of a constant value.\n00:08:19 So the best mean squared error predictor of y with a constant value is simply its mean.\n00:08:26 And so we start off by predicting mu, which is an extremely simple predictor.\n00:08:33 We calculate its error residual y minus mu.\n00:08:39 Then we run from 1 to the number of boosted predictors that we'd like to learn.\n00:08:46 Each learner we do by training a regression model from the features x to our error residual dy.\n00:08:53 And we take some step size alpha k, which I'll discuss in just a moment.\n00:08:59 This becomes the weight of that learner in the model.\n00:09:06 Then, we update our predictions.\n00:09:13 So, our new prediction consists of a sum of learners, and dy already captures most of those.\n00:09:19 So, we have only to subtract the new addition of our predictions.\n00:09:26 So, we subtract our step size alpha times our new learner of the current error residual, and compute a new error residual, dy.\n00:09:33 That's the difference between what we were trying to predict, the old error residual, and what we were able to predict with this learner.\n00:09:39 So, alpha k is the step size of this particular learner, and you can think of it as a learning rate.\n00:09:46 So, often the choice is simply to set it equal to 1.\n00:09:53 What one typically finds is that if alpha is large, like 1, the system will converge very rapidly.\n00:09:59 to a model that's unable to get any better, meaning its error residual can't be modeled by the class of predictors anymore.\n00:10:11 On the other hand, choosing a smaller value of alpha will tend to use a lot more classifiers because it's taking smaller steps and so it needs more steps to cover the distance, but often it can produce a better prediction because it follows this gradient in a smoother way.\n00:10:23 So if you choose a small alpha, you tend to get better results, but computationally it's more difficult because you'll need to train more models and you'll need to use them in prediction time.\n00:10:35 And when it comes time to predict, you simply take a prediction vector, you run through all of your boosted predictors and your overall prediction is the weighted sum of those predictions.\n00:10:47 So here we predict something that is the sum over k of.\n00:10:59 are weight alpha k times the predictor of that instance, or rather the kth predictor.\n00:11:09 In summary, gradient boosting is a particular kind of ensemble method that uses weighted sums of regressors to produce an overall better regressors.\n00:11:19 It works by using a very simple regression model to begin with, and then subsequent models are trained to predict the errors made by the model so far.\n00:11:28 So the models are trained sequentially, and slowly focus in on the difficult to predict models.\n00:11:38 The overall prediction is then given a weighted sum of the collection, so that each predictor is evaluated and summed together with a weight given by the step size alpha.", "start_char_idx": 6593, "end_char_idx": 10354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce941a0f-87c1-4433-9aba-839c5a227b4d": {"__data__": {"id_": "ce941a0f-87c1-4433-9aba-839c5a227b4d", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fea08087-d98d-44fa-9415-8d1580ba13a2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "hash": "0c14475784166e47c92dad495628e819e1b464dffe70027487431c840c97d0d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c10ea66d-6baf-4b42-8175-c8f215105655", "node_type": "1", "metadata": {}, "hash": "a654a95e53a3c1b28dd21f075bc5eaf7385f6f3ee905aa75b1f9c50883f4951e", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Boosting uses ensemble techniques to create a sequence of increasingly complex predictors out of building blocks made out of very simple predictors.\n00:00:10 Boosting trains models by sequentially training a new simple model based on the errors of the previous model.\n00:00:20 So we start off by learning a very simple predictor and then evaluate its errors and focus the next predictor on getting these examples right.\n00:00:30 So this procedure tends to discover the examples and data points that are hard to predict and focuses later classifiers on predicting these examples better.\n00:00:40 In the end we combine the whole set uses some weighted combination and this is a way of scaling up complexity.\n00:00:50 So each individual predictor tends to be very simple and by combining many of these weak learners that are not able to learn complex functions we can convert them all into an overall much more complex classifier.\n00:01:00 So, here's an example of boosting for classification.\n00:01:06 Suppose we have the following data set, which has some plus one class entries in red, some minus one class entries in blue.\n00:01:12 We learn a classifier for these data, but we restrict ourselves to an extremely simple kind of classifier, let's say a decision stump, so a one-level decision tree.\n00:01:18 So, we learn this decision stump, which splits on X2 at this point.\n00:01:24 This gets most of the points right, but it gets these two negative points wrong and this positive point wrong.\n00:01:30 So what we'll do is we'll focus the next trainer on those.\n00:01:36 The best such classifier in this extremely simple class is the following.\n00:01:42 So we split on X2 at this point, and this gets many of the data right, but not all of them.\n00:01:48 In particular, it makes three mistakes, two minuses over here, and one plus right there.\n00:01:54 These are the data we'll focus on next for the next\n00:02:00 classifier.\n00:02:06 We increase the weights of these points, the importance of getting them right in the classifier.\n00:02:12 Now we have a new training data set which consists of the same points as originally, but now we have weights assigned to these points.\n00:02:18 So here I've indicated those visually by the size of the indicators.\n00:02:24 So these smaller pluses and smaller minuses are less important for us to get right, while the larger points, the large plus and the larger minuses, are more important.\n00:02:30 We now train another classifier of the same kind, so a very simple decision stump, but we train it to try to get the weighted error to be low.\n00:02:36 So here's the best decision stump that we find.\n00:02:42 Again, we see that it gets a lot of points right.\n00:02:48 In particular, it tries to get these weighted points more right.\n00:02:54 So here we've learned something that, unlike the first data set, the first prediction over here is now focused on getting...\n00:03:00 say, those three points correct.\n00:03:08 It also happens to get all of those correct, but is willing to make a mistake on three small minuses, as opposed to the, say, two large minuses that we got wrong in the first step.\n00:03:17 Again, we identify the data points where we've made a mistake, and we increase their weight for the next round, decreasing the weights of the ones we get right.\n00:03:25 Since we got these points correct, their weight's gone down.\n00:03:34 Since we got these pluses correct, their weight's gone down even further, and these points, which got smaller in the first step, have now gotten larger again.\n00:03:42 We now train another model of the same simple type to try to predict these weighted data points with low weighted error.\n00:03:51 Again, this classifier is focused primarily on getting the larger data points right, and thus it will be slightly different than the previous ones, focusing on getting, say, this red plus and those three minuses correct, and being willing to perhaps make a mistake on these.\n00:04:00 for very small red pluses.\n00:04:12 Although we haven't talked about minimizing weighted error, it's not particularly different than the unweighted error we've minimized in the past.", "start_char_idx": 0, "end_char_idx": 4162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c10ea66d-6baf-4b42-8175-c8f215105655": {"__data__": {"id_": "c10ea66d-6baf-4b42-8175-c8f215105655", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fea08087-d98d-44fa-9415-8d1580ba13a2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "hash": "0c14475784166e47c92dad495628e819e1b464dffe70027487431c840c97d0d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce941a0f-87c1-4433-9aba-839c5a227b4d", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "hash": "7b450bbe1fb6790277bf06aa19320d5cfc56738f46ad2c3c62565048ace512cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7bacbd6-3f66-4c29-9421-d29e69ec3d4d", "node_type": "1", "metadata": {}, "hash": "b85896d70a856e9d7196e0f494d0b16d98cbf15dd18053c2e0c9588e5f1c115c", "class_name": "RelatedNodeInfo"}}, "text": "00:03:25 Since we got these points correct, their weight's gone down.\n00:03:34 Since we got these pluses correct, their weight's gone down even further, and these points, which got smaller in the first step, have now gotten larger again.\n00:03:42 We now train another model of the same simple type to try to predict these weighted data points with low weighted error.\n00:03:51 Again, this classifier is focused primarily on getting the larger data points right, and thus it will be slightly different than the previous ones, focusing on getting, say, this red plus and those three minuses correct, and being willing to perhaps make a mistake on these.\n00:04:00 for very small red pluses.\n00:04:12 Although we haven't talked about minimizing weighted error, it's not particularly different than the unweighted error we've minimized in the past.\n00:04:24 In particular, for algorithms that directly minimize a cost function, like linear classifiers and others, instead of an unweighted sum, where each term Ji corresponds to some cost function on data point I, and we sum those up and take their average, we might have a weighted average where we have weights Wi, which in the unweighted case, simply correspond to one over N for all I, but in this case might correspond to different values of W.\n00:04:36 And these terms weight the cost function for point I, and tell us which data points are more important to minimize error or less important.\n00:04:48 For algorithms that are not direct minimizations, like decision trees, so forth, we can simply modify the techniques.\n00:05:00 had before to now take weights.\n00:05:08 So for instance, in the decision tree, we might use a weighted impurity score where we still calculate the entropy, but instead of calculating the entropy of the unweighted fraction of data in one class or another, we define, for instance, the probability of a class plus 1 to be the total weight of data in class plus 1, and class minus 1 have the total weight of data in class minus 1.\n00:05:17 And that will still define a probability distribution over classes as long as the weights are forced to sum to 1.\n00:05:25 And we can still take that as a probability and define an impurity score, h of p, which we can then use to score information gain and so forth.\n00:05:34 In the end, each of these classifiers is assigned a weight, which we'll describe in a moment.\n00:05:42 And their weighted combination becomes an overall combined predictor.\n00:05:51 Since we're predicting plus or minus 1, we simply.\n00:06:00 add up the weighted sum of the predictions and we check its sign.\n00:06:07 So if the sign of the weighted sum is positive, we predict plus one.\n00:06:15 If the sign is negative, we predict minus one.\n00:06:22 When we do this, we find that the combination of several extremely simple models, decision stumps, produces an overall decision region that is more complex.\n00:06:30 So here, this weighted combination has managed to carve out a shape, a region to predict plus one and minus one in, that is more complex than any simple decision stump could have.\n00:06:37 The classic example of boosting for classification is the AdaBoost, or adaptive boosting algorithm.\n00:06:45 AdaBoost trains a series of models, say n-boost models sequentially, using a weighted trainer.\n00:06:52 So we can use any black box machine learning algorithm that we would like, as long as it's able to accept.\n00:07:00 a weighted collection of training points and try to minimize the weighted error.\n00:07:06 So here, when we train classifier I, we train it on not just a data set X with targets Y, but we have a vector of weights that tell us how important it is to get that particular data point right.\n00:07:13 When we start off, we set all the weights to uniform.\n00:07:20 And as we go on, these weights evolve to tell us which points should be focused on by the next learner.\n00:07:26 We train our model to try to minimize the weighted error, and then we compute what that model predicts for us.\n00:07:33 So Y hat is the predictions of model I, and then we check to see which data points have been gotten wrong by this predictor.", "start_char_idx": 3319, "end_char_idx": 7463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7bacbd6-3f66-4c29-9421-d29e69ec3d4d": {"__data__": {"id_": "a7bacbd6-3f66-4c29-9421-d29e69ec3d4d", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fea08087-d98d-44fa-9415-8d1580ba13a2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "hash": "0c14475784166e47c92dad495628e819e1b464dffe70027487431c840c97d0d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c10ea66d-6baf-4b42-8175-c8f215105655", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "hash": "80118a8e0a5a6d324f2c3f812b80fae3530d144816e67f85989db930a45c39e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6441edd-7f7b-4260-8b90-b76f45044ae0", "node_type": "1", "metadata": {}, "hash": "204eac661cb1caa213edf82480914b93506824b243e84bf1e313ce0322a97015", "class_name": "RelatedNodeInfo"}}, "text": "00:07:00 a weighted collection of training points and try to minimize the weighted error.\n00:07:06 So here, when we train classifier I, we train it on not just a data set X with targets Y, but we have a vector of weights that tell us how important it is to get that particular data point right.\n00:07:13 When we start off, we set all the weights to uniform.\n00:07:20 And as we go on, these weights evolve to tell us which points should be focused on by the next learner.\n00:07:26 We train our model to try to minimize the weighted error, and then we compute what that model predicts for us.\n00:07:33 So Y hat is the predictions of model I, and then we check to see which data points have been gotten wrong by this predictor.\n00:07:40 So we check for all the data points where Y is not equal to Y hat, and we compute an error vector that's the weighted sum error.\n00:07:46 So weights is a vector of weights on the data points.\n00:07:53 This is a vector of zeros and ones\n00:08:00 us whether we got that point wrong.\n00:08:06 And so e will be the sum, the vector product of those things.\n00:08:13 So the sum of the weighted errors.\n00:08:20 We compute our weight alpha i to be one half times the log odds ratio of that error.\n00:08:26 And we compute a new weight vector.\n00:08:33 So we update the weight vector for the data by multiplying it by e to the minus alpha i times y times y hat.\n00:08:40 So y times y hat, since these classes are plus and minus one, if we've gotten the data correct, we've made a correct prediction, y and y hat will be the same sign, and this will be positive.\n00:08:46 And so e to the minus alpha i, alpha will be a, will be a positive number as well.\n00:08:53 So e to the minus alpha i will be, alpha i, y, y hat will be.\n00:09:00 be a positive number and e to the minus it will be a small number.\n00:09:10 On the other hand if y and y hat are of different classes their sign will be opposite and so this will be a positive number and so e to the positive number will be a large number so we'll up weight things.\n00:09:20 So we update the weights increasing the weights of anything where y is not equal to y hat and decreasing the weight of things where y does equal y hat and then we normalize the weights to sum to one because we want to have a normalized collection of weights.\n00:09:30 We repeat this procedure over and over until we've learned however many classifiers we want and our final classifier is just the weighted sum of our predictions.\n00:09:40 So again these are signs plus and minus one so the weighted sum of them will be above zero and we'll predict positive one or below zero and we'll predict negative one.\n00:09:50 A few things about this algorithm...\n00:10:00 might seem a bit mysterious, like why that choice of alpha or why that choice of weights.\n00:10:06 And although I'm not going to go into very many details, it has a nice theoretical justification.\n00:10:13 It turns out that this algorithm, AdaBoost, corresponds to minimizing a surrogate loss function of our error.\n00:10:20 In particular, AdaBoost minimizes something called the exponential loss.\n00:10:26 So the sum over the data points of e to the minus class times our linear predictor, or our prediction function, rather.\n00:10:33 So again, if these two things have the same sign, this is a positive number, e to the minus, a positive number, will be small.\n00:10:39 If they have a different sign, it'll grow.\n00:10:46 So this is a smooth, convex surrogate for the 0, 1 loss, and thus is fairly easier to optimize.\n00:10:53 And it turns out this AdaBoost procedure is iteratively minimizing this exponential loss.\n00:10:59 One example where Adaboost was really applied with tremendous success is to the face detection problem and a classic algorithm called the Viola-Jones face detector.\n00:11:07 The idea here is that we can, well, again, combine a collection of many very weak and easy-to-compute classifiers.", "start_char_idx": 6739, "end_char_idx": 10667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6441edd-7f7b-4260-8b90-b76f45044ae0": {"__data__": {"id_": "e6441edd-7f7b-4260-8b90-b76f45044ae0", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fea08087-d98d-44fa-9415-8d1580ba13a2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "hash": "0c14475784166e47c92dad495628e819e1b464dffe70027487431c840c97d0d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7bacbd6-3f66-4c29-9421-d29e69ec3d4d", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "hash": "097b1461978ac70960bef00f2cd3247632ea37246f2d3d0d0f5b84b6be0c588e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59c66e4b-2a26-4bd1-b138-879e17a2e4bc", "node_type": "1", "metadata": {}, "hash": "72b16f7673a225cde990abd427f961cd52e38e18f61493b3e670d873570408de", "class_name": "RelatedNodeInfo"}}, "text": "00:10:26 So the sum over the data points of e to the minus class times our linear predictor, or our prediction function, rather.\n00:10:33 So again, if these two things have the same sign, this is a positive number, e to the minus, a positive number, will be small.\n00:10:39 If they have a different sign, it'll grow.\n00:10:46 So this is a smooth, convex surrogate for the 0, 1 loss, and thus is fairly easier to optimize.\n00:10:53 And it turns out this AdaBoost procedure is iteratively minimizing this exponential loss.\n00:10:59 One example where Adaboost was really applied with tremendous success is to the face detection problem and a classic algorithm called the Viola-Jones face detector.\n00:11:07 The idea here is that we can, well, again, combine a collection of many very weak and easy-to-compute classifiers.\n00:11:14 In particular, Viola-Jones uses decision stumps with thresholds on single feature, just like my example earlier.\n00:11:22 And we'll do this by defining lots and lots of features, but using very simple classifiers of those features.\n00:11:29 So the number of features will make us prone to overfitting.\n00:11:37 The choice of an extremely simple classifier function will make us less prone to overfitting.\n00:11:44 And then we'll use Adaboost to slowly build up a more complex function and fit well.\n00:11:52 So the idea of face detection is we're going to build a classifier that looks at a single patch of an image.\n00:11:59 and decides whether or not that image patch has a face.\n00:12:07 So, cameras do this all the time, they look at a single patch and they decide whether that looks more like a natural background or whether it actually has a human face in it, meaning that probably it's something that the camera should focus on.\n00:12:14 To do this well, you want the model to be extremely accurate and also extremely fast.\n00:12:22 In any computer vision or image processing problem, the feature representation turns out to be critically important.\n00:12:29 It turns out that there's very little to be told about a particular pixel being a particular shade of gray.\n00:12:37 It's not very helpful in recognizing visual phenomena.\n00:12:44 And so, computer vision researchers have done a lot of work to try to find feature representations that are evocative of actual phenomena in the visual image.\n00:12:52 The basis chosen here is called the Haar wavelet basis.\n00:12:59 and consists of image responses to patches of filters.\n00:13:05 So there are four basic types.\n00:13:11 There's a vertical response and a horizontal response and some more.\n00:13:17 And the way that you interpret this is this is a region which will be used to extract out an appearance of a particular piece of this image.\n00:13:23 So over here is a patch which may or may not contain a face.\n00:13:29 And each feature in our new representation will consist of a sum, a weighted sum, of many of these pixels in the image.\n00:13:35 So for instance, here is a HAR wavelet that's a horizontal response.\n00:13:41 It's got black region above a white region.\n00:13:47 And so what we do is we add together all the pixels under the black region and then we subtract from that the sum of all the pixels under the white region.\n00:13:53 So.\n00:13:59 A patch like this, a filter like this, is going to have a high response when there's a light region, say over the eyes, and a dark region, so small values, right beneath it.\n00:14:11 Similarly, here's another type, so another type has a black or a white in the interior and then the opposite color on the exteriors, so sort of a three-bar pattern.\n00:14:23 And so, something like that here, this is going to have a response when there's a large intensity on the outer and some dark patch in the interior.\n00:14:35 So in this case, something like the eyes, which might have dark on the outside and the nose causing a bright patch in the middle, might have a large negative response to this filter.", "start_char_idx": 9849, "end_char_idx": 13793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59c66e4b-2a26-4bd1-b138-879e17a2e4bc": {"__data__": {"id_": "59c66e4b-2a26-4bd1-b138-879e17a2e4bc", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fea08087-d98d-44fa-9415-8d1580ba13a2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "hash": "0c14475784166e47c92dad495628e819e1b464dffe70027487431c840c97d0d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6441edd-7f7b-4260-8b90-b76f45044ae0", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "hash": "7d9f7d568317f9ce92e30cd3304bdedb6b83fb16415dae281988d6e721c4e589", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3e495b1-dec6-4d60-b7bf-ca0ddd27402d", "node_type": "1", "metadata": {}, "hash": "605e5985882870aaeb88278b96a25303fca5026e8e40b6f1f9ef54b7f8dd59b8", "class_name": "RelatedNodeInfo"}}, "text": "00:13:53 So.\n00:13:59 A patch like this, a filter like this, is going to have a high response when there's a light region, say over the eyes, and a dark region, so small values, right beneath it.\n00:14:11 Similarly, here's another type, so another type has a black or a white in the interior and then the opposite color on the exteriors, so sort of a three-bar pattern.\n00:14:23 And so, something like that here, this is going to have a response when there's a large intensity on the outer and some dark patch in the interior.\n00:14:35 So in this case, something like the eyes, which might have dark on the outside and the nose causing a bright patch in the middle, might have a large negative response to this filter.\n00:14:47 And so that might be useful in recognizing faces, because we would see that spike in a particular area of the image, and we would know that the...\n00:14:59 that might correspond to eyes and a nose in that piece.\n00:15:09 They're also not too inefficient to calculate, because you can pre-process the image to compute something called the integral image.\n00:15:19 And what this is, is you compute the partial sum of the image up to a particular pixel, so in a rectangle to, say, the up and left of that point.\n00:15:29 And then the integral over some region, let's say over this white region that I want to subtract, I can compute by just using the four points at the corners of the pixels.\n00:15:39 And that will tell me what the sum in a number of regions are, and I can just calculate it by the difference of those points.\n00:15:49 So if you look over these four shapes and all of the different sizes, so lengths and scales that one could look at, it turns out that for a fairly simple, small image, say a 28 by 28 image patch like the one I showed.\n00:15:59 there are about 140,000, maybe 180,000 features of this wavelet type.\n00:16:05 So 180,000 features is quite a lot, and so already we're probably in danger of perhaps overfitting.\n00:16:11 And so we'll choose an extremely simple classifier, a decision stump.\n00:16:17 So we'll look at only one feature and perform some thresholding.\n00:16:23 So each of these features is going to be one possible classifiers.\n00:16:29 And so to train, we'll train a sequence of classifiers.\n00:16:35 We'll train a classifier on, each of these classifiers will be trained by learning one classifier per feature using whatever our weights are.\n00:16:41 Then we pick only the best one.\n00:16:47 We find where it makes its errors and we reweight the data.\n00:16:53 So this can take a long, long time to train.\n00:16:59 because there are hundreds of thousands of features to test, and we have to test each one of them for each of our sequence of boosted classifiers.\n00:17:07 So one thing you could do is to make this classifier even weaker by simply not training it very well.\n00:17:14 So instead of finding the best decision stump, you could just find a quick decision stump.\n00:17:22 So this is fairly easy to do.\n00:17:29 You can do something like choose the weighted midpoint between the means of the plus 1 class and the means of the minus 1 class or something.\n00:17:37 So any procedure that doesn't directly optimize and thus spend a lot of time.\n00:17:44 I'm also glossing over a lot of tricks in the real Viola-Jones paper that were really used to make this an effective and practical technique.\n00:17:52 So in particular, the actual Viola-Jones method used a cascade of decisions.\n00:17:59 of just a weighted combination.\n00:18:09 So, if earlier classifiers had rejected a data point as having a face, it would be immediately discarded and not considered further.\n00:18:19 And that allowed them to process many more patches quickly than if all of the boosted predictors need to be applied to every patch.", "start_char_idx": 13075, "end_char_idx": 16856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3e495b1-dec6-4d60-b7bf-ca0ddd27402d": {"__data__": {"id_": "b3e495b1-dec6-4d60-b7bf-ca0ddd27402d", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fea08087-d98d-44fa-9415-8d1580ba13a2", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "hash": "0c14475784166e47c92dad495628e819e1b464dffe70027487431c840c97d0d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59c66e4b-2a26-4bd1-b138-879e17a2e4bc", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}, "hash": "a44360c02e901a43a9e34c7af387e92c01f816c6394bef4cddf2fe95712a3c72", "class_name": "RelatedNodeInfo"}}, "text": "00:17:22 So this is fairly easy to do.\n00:17:29 You can do something like choose the weighted midpoint between the means of the plus 1 class and the means of the minus 1 class or something.\n00:17:37 So any procedure that doesn't directly optimize and thus spend a lot of time.\n00:17:44 I'm also glossing over a lot of tricks in the real Viola-Jones paper that were really used to make this an effective and practical technique.\n00:17:52 So in particular, the actual Viola-Jones method used a cascade of decisions.\n00:17:59 of just a weighted combination.\n00:18:09 So, if earlier classifiers had rejected a data point as having a face, it would be immediately discarded and not considered further.\n00:18:19 And that allowed them to process many more patches quickly than if all of the boosted predictors need to be applied to every patch.\n00:18:29 So, that makes it more computationally efficient, particularly since it also needs to be applied at multiple scales of the image, since the face might be very large in the picture, taking up most of the frame, or it might be small somewhere in the background.\n00:18:39 Overall, though, this boosted technique produced an extremely good facial recognition or facial detection process and is one of the baselines of facial detection to this day.\n00:18:49 So, in summary, ensemble methods try to combine multiple classifiers to make a better one, and boosting in particular tries to train\n00:18:59 a sequence of models, each of which is trained using the errors of the previous models.\n00:19:11 This makes later predictors able to focus on the mistakes of the earlier ones and focus on getting hard examples right.\n00:19:23 The example of boosting for classification in this case is AdaBoost or adaptive boosting, and this uses the result of earlier classifiers to up-weight training examples, and then in the classification process the black box classifier tries to minimize the weighted errors.", "start_char_idx": 16019, "end_char_idx": 17959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9063cc09-9b45-497c-8fe9-e8845bbef073": {"__data__": {"id_": "9063cc09-9b45-497c-8fe9-e8845bbef073", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6R16reLVl3I", "Link": "https://www.youtube.com/watch?v=6R16reLVl3I"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08bf9903-4e29-4ce8-8e26-10d4e97702ef", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6R16reLVl3I", "Link": "https://www.youtube.com/watch?v=6R16reLVl3I"}, "hash": "b3d7b301295827f5922523d3161d56f9f3dbfc08340b156cf72dd8f93ba0416b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b150b699-b9b9-48ea-8904-98c3ef4576cf", "node_type": "1", "metadata": {}, "hash": "615610452029e647fbb2951dd14fd9d7c0e86bf558cd54055a0252e2c012bd2b", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Clustering is one of the most common forms of unsupervised learning, and often helpful in understanding the characteristics of our data.\n00:00:06 Unsupervised learning tasks are quite different from supervised prediction tasks.\n00:00:12 In supervised learning, we have something in particular we'd like to predict, for example, whether an email is spam or the price of a stock tomorrow.\n00:00:18 This prediction variable is called the target, y.\n00:00:24 In order to predict it, we observe a number of features, x, that we think will be helpful.\n00:00:30 The learning task, then, is to learn a mapping from x to y.\n00:00:36 In unsupervised learning, in contrast, we have no particular target variable, y.\n00:00:42 Instead, the task is just to understand the data, identify patterns, and so on.\n00:00:48 Thus, we assume we have only the set of features, x, that we'd like to use to understand these patterns.\n00:00:54 Unsupervised learning is useful in many situations, most commonly to find patterns in the data and explain how the data came about in a way that a human can understand.\n00:01:00 However, even if our ultimate task is something like a more supervised predictive problem, unsupervised learning can be helpful.\n00:01:08 For example, if some of our data are missing features, so meaning we couldn't observe that feature's value, then unsupervised learning models can be used to fill in these missing values in a process called imputation.\n00:01:17 Similarly, unsupervised model information could be used as features, either to augment the original features or replace them with our simplified explanation of the data.\n00:01:25 In these slides, we'll look at the concept of clustering, or understanding the data by automatically organizing them into groups with shared characteristics.\n00:01:34 For example, in these data, we can readily recognize that there are three groups to the data and summarize the data set in some sense by just describing these groups.\n00:01:42 Clustering models group the data, partitioning the data points and placing similar data together in each group.\n00:01:51 Clustering models group the data, partitioning the data points and placing similar data together \n00:01:59 and dissimilar data in different groups.\n00:02:08 However, the concept of grouping is a product of our human understanding of the world and often a holistic understanding of the entire dataset.\n00:02:17 So what makes a sensible grouping might vary by the dataset.\n00:02:25 For example, when we look at these two-dimensional points, we immediately see an explanation that the data are organized into about three groups, like so, and these groups share similarity in their general location in this feature space.\n00:02:34 So points over here belong to this group, points over here belong to this group, and so on.\n00:02:42 A different set of data, however, might not vary by location exactly, but our brains can still see patterns that define the similarity which we can turn into groupings or clusters.\n00:02:51 So these clusters are not defined by location in the space precisely but by some holistic property.\n00:03:00 of a collection of points.\n00:03:07 In the third example, we might see that the data have different properties in different parts of the space, for example, a different density in one part of the space than another.\n00:03:15 That density, then, might suggest a grouping of the data into two sets, one of which has sparse data and one of which has dense data.\n00:03:22 Complex learning is closely related to data compression concepts.\n00:03:30 In particular, clustering is related to vector quantization, a staple of lossy compression algorithms.\n00:03:37 Suppose we need to transmit a large data set, for example, a large image with many pixels.\n00:03:45 We can chunk the image into vectors of some fixed size, for example, grouping all the pixels into 2x2 squares, and then we can interpret those as length-4 vectors and transmit them in some order.\n00:03:52 Now, some of these squares may be very similar.\n00:04:00 For example, this patch and this one are nearly identical.\n00:04:06 So instead of sending the pixel values twice, we can simply say, use patch 13, or whatever, and send fewer data.", "start_char_idx": 0, "end_char_idx": 4236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b150b699-b9b9-48ea-8904-98c3ef4576cf": {"__data__": {"id_": "b150b699-b9b9-48ea-8904-98c3ef4576cf", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6R16reLVl3I", "Link": "https://www.youtube.com/watch?v=6R16reLVl3I"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08bf9903-4e29-4ce8-8e26-10d4e97702ef", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6R16reLVl3I", "Link": "https://www.youtube.com/watch?v=6R16reLVl3I"}, "hash": "b3d7b301295827f5922523d3161d56f9f3dbfc08340b156cf72dd8f93ba0416b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9063cc09-9b45-497c-8fe9-e8845bbef073", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Clustering (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6R16reLVl3I", "Link": "https://www.youtube.com/watch?v=6R16reLVl3I"}, "hash": "3afa354cec26cf6f5242bc87f9d1499d792dfab2d766703d2230d5bd45333a9a", "class_name": "RelatedNodeInfo"}}, "text": "00:03:22 Complex learning is closely related to data compression concepts.\n00:03:30 In particular, clustering is related to vector quantization, a staple of lossy compression algorithms.\n00:03:37 Suppose we need to transmit a large data set, for example, a large image with many pixels.\n00:03:45 We can chunk the image into vectors of some fixed size, for example, grouping all the pixels into 2x2 squares, and then we can interpret those as length-4 vectors and transmit them in some order.\n00:03:52 Now, some of these squares may be very similar.\n00:04:00 For example, this patch and this one are nearly identical.\n00:04:06 So instead of sending the pixel values twice, we can simply say, use patch 13, or whatever, and send fewer data.\n00:04:13 Equivalently, we could first transmit all the patches we plan to use first, as a dictionary, and then we'd send a sequence of patches in the image by listing the index of the closest entry in our dictionary.\n00:04:20 So this process is called vector quantization.\n00:04:26 Each 4D feature vector is quantized into one entry of the dictionary.\n00:04:33 Over here is a 2D visualization of that procedure.\n00:04:40 So each data point, a dot, is being approximated by its closest dictionary entry, a plus.\n00:04:46 So each dictionary entry claim all the data that are nearer to it than to any other plus.\n00:04:53 The process of selecting good dictionary vectors, so where to locate the pluses in the sequence,\n00:05:00 space, give a collection of data vectors that we plan to transmit, and that's just clustering.\n00:05:06 We're grouping the data by visual similarity, usually just Euclidean similarity, and each partition is assigned one entry in the dictionary.\n00:05:13 Here's a more trivial version, working with just one-dimensional or scalar data values.\n00:05:19 Each pixel, then, is considered a data point.\n00:05:26 It takes on one of 256 values.\n00:05:32 And then to compress the data, we'd like to transmit fewer values.\n00:05:39 So we group the data by their value.\n00:05:46 We might divide the space up into eight parts, or four parts, or only two parts.\n00:05:52 This quantizes the values of the data, the levels of gray, in the recovered image, so that we send fewer bits over here, giving us compression, but end up with only an approximation to the original data.", "start_char_idx": 3498, "end_char_idx": 5823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97feb81a-b5b5-48fb-a956-b72c28aaf1e6": {"__data__": {"id_": "97feb81a-b5b5-48fb-a956-b72c28aaf1e6", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (2) Hierarchical Agglomerative Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OcoE7JlbXvY", "Link": "https://www.youtube.com/watch?v=OcoE7JlbXvY"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99c39f0a-04f0-4911-a220-d028fee40787", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (2) Hierarchical Agglomerative Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OcoE7JlbXvY", "Link": "https://www.youtube.com/watch?v=OcoE7JlbXvY"}, "hash": "2210aef74e0e50cc9d510cffb5a09fb0e540a33ed0558e62d477c05f357ad6a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "019eb502-40bb-444a-aabf-5f475b752494", "node_type": "1", "metadata": {}, "hash": "41c0c7026cd1140de47a0303ce987cdcecd05e2cf5e904179132d8d2d88cfc32", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 The first clustering algorithm we'll look at is a procedure called hierarchical agglomerative clustering.\n00:00:05 Hierarchical agglomerative clustering is a simple algorithm for partitioning the data into groups.\n00:00:10 We start by defining a measure of the distance, or dissimilarity, between two groups.\n00:00:16 We'll return to what this is exactly in a moment.\n00:00:21 Then we initialize the algorithm by creating a separate cluster for each data point, so m clusters total.\n00:00:27 We compute our cluster distance score between all pairs of clusters.\n00:00:32 We'll store and reuse this for efficiency.\n00:00:38 And then find the closest pair according to our distance and merge the clusters.\n00:00:43 We'll continue until we end up with the desired number of clusters.\n00:00:49 Or, more commonly, we can continue until all the data are in one cluster and output the entire sequence of clusterings in a data structure called a dendrogram.\n00:00:54 Let's walk through this procedure on these data, and we'll keep track of the results.\n00:00:59 of the computational complexity.\n00:01:06 Initially, every datum is a cluster.\n00:01:13 Computing the distances between all pairs of data points takes O of m squared computation.\n00:01:20 We'll also sort them so we can find the smallest easily, which takes O of m squared log m time.\n00:01:26 We'll now merge the closest pair, giving us a new cluster shown in red.\n00:01:33 Now we need to compute all the cluster pairs distances again.\n00:01:40 Most of these are the same as in the previous step, but we need to recompute m minus 1 of them, the distances from each cluster to the new red cluster, and insert them into our sorted data structure, which takes m log m time.\n00:01:46 We'll visualize this process as a dendrogram here by drawing two points, shown in red, and connecting them with a line whose height indicates their distance when they were merged.\n00:01:53 Thank you.\n00:02:00 We again select the closest pair, perhaps this pair here, merge them, and recompute the distances from all the clusters to this new one, adding another m log m computation to our total.\n00:02:06 We'll draw these points over in the dendrogram and merge them.\n00:02:13 Since this pair was more distant than the first pair, their merge height will be higher.\n00:02:20 Finding the next closest pair of clusters, it might be that it involves one of our previously merged groups.\n00:02:26 In that case, we again connect the two clusters, forming a larger cluster, and join them in the dendrogram with a height that indicates their computed dissimilarity.\n00:02:33 Each merge reduces the total number of clusters by one.\n00:02:40 Eventually, we have only a few clusters.\n00:02:46 We can stop when we reach the desired number of clusters and use the resulting partitioning, purple, blue, and red.\n00:02:53 This procedure is implemented in the MATLAB Statistics Toolbox as the linkage function.\n00:02:59 However, it's more common to simply continue merging.\n00:03:05 So three clusters become two, we join them here.\n00:03:11 Until finally, after m minus one steps, all the data must belong to the same single cluster.\n00:03:17 Since each step was m log m, the total work for the merging process is also O of m squared log m.\n00:03:23 Hence, the total computation is m squared log m as well.\n00:03:29 Moreover, our dendrogram over here illustrates the trace of this merging procedure, showing not only which data were joined at each point, but when they were merged together in the order of increasing cluster distance.\n00:03:35 Given this sequence of merges, we can easily replay the clustering to select the output, given that we stop at a particular number of clusters.\n00:03:41 Alternatively, we might not know how many clusters there should be.\n00:03:47 Instead, we could choose clusters subject to some threshold on their dissimilarity.\n00:03:53 In other words, stop merging.\n00:03:59 once the clusters are more dissimilar than some threshold.", "start_char_idx": 0, "end_char_idx": 3990, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "019eb502-40bb-444a-aabf-5f475b752494": {"__data__": {"id_": "019eb502-40bb-444a-aabf-5f475b752494", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (2) Hierarchical Agglomerative Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OcoE7JlbXvY", "Link": "https://www.youtube.com/watch?v=OcoE7JlbXvY"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99c39f0a-04f0-4911-a220-d028fee40787", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (2) Hierarchical Agglomerative Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OcoE7JlbXvY", "Link": "https://www.youtube.com/watch?v=OcoE7JlbXvY"}, "hash": "2210aef74e0e50cc9d510cffb5a09fb0e540a33ed0558e62d477c05f357ad6a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97feb81a-b5b5-48fb-a956-b72c28aaf1e6", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Clustering (2) Hierarchical Agglomerative Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OcoE7JlbXvY", "Link": "https://www.youtube.com/watch?v=OcoE7JlbXvY"}, "hash": "e63ebc14de1c1a08d17447eebb0b148c47a2f766f9bd6cc1cc46729218318565", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa21369e-e5e0-4542-b503-eb8db636dd87", "node_type": "1", "metadata": {}, "hash": "b3e5c354784e244e1ef4895e86a24d22c6dc7f916e73765cda886386e50e2af2", "class_name": "RelatedNodeInfo"}}, "text": "00:03:17 Since each step was m log m, the total work for the merging process is also O of m squared log m.\n00:03:23 Hence, the total computation is m squared log m as well.\n00:03:29 Moreover, our dendrogram over here illustrates the trace of this merging procedure, showing not only which data were joined at each point, but when they were merged together in the order of increasing cluster distance.\n00:03:35 Given this sequence of merges, we can easily replay the clustering to select the output, given that we stop at a particular number of clusters.\n00:03:41 Alternatively, we might not know how many clusters there should be.\n00:03:47 Instead, we could choose clusters subject to some threshold on their dissimilarity.\n00:03:53 In other words, stop merging.\n00:03:59 once the clusters are more dissimilar than some threshold.\n00:04:09 So this condition corresponds to a cut across the dendrogram at some height, which then tells us how many clusters there are here, 1, 2, 3, 4.\n00:04:20 A critical choice in a kilometer of clustering is the cluster distance, which measures the dissimilarity between two groups of data points.\n00:04:30 This choice often determines the characteristics of the clustering that results.\n00:04:40 Perhaps the most common choice is to measure the distance between one group of points, Ci, and another group of points, Cj, using the minimum distance between any pair of points, x in cluster i and y in cluster j.\n00:04:50 So here, if the red points have already been merged to form cluster i, and the blue points have been merged to form cluster j, then the minimum distance will measure the similarity between red and blue clusters.\n00:05:00 by their closest distance, this pair here.\n00:05:08 This often goes by the special name single linkage.\n00:05:17 Notice that by merging the closest pair of the connected components, this is effectively just finding the minimum spanning tree of the data and using that to construct the clustering.\n00:05:25 Another common choice is to use so-called complete linkage, or the maximum distance between any pair of points X and I and Y and J.\n00:05:34 Unlike single linkage, which merges two clusters if they're close somewhere, this gives low dissimilarity only to clusters that are close everywhere.\n00:05:42 We'll see some examples of how these differ in a moment.\n00:05:51 Other choices include using the average distance between all pairs of points across the sets, so D average here, or using the distance between the two clusters' centers, means here, where mu i and mu j\n00:05:59 are the centroids of cluster I and cluster J.\n00:06:09 All these choices result in slightly different clusterings with different behaviors and resulting clusters.\n00:06:19 Notice that the mean of the data distances here is different from the distance between the data means here.\n00:06:29 One point to mention, however, is that our computational analysis in the previous slides assumed that it was easy, so constant time, to compute the distance between the clusters.\n00:06:39 Effectively, to have this assumption be true, we need that if we've already computed the distances from A to C, two clusters A and C, and between clusters B and C, then we should be able to compute the distance between a merged cluster, A and B, with C as well in constant time without examining all of the data that live inside that cluster.\n00:06:49 All the distance choices that are listed here have that behavior.\n00:06:59 They have sufficient statistics that allow us to do this.\n00:07:08 For example, trivially, if we use the minimum distance, say, then we can just adopt the smaller of the two values, D of A to C and D of B to C, and that will be the minimum distance between the joint group A and B together and C.\n00:07:17 So let's see a few examples.\n00:07:25 As I mentioned, the choice of the dissimilarity measure can dramatically affect the clusters that result from a glomerative clustering.\n00:07:34 For example, here are two data sets in which we'll find, say, five clusters using either single or complete linkage.", "start_char_idx": 3160, "end_char_idx": 7227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa21369e-e5e0-4542-b503-eb8db636dd87": {"__data__": {"id_": "fa21369e-e5e0-4542-b503-eb8db636dd87", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (2) Hierarchical Agglomerative Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OcoE7JlbXvY", "Link": "https://www.youtube.com/watch?v=OcoE7JlbXvY"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99c39f0a-04f0-4911-a220-d028fee40787", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (2) Hierarchical Agglomerative Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OcoE7JlbXvY", "Link": "https://www.youtube.com/watch?v=OcoE7JlbXvY"}, "hash": "2210aef74e0e50cc9d510cffb5a09fb0e540a33ed0558e62d477c05f357ad6a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "019eb502-40bb-444a-aabf-5f475b752494", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Clustering (2) Hierarchical Agglomerative Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OcoE7JlbXvY", "Link": "https://www.youtube.com/watch?v=OcoE7JlbXvY"}, "hash": "5dbe1a6e0601f49349fd6ed96710c34ec49884c6649ad8e1e49d4fb191964118", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1510a1c6-f7de-42b3-b93f-fe7bacca89cc", "node_type": "1", "metadata": {}, "hash": "afe05003f78d9ff8893c6c8e650445d05419d7bb1b3f08a43509ebbb9b45e4e3", "class_name": "RelatedNodeInfo"}}, "text": "00:06:49 All the distance choices that are listed here have that behavior.\n00:06:59 They have sufficient statistics that allow us to do this.\n00:07:08 For example, trivially, if we use the minimum distance, say, then we can just adopt the smaller of the two values, D of A to C and D of B to C, and that will be the minimum distance between the joint group A and B together and C.\n00:07:17 So let's see a few examples.\n00:07:25 As I mentioned, the choice of the dissimilarity measure can dramatically affect the clusters that result from a glomerative clustering.\n00:07:34 For example, here are two data sets in which we'll find, say, five clusters using either single or complete linkage.\n00:07:42 In the first data set here, the clusters we would probably want, looking at this holistically, is two clusters forming these groups.\n00:07:51 The data form two coherent shapes, slightly separated.\n00:07:59 something like this clustering, since the nearest data point to any other is probably along the same wavy curve.\n00:08:09 So merges typically happen within the two clusters that we see visually.\n00:08:19 Complete linkage over here, on the other hand, is unable to find these clusters.\n00:08:29 Since the quality of a grouping is measured by its furthest pair, when we have a fine grouping like, say, two groups here instead of just one, it's much more likely to group this group with this, since its furthest distance is close, than this group with this part of this orange one, since their furthest distances is quite long.\n00:08:39 So complete linkage tends to avoid elongated clusters and merges the data in a way that doesn't respect the similarity that we see in these data.\n00:08:49 In the second data set, on the other hand, complete linkage results look much more appealing than the single linkage results.\n00:08:59 A slightly further separation of a few points here and here has led single linkage to put almost all of the data into one group within slightly separated groups for the smaller sets.\n00:09:06 Whereas in the complete linkage, things are more reasonably clustered.\n00:09:13 Thus, we may want to select our dissimilarity measure to reflect the types and shapes of clusters that we're hoping to get out of this procedure.\n00:09:19 Another common use of agglomerative clustering is to look at arrays of data in a technique called clustergrams.\n00:09:26 In MATLAB, this is found in the bioinformatics toolbox.\n00:09:33 Suppose that we measure the expression levels of a collection of genes across various experimental conditions, such as diseases, or time, or patient identities.\n00:09:39 Then every condition gives us a vector of gene observations.\n00:09:46 Or we could view each gene as a data point.\n00:09:53 We could view each gene as a data point and have a vector of the conditions measured.\n00:09:59 In this image, we'll visualize green as very positive measurements, so increased expression, and red as negative measurements, suppressed expression.\n00:10:05 The unsupervised task is to try to understand this data matrix.\n00:10:11 Which genes behave similar to which?\n00:10:17 Do they behave similarly in all the patients or only some?\n00:10:23 Which patients are similar to which?\n00:10:29 We can perform agglomerative clustering on both the rows here and columns here of the data.\n00:10:35 And then this dendrogram can be used to permute the data into an ordering that reflects and illustrates their grouping pattern.\n00:10:41 Then the overall reordered image reveals patterns.\n00:10:47 For example, these genes are expressed more in this set of conditions, and similarly express less in this set of conditions.\n00:10:53 Thus, the clustergram can help us understand patterns.\n00:10:59 patterns in a data matrix by organizing the rows and the columns into ways that form groups that behave similarly and expose some of these group-based behaviors.\n00:11:08 In summary, agglomerative clustering is a simple procedure for discovering groupings in the data.", "start_char_idx": 6538, "end_char_idx": 10524, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1510a1c6-f7de-42b3-b93f-fe7bacca89cc": {"__data__": {"id_": "1510a1c6-f7de-42b3-b93f-fe7bacca89cc", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (2) Hierarchical Agglomerative Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OcoE7JlbXvY", "Link": "https://www.youtube.com/watch?v=OcoE7JlbXvY"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99c39f0a-04f0-4911-a220-d028fee40787", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (2) Hierarchical Agglomerative Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OcoE7JlbXvY", "Link": "https://www.youtube.com/watch?v=OcoE7JlbXvY"}, "hash": "2210aef74e0e50cc9d510cffb5a09fb0e540a33ed0558e62d477c05f357ad6a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa21369e-e5e0-4542-b503-eb8db636dd87", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Clustering (2) Hierarchical Agglomerative Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OcoE7JlbXvY", "Link": "https://www.youtube.com/watch?v=OcoE7JlbXvY"}, "hash": "fc5a5db77febdbff3c465568e7c0ad32df411b2e43c221d9fad7765c25eb97ca", "class_name": "RelatedNodeInfo"}}, "text": "00:10:17 Do they behave similarly in all the patients or only some?\n00:10:23 Which patients are similar to which?\n00:10:29 We can perform agglomerative clustering on both the rows here and columns here of the data.\n00:10:35 And then this dendrogram can be used to permute the data into an ordering that reflects and illustrates their grouping pattern.\n00:10:41 Then the overall reordered image reveals patterns.\n00:10:47 For example, these genes are expressed more in this set of conditions, and similarly express less in this set of conditions.\n00:10:53 Thus, the clustergram can help us understand patterns.\n00:10:59 patterns in a data matrix by organizing the rows and the columns into ways that form groups that behave similarly and expose some of these group-based behaviors.\n00:11:08 In summary, agglomerative clustering is a simple procedure for discovering groupings in the data.\n00:11:17 Given a way of measuring the distance or the dissimilarity between a pair of clusters, groups of data, we successively merge the closest pair of clusters and update our distances until all the data are in a single cluster.\n00:11:25 We can then visualize the process by drawing a dendrogram, which shows the sequence in which the clusters were merged and how dissimilar they were at the time.\n00:11:34 Analyzing the computational complexity, we found it was a bit over quadratic in m, the number of data.\n00:11:42 We saw an application of this to understanding data matrices called clustergrams, where we cluster both the rows and columns of the data matrix.\n00:11:51 Reordering them to group them by their clusters, these newly ordered data matrix exposes differences\n00:11:59 and similarities, and changes in the feature values across subgroups of data.\n00:12:08 We also saw that the resulting clusters also depend critically on the measure of dissimilarity that we use.\n00:12:16 For example, single linkage will often give long connected clusters, but can result in small isolated clusters as well, while something like complete linkage will prefer more spatially grouped clusters.\n00:12:25 So we need to select a dissimilarity measure that reflects how we think the data should be grouped.", "start_char_idx": 9637, "end_char_idx": 11826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80c5d81d-6c0e-4d6e-b25c-1fca89b5746a": {"__data__": {"id_": "80c5d81d-6c0e-4d6e-b25c-1fca89b5746a", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e0ebf7f-d8c9-4bd4-8a15-d7df331ee735", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "hash": "ceabeba7ff66ad3338e338738e4928c00326686ab573b42b87ce8ca37da8082e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6570c955-9b41-419c-958c-95a57a21c90b", "node_type": "1", "metadata": {}, "hash": "2f8cd862977651471f2c3ea7ae9fbea9201f64c053883386249a1b159e82c024", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Another very popular method for clustering is the k-means algorithm, which we describe next.\n00:00:06 k-means is a simple procedure for finding clusters, which iterates between assigning the data points to clusters and updating the cluster's summarization based on the assigned data.\n00:00:13 In MATLAB, this is available as the k-means function in the Statistics Toolbox.\n00:00:20 For our notation, we'll denote data example i as x sub i.\n00:00:26 Notice I use a subscript here instead of our usual superscript.\n00:00:33 And we'll assume that there are exactly k clusters.\n00:00:40 Each cluster is described by a single center point, mu c.\n00:00:46 So here we have three clusters, mu1, mu2, and mu3.\n00:00:53 And each cluster claims a set of nearby points, here, so that the cluster is effectively defined by closeness to the cluster center.\n00:00:59 We'll also need to think about the assignment of particular data points to clusters.\n00:01:05 So we'll do this with a variable called z, indexed by i, running over the data points.\n00:01:10 So there's one assignment zi for each data point i.\n00:01:15 And z takes on values 1 through k.\n00:01:20 So z indicates which cluster the point is assigned to.\n00:01:25 So if this is point 1, then z1 tells us that this point has been assigned to cluster 1.\n00:01:30 If this is point 2, then z2 equals 3 tells us that that point is assigned to cluster 3, and so on.\n00:01:35 Now, to run k-means, we start by initializing the cluster center's mu to some locations.\n00:01:40 We'll discuss the initialization in more detail later.\n00:01:45 k-means then proceeds by iterating over two steps.\n00:01:50 First, for each data point, we'll find the closest cluster center.\n00:01:55 So zi takes on the value c that minimizes the distance between point xi and the cluster center.\n00:02:00 center, mu C.\n00:02:06 So in this plot, for each data point, we're going to decide which of these two cluster centers to assign each data point to.\n00:02:13 We can visualize that with arrows.\n00:02:20 So for instance, this data point's assignment is to cluster 1, this one also, while this data point's assignment is to cluster 2.\n00:02:26 Then we update the cluster center's mu sub C by setting mu C to the centroid, or mean, of the data that are assigned to cluster C.\n00:02:33 So SC is the set of all data points that have ZI equal to C, MC is their total number, and then mu C is simply the average of those assigned data points.\n00:02:39 Over here in B, we show that process.\n00:02:46 Once these points have been assigned to cluster 1, then cluster 1 is updated to be the centroid of those two points.\n00:02:53 These points are assigned to cluster 2, and mu 2 is moved to be\n00:02:59 centroid of those three points.\n00:03:07 We can now go back and repeat step A, finding the minimizing cluster center, the closest clusters to each data point, and then repeat step B, moving the means to be the mean of the assigned points, and so on.\n00:03:14 This procedure can be viewed as optimizing a cost function over the data, assignments, and cluster centers, which measures how much error is introduced by summarizing data point I with its cluster center that it's assigned to.\n00:03:22 So mu sub zI is the cluster center that's assigned via index zI.\n00:03:29 We measure this just with Euclidean distance.\n00:03:37 Then the algorithm is simply a coordinate descent procedure.\n00:03:44 Step A is minimizing cost function over the assignments z.\n00:03:52 If we fix the cluster center's mu sub c,\n00:03:59 You can see that only one term in the sum depends on each zi, the term corresponding to data point i.\n00:04:08 So if we minimize over zi, we can just check the distances from that point to each of the two clusters and choose the one that's smallest.", "start_char_idx": 0, "end_char_idx": 3778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6570c955-9b41-419c-958c-95a57a21c90b": {"__data__": {"id_": "6570c955-9b41-419c-958c-95a57a21c90b", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e0ebf7f-d8c9-4bd4-8a15-d7df331ee735", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "hash": "ceabeba7ff66ad3338e338738e4928c00326686ab573b42b87ce8ca37da8082e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80c5d81d-6c0e-4d6e-b25c-1fca89b5746a", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "hash": "4c6ad37a76a5fdaf5982eee54deadeb7e71b964bbd7bc96d482d18af35122395", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "896773ec-8662-4e21-b508-aa20884741f4", "node_type": "1", "metadata": {}, "hash": "0e61eea1caeb5aaef2c8ceeea88789c1e299c4dc827775bf42e4f3297beee7fd", "class_name": "RelatedNodeInfo"}}, "text": "00:03:14 This procedure can be viewed as optimizing a cost function over the data, assignments, and cluster centers, which measures how much error is introduced by summarizing data point I with its cluster center that it's assigned to.\n00:03:22 So mu sub zI is the cluster center that's assigned via index zI.\n00:03:29 We measure this just with Euclidean distance.\n00:03:37 Then the algorithm is simply a coordinate descent procedure.\n00:03:44 Step A is minimizing cost function over the assignments z.\n00:03:52 If we fix the cluster center's mu sub c,\n00:03:59 You can see that only one term in the sum depends on each zi, the term corresponding to data point i.\n00:04:08 So if we minimize over zi, we can just check the distances from that point to each of the two clusters and choose the one that's smallest.\n00:04:17 Step b, then minimizes the function over the locations mu.\n00:04:25 If we fix the zis, then the terms that depend on any particular mu, say mu1, are only those terms that have the data assigned to cluster 1, so zi equal to 1.\n00:04:34 Then it's easy to show that the location of mu that minimizes the sum of squared errors is simply the centroid or mean.\n00:04:42 Then, since each step, a and b, is minimizing over some variables given the others, each step is guaranteed to decrease the cost at every step.\n00:04:51 Since the cost is decreasing and bounded below, that means it must converge.\n00:04:59 At some point, the updated means will lead us to the same assignments, which lead us to the same means, and so on.\n00:05:09 Although this procedure is guaranteed to converge, it's not guaranteed to find a global optimum of the cost function.\n00:05:19 In fact, the resulting clustering often depends quite a lot on the algorithm's initialization.\n00:05:29 So in practice, we often work by starting with several randomly chosen initializations and then running the algorithm from each, we get several different clusterings.\n00:05:39 Then we can use the cost C of each clustering to select among our results, and this gives us a better approximation to the global optimum.\n00:05:49 So for example, after one initialization and run, we might end up with this clustering here with cost 223.3. If we initialize differently and then run again, we might find a better clustering here, so slightly lower cost.\n00:05:59 212.6. And after we've run a few more with different random initializations, we might find yet a better clustering here, meaning a lower cost.\n00:06:08 This has cost 167.0. So after several such executions, we can then keep the best according to this cost function C.\n00:06:17 Given the importance of initialization to the results, there's been a fair amount of thought into how to initialize k-means.\n00:06:25 The simplest way is to initialize randomly.\n00:06:34 Usually that's done by choosing k of the data points themselves at random to be the initial cluster centers.\n00:06:42 This ensures that the cluster centers will at least be near some data, since they're actual data points, and large groups of data will probably end up getting one.\n00:06:51 But the drawback is that it may choose two points that are near each other or within the same group of data, such as has happened over here.\n00:06:59 To avoid cluster centers that are too close to each other, one option is to choose faraway points.\n00:07:06 So we could start by one randomly chosen data point, say here, to be our first cluster center.\n00:07:13 Then, we find the data point that's farthest from that data point as the next cluster center, here.\n00:07:19 At each step, we score all the data by their distance to the closest of the already selected points.\n00:07:26 Then, we choose as our next cluster center the data point that's furthest away.\n00:07:33 While this solves the issue of choosing too nearby data, it's also imperfect, as it will often choose outlier data points, so data that are very far away from all the other data.\n00:07:39 These data tend to be unusual in the data set and not very good centers.", "start_char_idx": 2967, "end_char_idx": 6986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "896773ec-8662-4e21-b508-aa20884741f4": {"__data__": {"id_": "896773ec-8662-4e21-b508-aa20884741f4", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e0ebf7f-d8c9-4bd4-8a15-d7df331ee735", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "hash": "ceabeba7ff66ad3338e338738e4928c00326686ab573b42b87ce8ca37da8082e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6570c955-9b41-419c-958c-95a57a21c90b", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "hash": "0bcd1ad69a4eb7661bdd82ec95ba483e5467710ebc5ec2aaf0bdb9704ccc383e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe0b072a-c451-415d-89ae-d4448683d338", "node_type": "1", "metadata": {}, "hash": "8ce9a6bb616fa460aec9fefcc98cf475f374d75cf3a027e80b0871d8eb7f8083", "class_name": "RelatedNodeInfo"}}, "text": "00:06:59 To avoid cluster centers that are too close to each other, one option is to choose faraway points.\n00:07:06 So we could start by one randomly chosen data point, say here, to be our first cluster center.\n00:07:13 Then, we find the data point that's farthest from that data point as the next cluster center, here.\n00:07:19 At each step, we score all the data by their distance to the closest of the already selected points.\n00:07:26 Then, we choose as our next cluster center the data point that's furthest away.\n00:07:33 While this solves the issue of choosing too nearby data, it's also imperfect, as it will often choose outlier data points, so data that are very far away from all the other data.\n00:07:39 These data tend to be unusual in the data set and not very good centers.\n00:07:46 Also, the procedure is not very random.\n00:07:53 So that means that if we run k-means multiple times, we'll often get very similar initializations.\n00:07:59 lot of opportunity to improve on the quality of our overall clustering by starting with a better initialization.\n00:08:08 A nice balance is the k-means plus plus algorithm which was proposed some years ago.\n00:08:17 This technique chooses the next cluster location to be far but random from the already selected locations.\n00:08:25 So like the distance method, we score each point by its distance to the already selected clusters.\n00:08:34 But then instead of choosing the furthest point, we choose the next center randomly with probability proportional to the squared distance.\n00:08:42 So this means that data that are far away here will have higher probability, but also areas of the space that have a lot of data will also tend to have higher probability since we might choose any of those points to be a cluster center.\n00:08:51 So this procedure also has significantly more randomness than the max distance method, which helps us get diverse initializations across runs.\n00:08:59 Once we have a clustering of our data, one common issue we might run into is that we'd like to describe new points that weren't available during the clustering using our cluster centers.\n00:09:07 So these are called out-of-sample data, since they weren't in the data sample that was available during clustering.\n00:09:14 This is actually quite easy for k-means.\n00:09:22 We can just compute the cluster assignment of the new data point without updating that cluster's location.\n00:09:29 So each cluster claims a part of the space that's nearer to that cluster center than any other.\n00:09:37 So this results in a Voronoi tessellation, just like we saw in our nearest neighbor decision boundaries.\n00:09:44 In fact, we can evaluate the out-of-sample assignment using something like our k-nearest neighbor code here.\n00:09:52 After finding the means, mu, using our clustering, we can create a nearest neighbor classifier using those centers and the IDs of the clusters, and then compute the assignment by just predicting in that model.\n00:09:59 Another critical choice we run into in k-means is the value of k, the number of clusters that we try to find.\n00:10:08 So what happens if we try to choose the best value of k for this cost function here?\n00:10:17 Suppose we cluster these data with three clusters, and we do a lot of initializations and find the optimal clustering.\n00:10:25 If we then compare to what happens when we cluster with five clusters, we'll find that we reduce the total cost.\n00:10:34 So here, for example, two of the clusters, this red one has been split into two parts, and this green one has been split into two parts.\n00:10:42 And so the data in these clusters must be closer to their cluster centers than the data in these ones.\n00:10:51 If we increase k still further to say 10 clusters, this continues with the clusters growing ever smaller, and hence the cluster centers being ever closer to their associated...\n00:10:59 data points.\n00:11:05 Thus, the cost function, C, always decreases with larger K.\n00:11:10 So, if K equals M, the number of data points, the cost is actually zero.\n00:11:16 Each data point will be its own cluster center.", "start_char_idx": 6197, "end_char_idx": 10309, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe0b072a-c451-415d-89ae-d4448683d338": {"__data__": {"id_": "fe0b072a-c451-415d-89ae-d4448683d338", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e0ebf7f-d8c9-4bd4-8a15-d7df331ee735", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "hash": "ceabeba7ff66ad3338e338738e4928c00326686ab573b42b87ce8ca37da8082e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "896773ec-8662-4e21-b508-aa20884741f4", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "hash": "ef76f9b2e57a42ad5752eb7684c6ae98beff5e88e63e3c65ea1727590ff114bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "960e6665-7228-48fc-9bdf-0c68d3d3392e", "node_type": "1", "metadata": {}, "hash": "82d03d7bdb858fc283ce2845dd8f188c2abd0cd53221eb5444c5ad96bd68f520", "class_name": "RelatedNodeInfo"}}, "text": "00:10:25 If we then compare to what happens when we cluster with five clusters, we'll find that we reduce the total cost.\n00:10:34 So here, for example, two of the clusters, this red one has been split into two parts, and this green one has been split into two parts.\n00:10:42 And so the data in these clusters must be closer to their cluster centers than the data in these ones.\n00:10:51 If we increase k still further to say 10 clusters, this continues with the clusters growing ever smaller, and hence the cluster centers being ever closer to their associated...\n00:10:59 data points.\n00:11:05 Thus, the cost function, C, always decreases with larger K.\n00:11:10 So, if K equals M, the number of data points, the cost is actually zero.\n00:11:16 Each data point will be its own cluster center.\n00:11:21 We can see this as a model complexity issue.\n00:11:27 Choosing K, like other model selection or hyperparameter choice problems, just can't be accomplished using the training data.\n00:11:32 In fact, in this case, even testing error, using our out-of-sample procedure from a second ago, is not really meaningful, since the assignment of the test error is chosen to be the closest cluster center.\n00:11:38 So, the more cluster centers there are, the closer the nearest one will be.\n00:11:43 A typical solution is to penalize for model complexity.\n00:11:49 So, our total cost is not only the squared error of the data, but plus a term that increases with the number of clusters.\n00:11:54 So, the total error is the squared error plus a complexity term.\n00:12:00 That way, adding more clusters can actually increase the cost if adding them doesn't decrease the squared error enough to compensate for the increased complexity.\n00:12:05 So we visualize that here.\n00:12:10 Red is the squared error, which gets better with k.\n00:12:16 Green is the complexity term, which increases with k.\n00:12:21 And blue is their sum, which gets better as k increases for a while.\n00:12:27 But then, as the squared error becomes near zero, it starts to increase as the complexity keeps going up.\n00:12:32 For example, a common choice for this complexity penalty is the Bayesian Information Criterion, or BIC, penalty.\n00:12:38 So here's a simple version of that penalty.\n00:12:43 BIC is used to penalize the likelihood of a probability model over data.\n00:12:49 So to apply it here, we can interpret the cost function squared error as a likelihood of a multivariate Gaussian with a fixed variance and unknown mean.\n00:12:54 So BIC actually balances the log.\n00:13:00 of the squared error with a term that increases with k times log m over m.\n00:13:10 So what we find is that the penalty here increases with the number of clusters k, but decreases with the number of data m, so that the more data we have, the more clusters we're able to pick.\n00:13:20 So this penalty is a rather simple version of the probabilistic model plus BIC.\n00:13:30 For a more precise and detailed version of this approach, along with tricks for computational efficiency and other things, you can see, for example, a paper called x-means from a KDD conference a few years back.\n00:13:40 In summary, the k-means clustering approach describes each cluster using a single point in the data feature space called the cluster center.\n00:13:50 After initializing these locations, we proceed by updating the assignment of each data point to its closest center.\n00:14:00 Then, moving the cluster center to be as close as possible to all the assigned data.\n00:14:08 This can be viewed as a coordinate descent procedure on a mean squared error criterion.\n00:14:17 However, this optimization process is prone to local optima, so initialization can be very important.\n00:14:25 By starting from several randomized initializations, we can improve our representation by selecting the final clustering that has lowest cost.\n00:14:34 It's also easy to apply the cluster definitions to out-of-sample data by just applying the assignment procedure to new data points.", "start_char_idx": 9514, "end_char_idx": 13519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "960e6665-7228-48fc-9bdf-0c68d3d3392e": {"__data__": {"id_": "960e6665-7228-48fc-9bdf-0c68d3d3392e", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e0ebf7f-d8c9-4bd4-8a15-d7df331ee735", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "hash": "ceabeba7ff66ad3338e338738e4928c00326686ab573b42b87ce8ca37da8082e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe0b072a-c451-415d-89ae-d4448683d338", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}, "hash": "6491cace05de199e9b91dc73fd7ce4708eab90e7ce5f66d0c7a824d53cb16ec8", "class_name": "RelatedNodeInfo"}}, "text": "00:13:40 In summary, the k-means clustering approach describes each cluster using a single point in the data feature space called the cluster center.\n00:13:50 After initializing these locations, we proceed by updating the assignment of each data point to its closest center.\n00:14:00 Then, moving the cluster center to be as close as possible to all the assigned data.\n00:14:08 This can be viewed as a coordinate descent procedure on a mean squared error criterion.\n00:14:17 However, this optimization process is prone to local optima, so initialization can be very important.\n00:14:25 By starting from several randomized initializations, we can improve our representation by selecting the final clustering that has lowest cost.\n00:14:34 It's also easy to apply the cluster definitions to out-of-sample data by just applying the assignment procedure to new data points.\n00:14:42 Choosing the number of clusters K can be very difficult and is a model selection problem, since increasing K always reduces the K-means cost function.\n00:14:51 By adding a complexity penalty like BIC that increases with the number of clusters K, we can try to trade off a relative improvement in the mean squared error and weigh whether it was worth the increase in model complexity from increasing K.\n00:15:00 you", "start_char_idx": 12650, "end_char_idx": 13943, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a9a3b61-d5d9-40d1-a4c4-7c384c5ac200": {"__data__": {"id_": "4a9a3b61-d5d9-40d1-a4c4-7c384c5ac200", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "23870692-93e9-4897-88f1-43397233b337", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "hash": "cb3970fdd3f6e5a6bafc1ee90a6b646ca81473c858810eac70238e830e0559f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d177c318-1fe3-47de-a000-0defd7fba24b", "node_type": "1", "metadata": {}, "hash": "54b47b8e614da573f15febd4bc5ebe3169513fddbd87794962b673046882e5fa", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 Another common and important clustering technique is based on probability density estimation using Gaussian mixture models and a procedure called expectation maximization, or EM, to fit the model parameters.\n00:00:10 If you recall the k-means algorithm, we described each cluster center using just a single point in feature space, the cluster center, and assigned each data point to its nearest cluster.\n00:00:20 But what if our concept of the grouping of the data have groups that overlap in the feature space?\n00:00:30 So then first of all, it's hard to know which assignment is right, since both are plausible.\n00:00:40 And secondly, in k-means, we always use Euclidean distance to the center, but what if our cluster is defined by some non-circular shape?\n00:00:50 So for example, in these data over here, the data can be seen as forming two groups, one of which has a lot of spread in x2 here, and the other of which has a lot of spread in x1.\n00:01:00 one.\n00:01:06 Both are centered at the same place.\n00:01:13 So it's easy mentally to see these as falling into two groups and to cluster these data in one and these data in the other, but something like k-means won't be able to discover that assignment.\n00:01:20 Gaussian mixture models are an extension of the k-means model in which clusters are modeled with Gaussian distributions.\n00:01:26 So we have not only their mean, but also a covariance that describes their ellipsoidal shape.\n00:01:33 So then we can fit the model by maximizing the likelihood of the observed data.\n00:01:40 We do so with an algorithm called EM.\n00:01:46 It'll be very reminiscent of k-means, except it'll assign data to each cluster with some soft probability.\n00:01:53 As a positive side effect, after clustering, we're actually creating a generative model for the data X, a probability model, which means we can do a lot of useful tasks like sampling new examples that we think are like the data that we measured or\n00:02:00 comparing two collections of data, like the training and test set, to see if they differ, or imputing missing data values from some of our data.\n00:02:06 In fact, often k-means is actually viewed as a special case of a Gaussian mixture model to derive some of these benefits.\n00:02:13 A Gaussian mixture model is a useful probability distribution model.\n00:02:20 We begin with several mixture components indexed by c, each of which is described by a Gaussian distribution.\n00:02:26 So each has a mean, mu c, a variance or covariance, sigma c, and a size, pi c.\n00:02:33 For example, in this figure, we have three Gaussian components with means here, here, and here.\n00:02:40 Each of these components has a different variance and a different height or area.\n00:02:46 So the joint probability distribution has been defined by the weighted average of these individual components.\n00:02:53 So pi c.\n00:02:59 times the Gaussian defined by mu c and sigma c.\n00:03:06 We can interpret this joint probability distribution over X in a simple generative way.\n00:03:13 To draw a sample from X, from P of X, we first select one of the components with discrete probability pi.\n00:03:19 So components with large pi are selected more often.\n00:03:26 As in k-means, we'll view this as the assignment of that sample to one of the components and denote it by Z.\n00:03:33 Then given the component assignment Z equals C, we can draw a value from X from the corresponding Gaussian distribution.\n00:03:39 So together these two distributions make a joint model over X and Z.\n00:03:46 Discarding the value of Z gives a sample from the marginal P of X defined above.\n00:03:53 Models like this are sometimes called latent variable models.\n00:03:59 are modeled jointly with an additional variable, z, that we don't get to observe.\n00:04:08 It's hidden.\n00:04:17 The presence of the unknown value of z helps explain patterns in the values of x.\n00:04:25 For example, in this case, groups or clusters.\n00:04:34 Usually, our features are multivariate, even high-dimensional.", "start_char_idx": 0, "end_char_idx": 4016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d177c318-1fe3-47de-a000-0defd7fba24b": {"__data__": {"id_": "d177c318-1fe3-47de-a000-0defd7fba24b", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "23870692-93e9-4897-88f1-43397233b337", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "hash": "cb3970fdd3f6e5a6bafc1ee90a6b646ca81473c858810eac70238e830e0559f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a9a3b61-d5d9-40d1-a4c4-7c384c5ac200", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "hash": "75764640262638040c830ab428b9324e18ff9b7b0a139da38b7f1b64a75c719f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1da52d1b-d143-4d2b-90a3-a1d86f687181", "node_type": "1", "metadata": {}, "hash": "0f9f6b7a4de18c28d2c30e320178a6cf88203b7d349aa9e1d8240f45ccfc0c9e", "class_name": "RelatedNodeInfo"}}, "text": "00:03:33 Then given the component assignment Z equals C, we can draw a value from X from the corresponding Gaussian distribution.\n00:03:39 So together these two distributions make a joint model over X and Z.\n00:03:46 Discarding the value of Z gives a sample from the marginal P of X defined above.\n00:03:53 Models like this are sometimes called latent variable models.\n00:03:59 are modeled jointly with an additional variable, z, that we don't get to observe.\n00:04:08 It's hidden.\n00:04:17 The presence of the unknown value of z helps explain patterns in the values of x.\n00:04:25 For example, in this case, groups or clusters.\n00:04:34 Usually, our features are multivariate, even high-dimensional.\n00:04:42 So, we'll typically use a multivariate Gaussian, which has the same quadratic form, but now involves a vector mean, mu, of length n, the same size as the number of features in a data point x, and an n by n covariance matrix, sigma.\n00:04:51 Recall that if we were given data from a multivariate Gaussian, the maximum likelihood estimate for the model parameters were simply the mean of the data, so the first moment of the data, and the covariance estimate was the mean of the n by n matrices formed by the outer product.\n00:04:59 of X minus mu with itself.\n00:05:07 So this is the centered second moment of the data.\n00:05:14 EM then proceeds iteratively in two steps.\n00:05:22 The first, the expectation or E-step, treats the Gaussian parameters mu, sigma, and pi as fixed.\n00:05:29 Then for each data point, i, and each cluster, c, we compute a responsibility value, R sub i, c, that measures the relative probability that data point xi belongs to cluster c.\n00:05:37 To do this, we just compute the probability of x under model component c, a weighted Gaussian, and normalize by the total over all the values of c.\n00:05:44 So here, we evaluate a data point x under component 1, so pi 1 times the Gaussian defined by mean mu 1 and covariance sigma 1.\n00:05:52 We then also evaluate its probability under component 2.\n00:05:59 So pi 2 times the Gaussian defined by mu 2 and sigma 2.\n00:06:08 If a particular component C is not a very good explanation for x, it will typically have a small RIC value.\n00:06:17 Conversely, if it's by far the best possible explanation for x, it will have RIC approximately equal to 1.\n00:06:25 Here, component 2 is about twice as good an explanation as component 1 for that data point.\n00:06:34 So component 2 gets responsibility two-thirds, and component 1 gets responsibility one-third.\n00:06:42 Practically speaking, RIC is a number of data by number of clusters, so m by k matrix, that sums to 1 over the index C.\n00:06:51 Then, in the second step of EM, the maximization or M step, we fix these assignment responsibilities RIC and update the parameters of the clusters.\n00:06:59 mu, sigma, and pi.\n00:07:07 Then for each cluster C, we update its parameters using an estimate weighted by the probabilities RIC, as if it observed some fraction RIC of data point I.\n00:07:14 So cluster C sees some total number of data points MC that's the sum of these soft memberships, or fractional weights assigned to cluster C.\n00:07:22 Then pi C is just this value normalized by the total number of data M.\n00:07:29 So this is the fraction of data point probabilities that's assigned to cluster C.\n00:07:37 The weighted mean, mu C, is just the weighted average of the data.\n00:07:44 So each point Xi is given weight RIC, and we divide by the total MC.\n00:07:52 So if a point Xi was poorly explained by some cluster C, its RIC will be small.\n00:07:59 and that point won't influence this average very much.\n00:08:07 But on the other hand, if C was the best explanation for X, then RIC will be large, and Xi will influence the mean more.", "start_char_idx": 3316, "end_char_idx": 7085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1da52d1b-d143-4d2b-90a3-a1d86f687181": {"__data__": {"id_": "1da52d1b-d143-4d2b-90a3-a1d86f687181", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "23870692-93e9-4897-88f1-43397233b337", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "hash": "cb3970fdd3f6e5a6bafc1ee90a6b646ca81473c858810eac70238e830e0559f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d177c318-1fe3-47de-a000-0defd7fba24b", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "hash": "5e7a2f1cb1f221394e61cc00e2cf22292abb222fdcc2e68d650824c205c4aaa9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1654acd9-de14-4d37-957b-4c5aae69b2cb", "node_type": "1", "metadata": {}, "hash": "e0ef494fbd81b50fac788181ce85d6bcfcccaadfea16ad7491b12be21ee3c321", "class_name": "RelatedNodeInfo"}}, "text": "00:07:14 So cluster C sees some total number of data points MC that's the sum of these soft memberships, or fractional weights assigned to cluster C.\n00:07:22 Then pi C is just this value normalized by the total number of data M.\n00:07:29 So this is the fraction of data point probabilities that's assigned to cluster C.\n00:07:37 The weighted mean, mu C, is just the weighted average of the data.\n00:07:44 So each point Xi is given weight RIC, and we divide by the total MC.\n00:07:52 So if a point Xi was poorly explained by some cluster C, its RIC will be small.\n00:07:59 and that point won't influence this average very much.\n00:08:07 But on the other hand, if C was the best explanation for X, then RIC will be large, and Xi will influence the mean more.\n00:08:14 The covariance is similarly just a weighted average of the n by n matrices formed by taking the outer product of Xi minus its cluster C's mean.\n00:08:22 Again, they're weighted by RIC, so that if Xi is a strong member of cluster C, this weight will be nearly 1.\n00:08:29 If Xi is not very well explained by cluster C, then it won't enter into this average very much.\n00:08:37 It's straightforward to prove, although I won't do it here, that these iterations strictly increase the log likelihood of the model, increasing its fit to the data.\n00:08:44 The log likelihood is just the log probability of the data points under the mixture model.\n00:08:52 So it's a sum over data points of log of the probability.\n00:08:59 P of X from before, where that probability is a mixture of Gaussians.\n00:09:06 In fact, EM is interpretable as a form of coordinate ascent, just like our previous k-means algorithm.\n00:09:13 So thus, we're again guaranteed to converge.\n00:09:19 However, convergence here won't typically be as abrupt as it was in k-means.\n00:09:26 So in practice, one usually stops once the parameters or the likelihood objective have stopped changing very much.\n00:09:33 Note that like in k-means, convergence is not guaranteed to be to a global optimum.\n00:09:39 So we may have to start from several initializations and use the log likelihood to select the best.\n00:09:46 One quick point, the result of this algorithm is a description of a collection of clusters, so centers and covariances and so on, and soft membership probabilities for each data point.\n00:09:53 If we want to identify our data points with a single cluster, like in k-means, we could actually just choose the most probable cluster, so the cluster C that has largest value RIC for data points.\n00:09:59 point i.\n00:10:07 Just like in k-means, the view of clusters is easily applied to new out-of-sample data points as well.\n00:10:14 So, these points can be given either a soft cluster membership RIC using an E-step or a hard membership assignment ZI by, again, taking the largest RIC.\n00:10:22 Also like k-means, selecting the number of clusters is important and can't be done using the cluster data themselves.\n00:10:29 So, like k-means, one option is to use a complexity penalty in our score.\n00:10:37 An analog to the mean squared error cost in k-means is the negative log-likelihood of the data in our mixture model.\n00:10:44 And we can then include a complexity penalty like BIC just like before.\n00:10:52 Alternatively, since the Gaussian mixture model is a true probability model, we can use the log-likelihood of a validation or test set to assess the model fit as well by evaluating the mixture probability on the new data points.\n00:10:59 Here's a demonstration of YAM, courtesy of some slides by Professor Poric Smith here at UC Irvine, run on data measuring properties of patients' red blood cells.\n00:11:07 The data include both a group of normal patients and a group of anemic patients, but the diagnosis is actually hidden from us, so we only see the feature measurements themselves.\n00:11:14 Probably you can see the two groups of patients already in the data.", "start_char_idx": 6328, "end_char_idx": 10243, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1654acd9-de14-4d37-957b-4c5aae69b2cb": {"__data__": {"id_": "1654acd9-de14-4d37-957b-4c5aae69b2cb", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "23870692-93e9-4897-88f1-43397233b337", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "hash": "cb3970fdd3f6e5a6bafc1ee90a6b646ca81473c858810eac70238e830e0559f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1da52d1b-d143-4d2b-90a3-a1d86f687181", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "hash": "b8e981c9fe9d5fbbeabd10f4e655bfe649732c0fbc76ba7fab15e0e8c4277efa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca32de04-7f44-4fa7-8588-65fb97e84e95", "node_type": "1", "metadata": {}, "hash": "d155a337d5c6a4506d3e8203773ae6b799f7c0a209639ae5ea2b18101a6af13d", "class_name": "RelatedNodeInfo"}}, "text": "00:10:37 An analog to the mean squared error cost in k-means is the negative log-likelihood of the data in our mixture model.\n00:10:44 And we can then include a complexity penalty like BIC just like before.\n00:10:52 Alternatively, since the Gaussian mixture model is a true probability model, we can use the log-likelihood of a validation or test set to assess the model fit as well by evaluating the mixture probability on the new data points.\n00:10:59 Here's a demonstration of YAM, courtesy of some slides by Professor Poric Smith here at UC Irvine, run on data measuring properties of patients' red blood cells.\n00:11:07 The data include both a group of normal patients and a group of anemic patients, but the diagnosis is actually hidden from us, so we only see the feature measurements themselves.\n00:11:14 Probably you can see the two groups of patients already in the data.\n00:11:22 Up here, with high red blood cell volume and high hemoglobin are the normals, and then this long dispersed group with lower volume and lower hemoglobin are the anemics.\n00:11:29 Let's see how YAM identifies these two groups automatically.\n00:11:37 We initialize our two clusters randomly.\n00:11:44 In this case, they're actually quite similar, but the means and covariances of the green cluster and the red cluster are slightly different, and that'll actually be enough.\n00:11:52 During the E-step, these points over here will be slightly more probable under the red model than the green, while these\n00:11:59 These points will be about equally explained, and these points will be slightly more probable under green.\n00:12:07 Given these soft assignments or responsibilities, when we update our model parameters to compute the weighted mean and covariances, we'll find that the red model shifts slightly to better accommodate the data down here that were given higher probability under the red model.\n00:12:14 And similarly, green shifts to better accommodate the data that were given slightly higher probability under its component.\n00:12:22 Then re-computing the probabilities in the E-step makes these data even more likely under red and these data even more likely under green.\n00:12:29 Another M-step evolves the components even further away to help explain their respective data points better.\n00:12:37 Repeating, we see that the clusters are continuing to separate.\n00:12:44 By iteration 10, they're explaining very different groups of data.\n00:12:52 And by iteration 15, red is almost entirely...\n00:12:59 for the dispersed data and the green cluster for the tight grouping of normals.\n00:13:08 Eventually this process converges, the models cease to change very much, and we can stop.\n00:13:17 Note that the model doesn't know which of these two groups is normal or anemic, just that it can find two distinct groups or clusters in the data that behave differently, and a description of what the data in each cluster look like.\n00:13:25 If we look at the log likelihood of the data over the course of the algorithm, we find that each iteration strictly increases the log likelihood score.\n00:13:34 So after about 15 iterations, the score plateaus and doesn't increase any further, indicating, again, that the model has converged.\n00:13:42 The EM algorithm is actually quite general and can be used for many models or problems involving partially observed data.\n00:13:51 From this viewpoint, here the complete data are the feature values and the cluster assignments, but unfortunately for us the assignments to the eye are more important.\n00:13:59 missing.\n00:14:05 Then EM corresponds to first computing the distribution over the hidden variable zi, given the current parameters of the model, and then maximizing the complete log-likelihood over x and z together in expectation over the distribution of z.\n00:14:11 In Gaussian mixture models, this gives us this elegant algorithm that we saw, where we compute soft assignments for each data point and then plug those soft assignments into the maximum likelihood estimates.\n00:14:17 But in more general models, it may be procedurally a bit more complex.\n00:14:23 Two simple alternatives are often used when EM is difficult to implement.\n00:14:29 There's a stochastic version of EM or a hard version of EM.\n00:14:35 In stochastic EM, instead of taking the expectation over z, we just sample its value and then fix it.", "start_char_idx": 9362, "end_char_idx": 13725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca32de04-7f44-4fa7-8588-65fb97e84e95": {"__data__": {"id_": "ca32de04-7f44-4fa7-8588-65fb97e84e95", "embedding": null, "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "23870692-93e9-4897-88f1-43397233b337", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "hash": "cb3970fdd3f6e5a6bafc1ee90a6b646ca81473c858810eac70238e830e0559f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1654acd9-de14-4d37-957b-4c5aae69b2cb", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}, "hash": "3870f41bae4bce848b3b569097c5c6e71422a4dfb89b8d19a3ac7fb1863b662f", "class_name": "RelatedNodeInfo"}}, "text": "00:13:59 missing.\n00:14:05 Then EM corresponds to first computing the distribution over the hidden variable zi, given the current parameters of the model, and then maximizing the complete log-likelihood over x and z together in expectation over the distribution of z.\n00:14:11 In Gaussian mixture models, this gives us this elegant algorithm that we saw, where we compute soft assignments for each data point and then plug those soft assignments into the maximum likelihood estimates.\n00:14:17 But in more general models, it may be procedurally a bit more complex.\n00:14:23 Two simple alternatives are often used when EM is difficult to implement.\n00:14:29 There's a stochastic version of EM or a hard version of EM.\n00:14:35 In stochastic EM, instead of taking the expectation over z, we just sample its value and then fix it.\n00:14:41 In hard EM, we select the most probable value and fix it.\n00:14:47 Often it's easier to work with a fixed value of z than maximizing the expectation.\n00:14:53 This process of\n00:14:59 of plugging a value into z is often called imputing the value of z.\n00:15:05 Both variants, stochastic and hard EM, are quite similar to regular EM.\n00:15:10 Hard EM, as you might imagine, is a little less smooth in its optimization, since the best value of z might change discontinuously.\n00:15:16 And it's often prone to more local optima, since once we have a hard assignment, we'll move the parameters to explain it, which then reinforces that hard assignment.\n00:15:21 Hard EM is very closely related to k-means with this best assignment update.\n00:15:27 Stochastic EM is less prone to local optima, but has more randomness built in, making it harder to gauge convergence.\n00:15:32 In summary, Gaussian mixture models are a useful and flexible class of probability distributions.\n00:15:38 And here, we're using them for clustering.\n00:15:43 They explain the distribution of the data x using a latent variable framework.\n00:15:49 So we posit that there's some hidden grouping, or clustering, that explains a lot of the variation in the data.\n00:15:54 Our model then contains a.\n00:15:59 latent membership or assignment variable Zi, and then the actual feature values Xi are Gaussian given that cluster assignment Zi. We can learn the parameters of a Gaussian mixture model using expectation maximization, or EM.\n00:16:09 EM applied to Gaussian mixture models has a simple and elegant form.\n00:16:19 We just iterate between computing soft membership or assignment probabilities, which we call the responsibilities, R sub IC, and updating the parameters of the mixture model components, so the cluster centers, covariances, and so on, using these soft memberships.\n00:16:29 This procedure is guaranteed to strictly increase the log likelihood, decrease the negative log likelihood of the model on training data, which ensures that it's convergent.\n00:16:39 But it's non-convex, so it may have local optima, which makes initialization potentially very important.\n00:16:49 We also discussed how we can set the number of clusters using either a complexity penalty like BIC, just like we did in k-means, or...\n00:16:59 Since the Gaussian mixture model is a true probability model over X, we can actually use the log-likelihood score on held-out validation or test data as well.", "start_char_idx": 12898, "end_char_idx": 16194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6aae3b6c-3dc7-4cb8-9129-97840dfeb038": {"__data__": {"id_": "6aae3b6c-3dc7-4cb8-9129-97840dfeb038", "embedding": null, "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b60632f-2c61-4d3c-9c95-65eb8fb0d6b0", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "hash": "2254ba4ee2e3b1ce33301f727e7b0d8130cace5d57d72f40c4786b59dee67414", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a016e878-2178-44c7-aa02-30bc4ce2865a", "node_type": "1", "metadata": {}, "hash": "bd4b46a2ee415c798c7ed36a4cc26b7a6ccb0ce0621b83c22b1d7e3f936a58a4", "class_name": "RelatedNodeInfo"}}, "text": "00:00:00 In these slides, we'll discuss the unsupervised learning problem of dimensionality reduction and, in particular, linear dimensionality reduction techniques like principal components analysis and the singular value decomposition.\n00:00:12 The idea of dimensionality reduction is that we have some high-dimensional data like images or text or, as an example, we'll use here a vector of stock price data.\n00:00:24 Although these data are described in a very high-dimensional way using many, many values per example, we'd like to describe them in a simpler way that will allow us to compare them or plot them or explore them in some simpler fashion.\n00:00:36 So as an example, suppose we had a vector of 500 stock price changes in the S&P 500.\n00:00:48 Although this is typically described with 500 real values per day, we might imagine that there are lots of structures in the way these data change, so we might not need to describe them using all of them.\n00:01:00 all 500 numbers.\n00:01:06 If we wanted to compress the data and describe it using, say, only a few values, we might try to explore this structure.\n00:01:12 So for example, elements of this vector tend to change together.\n00:01:18 Perhaps we only need to use a few of these values to describe most of the changes in the prices.\n00:01:24 For example, we might start by describing the overall average change in all the prices, since we expect that typically all stocks rise or fall together.\n00:01:30 Then we might describe changes in subcategories of the data.\n00:01:36 For instance, we might group many stocks as being in tech sector, and we might describe that those stocks rose by some amount or fell by some amount.\n00:01:42 Might have other stocks in a different sector, like manufacturing.\n00:01:48 We might describe them as rising or falling together as well.\n00:01:54 Although in this example, I've used human notions of what stocks might be grouped together, we'd like to access this in.\n00:02:00 an automatic way.\n00:02:10 So we'd like to design an algorithm that can tell which elements of this data vector are changing together and exploit that to describe them using only a smaller number of real values.\n00:02:20 As usual, we'll illustrate this with a very small feature space so that we can plot it.\n00:02:30 So we'll imagine we have data with two real values, x1 and x2, and we'd like to compress these to a smaller dimensional space, so say one real value, z.\n00:02:40 If we were to communicate only one real value, zi, for each data point, xi, in two-dimensional space, we could do so by first communicating a model, f, that would tell our receiver how to convert the single real number, z, into a vector, x.\n00:02:50 Then we could communicate just that model, and then only one real value per data point, which would essentially compress our data space.\n00:03:00 by about a factor of 2.\n00:03:06 Here, we'll consider linear functions f, so meaning z is a simple multiplier of a vector v that's in the same space as the data x.\n00:03:12 So we'll have a vector v described with two values, v1 and v2.\n00:03:18 And our receiver will reconstruct a point x just by multiplying whatever scalar we send z times this model vector v.\n00:03:24 So v will be the same for all the data points.\n00:03:30 We'll communicate it once, and then each data point will have a value z that tells us the closest point on that vector to the original point x.\n00:03:36 So here's a picture.\n00:03:42 All these blue data points on the left are the original data.\n00:03:48 v is the vector that we've sent to our receiver as our model to convert scalar values to vectors x.\n00:03:54 And when we send a value z for.\n00:04:00 each data point, each data point will send the value of z that will be closest on this v vector, and our receiver will reconstruct a point that's exactly along this v vector, but still hopefully very close to the data point xi.\n00:04:10 So xi will be approximated with zi times this vector v.", "start_char_idx": 0, "end_char_idx": 3975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a016e878-2178-44c7-aa02-30bc4ce2865a": {"__data__": {"id_": "a016e878-2178-44c7-aa02-30bc4ce2865a", "embedding": null, "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b60632f-2c61-4d3c-9c95-65eb8fb0d6b0", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "hash": "2254ba4ee2e3b1ce33301f727e7b0d8130cace5d57d72f40c4786b59dee67414", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6aae3b6c-3dc7-4cb8-9129-97840dfeb038", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "hash": "0756eef8a5d1fae509f371935254627b6e015c25f2a4d40ffffa6df95385b3ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ad27dcc-228e-44bc-8b42-e3fe0a0a7244", "node_type": "1", "metadata": {}, "hash": "c9b1f64099e8fbb54eade5001071ea275d3cbbe6fce8bcaf62411a675a57e41a", "class_name": "RelatedNodeInfo"}}, "text": "00:03:24 So v will be the same for all the data points.\n00:03:30 We'll communicate it once, and then each data point will have a value z that tells us the closest point on that vector to the original point x.\n00:03:36 So here's a picture.\n00:03:42 All these blue data points on the left are the original data.\n00:03:48 v is the vector that we've sent to our receiver as our model to convert scalar values to vectors x.\n00:03:54 And when we send a value z for.\n00:04:00 each data point, each data point will send the value of z that will be closest on this v vector, and our receiver will reconstruct a point that's exactly along this v vector, but still hopefully very close to the data point xi.\n00:04:10 So xi will be approximated with zi times this vector v.\n00:04:20 So what vector v and values a can we send that will most closely approximate the data point xi?\n00:04:30 Suppose we measure our error between the true data point xi and the reconstructed point a times v in a least squares sense.\n00:04:40 So we'd like to choose the vector of values a, one for each data point, and the vector v, v1, v2, that minimizes the mean squared error between the overall data points xi and the reconstructed data points a times v.\n00:04:50 If we fix v, choosing a is fairly simple.\n00:05:00 a is simply the projection of the point x onto the vector v.\n00:05:08 So if we have a data point here, and we choose the closest point in the least square sense to v, that will be the best value of a.\n00:05:17 So the main problem is to choose v, and then given v, we can compute the a's fairly easily.\n00:05:25 We'd like to choose v to minimize the residual variance, the variance in the errors that we make, given that we've chosen a in this optimal way.\n00:05:34 Equivalently, if we're trying to minimize the variance off the vector v, v will be the direction that maximizes the variance of the data within its space.\n00:05:42 Although in this example, I'm choosing a single vector v and a single scalar a, if we have a higher dimensional x, we can think of generalizing this to a more complicated model with more coefficients.\n00:05:51 So if x had, say, three or four dimensions, we might describe it as a linear combination of two vectors, v and w.\n00:05:59 each in four space, and two coefficients a, b.\n00:06:08 So this would compress the four-dimensional vector x into a two-dimensional vector a, b, and a model Vw with two components each.\n00:06:17 So, how can we choose the vector V that maximizes the variance of the data within that space?\n00:06:25 We can do it by exploiting some of the geometry of the Gaussian distribution.\n00:06:34 Recall that the covariance matrix sigma decomposed the variance into a set of directions and scales.\n00:06:42 In particular, the covariance matrix sigma could be written using an eigendecomposition as a set of eigenvectors U and a set of eigenvalues lambda.\n00:06:51 Geometrically, the eigenvectors determine the axes of an ellipse that surrounded the data and captured its variance, and the eigenvalues lambda determine the scale of each of the.\n00:06:59 these eigenvectors, the scale of the variance of the data in that direction.\n00:07:06 So, if we want the direction of maximal variance, we can simply take the eigendecomposition of the covariance.\n00:07:13 That'll tell us the directions of variation, and we can look at the scale of these numbers, lambda 1, lambda 2, and so forth.\n00:07:19 The one with the largest eigenvalue will be the direction with the largest spread.\n00:07:26 This technique is called principal components analysis, and is a classic and powerful way of doing linear dimensionality reduction.\n00:07:33 The basic procedure is as follows.\n00:07:39 First, we typically subtract the mean from each data point, so that it becomes data with a zero mean.\n00:07:46 Typically for PCA, we also scale each dimension by its variance.", "start_char_idx": 3214, "end_char_idx": 7095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ad27dcc-228e-44bc-8b42-e3fe0a0a7244": {"__data__": {"id_": "3ad27dcc-228e-44bc-8b42-e3fe0a0a7244", "embedding": null, "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b60632f-2c61-4d3c-9c95-65eb8fb0d6b0", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "hash": "2254ba4ee2e3b1ce33301f727e7b0d8130cace5d57d72f40c4786b59dee67414", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a016e878-2178-44c7-aa02-30bc4ce2865a", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "hash": "d36bccdf6f29932709d615ed354c0d7e985679d7683f061a369102d60f1eba1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9447632b-a8ce-4682-add9-87728802e4cf", "node_type": "1", "metadata": {}, "hash": "8cf451e72138ef23bdc27340e13e78062a440cebd7d832b7634f7f901f85494d", "class_name": "RelatedNodeInfo"}}, "text": "00:06:59 these eigenvectors, the scale of the variance of the data in that direction.\n00:07:06 So, if we want the direction of maximal variance, we can simply take the eigendecomposition of the covariance.\n00:07:13 That'll tell us the directions of variation, and we can look at the scale of these numbers, lambda 1, lambda 2, and so forth.\n00:07:19 The one with the largest eigenvalue will be the direction with the largest spread.\n00:07:26 This technique is called principal components analysis, and is a classic and powerful way of doing linear dimensionality reduction.\n00:07:33 The basic procedure is as follows.\n00:07:39 First, we typically subtract the mean from each data point, so that it becomes data with a zero mean.\n00:07:46 Typically for PCA, we also scale each dimension by its variance.\n00:07:53 This ensures that the principal component directions that we find will be not influenced by the magnitude or scale of.\n00:07:59 of the variable, meaning the units that that variable was measured in.\n00:08:08 So after we've centered it and perhaps scaled it, we compute the covariance matrix of the data in the usual way, the empirical covariance.\n00:08:17 Then we take the eigen decomposition of this covariance matrix, giving us a set of eigenvectors, V, and a set of eigenvalues, D.\n00:08:25 If we sort these in terms of the magnitude of their eigenvalues in order, we'll find the largest eigenvector, V1, we'll call the first principal component.\n00:08:34 The eigenvector with the second largest eigenvalue, we'll call V2, the second principal component, and so on.\n00:08:42 In MATLAB, this is fairly straightforward.\n00:08:51 We can compute the empirical mean and subtract it from the data, compute the covariance matrix, S, and take its eigendecomposition using EIG, pulling out the k largest eigenvectors.\n00:08:59 If your matrix is very large, if your data have very high dimension, this EIG function may be very inefficient, and it may be better to use an incremental search for the eigenvectors like EIGs. It would be slightly more efficient.\n00:09:11 Given the principal directions, V1 through Vk, we can then find the coefficients of those by simply projecting each point onto that linear subspace.\n00:09:23 Alternatively, instead of taking the eigendecomposition of the covariance matrix, we can take the singular value decomposition of the data matrix.\n00:09:35 You can think of this as just an alternative method to calculate the same eigenvectors and also the coefficients that go along with them to reconstruct the data.\n00:09:47 The idea here is we take the data matrix X and we decompose it into its singular value decomposition, which is a matrix decomposition into a matrix U, a diagonal matrix S, and another matrix V, where U...\n00:09:59 and V are unitary transforms, meaning they're orthogonal and orthonormal bases.\n00:10:11 To see how this connects to the eigen decomposition of the covariance matrix, consider the covariance matrix X transpose X.\n00:10:23 If we multiply X transpose by X, we simply get V times S transpose, since S is diagonal, that's just S, U transpose, U, but U transpose times U, because they're orthonormal, is just the identity matrix, then S, then V transpose.\n00:10:35 So U transpose X becomes V S V transpose, and denoting S times S, the product of two diagonal matrices as another diagonal matrix D, we see that this becomes an eigen decomposition, the eigen decomposition of the covariance matrix, where the matrix V are the eigenvectors of that covariance matrix.\n00:10:47 In addition to finding the eigenvectors...\n00:10:59 eigenvectors V of the covariance, this process also finds the coefficients.\n00:11:11 So that data point Xi is actually reconstructed by this coefficient, Ui1 S11, times the eigenvector V1, plus this coefficient, Ui2 times S22, times the second eigenvector, and so forth.\n00:11:23 So this process not only finds the eigenvectors, but also simultaneously finds the coefficients, A1, A2, and so forth, that will be used to reconstruct the data.", "start_char_idx": 6293, "end_char_idx": 10326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9447632b-a8ce-4682-add9-87728802e4cf": {"__data__": {"id_": "9447632b-a8ce-4682-add9-87728802e4cf", "embedding": null, "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b60632f-2c61-4d3c-9c95-65eb8fb0d6b0", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "hash": "2254ba4ee2e3b1ce33301f727e7b0d8130cace5d57d72f40c4786b59dee67414", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ad27dcc-228e-44bc-8b42-e3fe0a0a7244", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "hash": "c33775e793c9cee0836fa374a85cee42174bc57d4e359b98cd26a60aac046503", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2bc4904-34da-4966-95ef-cfd943627715", "node_type": "1", "metadata": {}, "hash": "a33dc6a5d7f1071f4f09238f24549995ad0bad6fe49e8def1e7ad4d83aefba09", "class_name": "RelatedNodeInfo"}}, "text": "00:10:35 So U transpose X becomes V S V transpose, and denoting S times S, the product of two diagonal matrices as another diagonal matrix D, we see that this becomes an eigen decomposition, the eigen decomposition of the covariance matrix, where the matrix V are the eigenvectors of that covariance matrix.\n00:10:47 In addition to finding the eigenvectors...\n00:10:59 eigenvectors V of the covariance, this process also finds the coefficients.\n00:11:11 So that data point Xi is actually reconstructed by this coefficient, Ui1 S11, times the eigenvector V1, plus this coefficient, Ui2 times S22, times the second eigenvector, and so forth.\n00:11:23 So this process not only finds the eigenvectors, but also simultaneously finds the coefficients, A1, A2, and so forth, that will be used to reconstruct the data.\n00:11:35 Another equivalent way of envisioning this decomposition is that the data matrix, an n by d matrix, is being composed into a product of low rank matrices.\n00:11:47 If we keep only the first k eigenvectors, this is approximating the data matrix as the product of one matrix that's of the same number of rows as the data, so one row per.\n00:11:59 example, data point, but only with K.\n00:12:08 Another diagonal matrix, S, and finally a matrix that's of the same dimensionality as the data, but has only K rows.\n00:12:17 So these are the set of eigenvectors that tell us how to reconstruct the data, and the product of U times S gives us a set of K coefficients for each data point.\n00:12:25 You can see that this is a simpler description of the original data matrix, since suppose we had 100 data points of dimension 10, then this matrix would have 1,000 entries.\n00:12:34 Now suppose we kept only the first three principal components.\n00:12:42 We would have 100 by 3, so 300, plus 3 entries, plus 10 times 3, or 30 total real numbers to communicate, or about 333 compared to 1,000.\n00:12:51 Each data point can then, instead of being described in a detail,\n00:12:59 space be simply described in a k-dimensional space where that space, the coefficients of the reconstruction, give the position in that space.\n00:13:06 So each data point corresponds to a row of U times S which gives k real-valued numbers.\n00:13:13 Let's look at a quick example of representing images of faces and finding a low-dimensional representation of those faces.\n00:13:19 So I have a large number of image patches, each of which is a 24 by 24 image patch.\n00:13:26 So it has 576 real values, one per pixel.\n00:13:33 I can think of this patch of image as a single observation vector just by moving in raster scan order over the image and assembling a 576 length vector of real numbers.\n00:13:39 Then I can create a data matrix by running through each example.\n00:13:46 Each example becomes a row of our data matrix X where each row contains 576 real values.\n00:13:53 Then I perform\n00:13:59 PCA by taking the singular value decomposition of this data matrix, which decomposes it into a product U times S, forming an N by K matrix, and another matrix V that's a K by D matrix.\n00:14:09 The principal component directions are this matrix V, so we have K principal components, each of which is a D dimensional vector.\n00:14:19 Because this is a 576 length vector, we can interpret it again as an image patch just by reforming it into a 24 by 24 patch.\n00:14:29 If we do this, the mean of that data matrix looks something like this, so it looks fairly like an average face, and these are the directions that we find as the principal components.\n00:14:39 So we'll be describing each image as a mean, plus some coefficient times this image, some coefficient times this image, a third coefficient times that one, a fourth times this one, and so forth.\n00:14:49 This representation can be used to construct fairly low dimensional representations of the image.\n00:14:59 patch that do a reasonable job of reconstructing the image, or approximating the image.", "start_char_idx": 9516, "end_char_idx": 13456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2bc4904-34da-4966-95ef-cfd943627715": {"__data__": {"id_": "a2bc4904-34da-4966-95ef-cfd943627715", "embedding": null, "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b60632f-2c61-4d3c-9c95-65eb8fb0d6b0", "node_type": "4", "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "hash": "2254ba4ee2e3b1ce33301f727e7b0d8130cace5d57d72f40c4786b59dee67414", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9447632b-a8ce-4682-add9-87728802e4cf", "node_type": "1", "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}, "hash": "e7862015f2d99bd19cf7a0837638bdaa050d86f34ffce01d0206bcc5bd371728", "class_name": "RelatedNodeInfo"}}, "text": "00:14:09 The principal component directions are this matrix V, so we have K principal components, each of which is a D dimensional vector.\n00:14:19 Because this is a 576 length vector, we can interpret it again as an image patch just by reforming it into a 24 by 24 patch.\n00:14:29 If we do this, the mean of that data matrix looks something like this, so it looks fairly like an average face, and these are the directions that we find as the principal components.\n00:14:39 So we'll be describing each image as a mean, plus some coefficient times this image, some coefficient times this image, a third coefficient times that one, a fourth times this one, and so forth.\n00:14:49 This representation can be used to construct fairly low dimensional representations of the image.\n00:14:59 patch that do a reasonable job of reconstructing the image, or approximating the image.\n00:15:07 Take these two examples, two faces from our data set, and consider the approximations that would be reconstructed using only four coefficients in the first four principal components, or 50 coefficients in the first 50 principal components.\n00:15:14 At four coefficients, both faces look fairly similar.\n00:15:22 They look fairly generic.\n00:15:29 It's not a particularly good reconstruction.\n00:15:37 But by the time we describe each image using 50 coefficients, we've gotten a pretty reasonable approximation.\n00:15:44 So although they're blurry, you can see the orientation of the face, their general shape, the position of features like the nose and mouth, and other characteristics like the collar and neckline and so forth of the image.\n00:15:52 To interpret, although sending this image in its complete form would require me to send almost 600 real values, I can get away with it.\n00:15:59 with sending only 50 real values, along with the principal components that are the same for all of the data points, and reconstruct an image that's reasonably close to the original face patch.\n00:16:11 So I've managed to describe this high dimensional data point using a feature space that's only about 10% of the size of the original data.\n00:16:23 In summary, dimensionality reduction is a technique of unsupervised learning for finding a new representation of our data points that are smaller and uses fewer coefficients than our original data representation.\n00:16:35 Here, we're considering linear dimensionality reduction, where we decompose it into a collection of basis vectors, called the principal components, and a set of coefficients that multiply those to reconstruct an approximation of each data point.\n00:16:47 A linear approach to this is called principal components analysis, and corresponds to an eye\n00:16:59 decomposition of the covariance matrix of the data, an alternative way of calculating it is the singular value decomposition of the original data matrix, which gives us both the eigenvectors of the covariance, the principal components, and also the coefficients that should multiply those to reconstruct our data matrix.\n00:17:18 We saw one example of this in face images of reconstructing 600-dimensional data that represent faces using a fairly small number of coefficients and basis vectors.", "start_char_idx": 12584, "end_char_idx": 15785, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"2a73d618-a0fb-47fa-a08f-2e07785d5d8f": {"node_ids": ["766a02fc-7e51-4158-976f-d61779c530f0", "3002568c-7732-4f1b-8307-bab5cdf97c06"], "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}}, "3c5e79d0-be54-4b95-b6b4-2da42b62a478": {"node_ids": ["32c73033-a333-4db8-977f-b09051c4bdff", "a061ad5d-9101-4671-ba12-ae7fa7d15d7b", "25a3b692-f483-4101-83e4-1d758d859434"], "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}}, "221b314e-7748-4d7f-a195-4bec511e846c": {"node_ids": ["b7d4f842-9ceb-41ec-9eaa-3df2fc47b14e", "72c351e9-ce3b-450f-b1a2-97ebfc0c207b", "fd80c443-2757-4b89-8b1b-520870a1da44"], "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}}, "fe380da3-000a-4742-9443-b5eaa35e5042": {"node_ids": ["2902509e-8b06-48fe-beb9-1c5670be2ee8", "d42ebddf-5646-4be6-9f84-fbd85492d3cd"], "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}}, "503fe2e6-0597-4404-b5b3-103cb2632808": {"node_ids": ["d829baee-5699-4398-bb50-885e65afa561", "93057f03-ade9-4a36-9ea6-7df0a4d1106d", "10370ebc-6551-4687-b1a9-4b18cc2af6c8"], "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}}, "88aa12d8-ab91-4cb0-8e27-c2c81c318ca4": {"node_ids": ["c517231c-9e19-46b5-8bf2-85e8ae8a05c9", "b422bef4-3051-4a00-97a9-414a2d12bd9f", "d9ff7407-55fa-47c6-8a43-11b28d350374"], "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}}, "7c770b46-ce8a-4e25-87ba-fc85fb1ec708": {"node_ids": ["a03953c5-d372-4086-a721-98efd52726e7", "47382864-30b1-4572-9790-abb13713f7a0"], "metadata": {"title": "Transcripts/CS273A/Introduction (1) AI & Machine Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qPhMX0vb6D8", "Link": "https://www.youtube.com/watch?v=qPhMX0vb6D8"}}, "a29404b2-5ccb-4a7a-975c-a1efa702e8f5": {"node_ids": ["bae5941d-5d4b-423c-9892-b48c472bd4c3", "4ae56c85-2dd6-48f6-8109-552daf5e7dd0", "010cd150-1ff8-42c4-9717-d51f5140971b"], "metadata": {"title": "Transcripts/CS273A/Introduction (2) Data and Visualization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=h4kB3r9uaVw", "Link": "https://www.youtube.com/watch?v=h4kB3r9uaVw"}}, "f92fa923-f522-44ef-b8c6-dd3f4169548d": {"node_ids": ["dad14003-d5da-40a7-b80b-46d1abc32ff4", "7845ea4a-c84a-460e-852e-098c51ebb80e", "9d53e701-d53a-4475-a745-ea08ec4c58a9"], "metadata": {"title": "Transcripts/CS273A/Introduction (3) Supervised Learning.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=0dP-YtYZREU", "Link": "https://www.youtube.com/watch?v=0dP-YtYZREU"}}, "11814e8b-07cf-4a83-9007-407e89485cc9": {"node_ids": ["9cf60006-b177-4589-83fb-9339f1c67a43", "35ea3d05-4902-42cd-8839-4438234fd328"], "metadata": {"title": "Transcripts/CS273A/Introduction (4) Complexity and Overfitting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=VZuKBKd4ck4", "Link": "https://www.youtube.com/watch?v=VZuKBKd4ck4"}}, "0f7f690f-5604-4e6a-b4f4-2e1b47e7fc3d": {"node_ids": ["5839e6a5-eaa0-431c-830d-d14b4a8e8bbc", "5bb0498f-0b64-450b-b834-436966d126b5"], "metadata": {"title": "Transcripts/CS273A/Nearest Neighbor (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=FMCZXFRqZvg", "Link": "https://www.youtube.com/watch?v=FMCZXFRqZvg"}}, "e26737a4-a6e0-44fb-a472-ccbc0760a1e6": {"node_ids": ["1fbe23d1-72ad-4516-9c9b-c65f3b22feb8", "9058178c-17c3-404e-b039-0bcf2cd0b162", "e3689ead-3f6f-4393-8383-1ac59c5fbe8e"], "metadata": {"title": "Transcripts/CS273A/Nearest neighbor (2) k-nearest neighbor.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ZWygMcenuWM", "Link": "https://www.youtube.com/watch?v=ZWygMcenuWM"}}, "44025363-bc17-4899-ab4b-c6b259f41354": {"node_ids": ["65ec63f8-fd43-4e05-8f2d-82873c9eb735", "3434e657-16e4-435a-a482-0d46aaf64678", "72b188f8-cdff-43b0-9b9c-77022445092c", "875d49c4-5294-4efa-a5c9-d540ddbb1170", "957ee664-a093-4a05-9dde-66c96ebded45", "8c9b09bb-01bf-49d0-a246-c0da6c9d834e", "f7e0e680-382b-4c10-99b3-1cddbe2f63a5"], "metadata": {"title": "Transcripts/CS273A/Review Probability.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=8kmkFO2lBlQ", "Link": "https://www.youtube.com/watch?v=8kmkFO2lBlQ"}}, "ea3fb7fe-a2b7-4d16-a969-fc97e37328ee": {"node_ids": ["e035af73-c916-403a-b52f-9683e76d0f68", "8c180bb5-173e-47f4-96ff-f5d0479a1d51", "7b3d0846-c6bd-4b63-812d-ff0d33d2d739", "4213762f-d15f-480f-b4ce-c48c0fa7a006"], "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ivBSZZyaRHY", "Link": "https://www.youtube.com/watch?v=ivBSZZyaRHY"}}, "bd947843-9be4-4dc4-84df-ca245ad4e9a6": {"node_ids": ["54936631-28a6-416c-84b9-8e7dae428d9c", "7ed96378-5b61-47b0-99ef-cc6a0c4f1172", "485e98b9-2f0e-47cd-b50a-93425a1710b7", "c828cb87-65fd-41f7-b117-64e0e8cc6c6d", "33fed07a-e4f5-4204-a3e1-616c099f2f56"], "metadata": {"title": "Transcripts/CS273A/Bayes Classifiers (2) Naive Bayes.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=54wfthrhwLQ", "Link": "https://www.youtube.com/watch?v=54wfthrhwLQ"}}, "11490156-0a15-4f83-b095-50e657d53446": {"node_ids": ["292e19a9-c5ed-444c-867c-2bac8cfbc822", "3552e3f4-2f63-4406-935c-99c458285615", "7826277c-9417-4413-9b9a-8e6bc35f1f42", "7ef4dca3-1aaf-4e15-a582-37b278e9a79c"], "metadata": {"title": "Transcripts/CS273A/Multivariate Gaussian distributions.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=eho8xH3E6mE", "Link": "https://www.youtube.com/watch?v=eho8xH3E6mE"}}, "4cbd714d-1dc7-4772-8616-809dd14dfa8c": {"node_ids": ["c576900f-0b9f-4cb8-bea1-87f320e17447", "149361b9-81a6-4f31-acd8-7ea4a8304c64"], "metadata": {"title": "Transcripts/CS273A/Linear regression (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hiOQDsdOZ7I", "Link": "https://www.youtube.com/watch?v=hiOQDsdOZ7I"}}, "a9bd8381-f1ca-4988-9f69-905b663fcd63": {"node_ids": ["5fa99543-adb9-4c6b-9e25-8a327c1ed2af", "1dca1e48-c968-4bba-940e-f5b9d6a7019b", "ff47ee7d-d1cb-4b0e-af68-2d9c63118990", "292270a7-ab53-42ef-83d9-fc1d0bee6e04"], "metadata": {"title": "Transcripts/CS273A/Linear regression (2) Gradient descent.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WnqQrPNYz5Q", "Link": "https://www.youtube.com/watch?v=WnqQrPNYz5Q"}}, "bfd7ab5a-ef0c-467c-81d7-3b0f04b82a7e": {"node_ids": ["deb239fa-0355-44af-831e-3e846245ebfe", "f738bc61-20ba-4adb-a594-ea9e61a81636", "ef3d2650-b6d0-4a66-81e4-acfed19b2906"], "metadata": {"title": "Transcripts/CS273A/Linear regression (3) Normal equations.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=N4d_9GQ9QFc", "Link": "https://www.youtube.com/watch?v=N4d_9GQ9QFc"}}, "fdf25942-8649-42aa-8b88-e46636dafd71": {"node_ids": ["4b6c04fb-57ae-431f-98be-966ce6ac433b", "5149e57b-559b-4ff0-911e-1d550e65289b"], "metadata": {"title": "Transcripts/CS273A/Linear regression (4) Nonlinear features.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=py8QrZPT48s", "Link": "https://www.youtube.com/watch?v=py8QrZPT48s"}}, "d68407ec-67f4-4efb-b3c6-b4fe4a6be662": {"node_ids": ["8f0bee1b-c68d-453d-bc81-ecbf8ac787f3", "f3b790f1-080e-42c1-b2f2-f119dfffbd4d"], "metadata": {"title": "Transcripts/CS273A/Linear regression (5) Bias and variance.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=djRh0Rkqygw", "Link": "https://www.youtube.com/watch?v=djRh0Rkqygw"}}, "824186cb-a303-46b4-b44b-aff2c4e05976": {"node_ids": ["2132b4c6-ed46-4f94-a04f-d653467bb4fb", "4b2225aa-9426-4650-87a8-ae02e6fa24ac", "46c8cbc6-b40e-40b0-ba93-31e8ed3a7153"], "metadata": {"title": "Transcripts/CS273A/Linear regression (6) Regularization.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sO4ZirJh9ds", "Link": "https://www.youtube.com/watch?v=sO4ZirJh9ds"}}, "5968ec8e-fdf7-4303-9893-a11fcf769a1d": {"node_ids": ["c60c1d2e-2cd0-4364-8584-fdb48fc08239", "415f5947-a43c-4c6d-a488-21de9d7b8b11"], "metadata": {"title": "Transcripts/CS273A/Decision Trees (1).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=WOOTNBxbi8c", "Link": "https://www.youtube.com/watch?v=WOOTNBxbi8c"}}, "81b9bd71-f7de-42af-b524-7a932c901501": {"node_ids": ["78d4fb6d-a10d-4ebb-a3f1-eb836fe2f061", "0951632a-9f6d-4adc-bc0b-893acbdce41c", "0c904fd0-7790-46f6-9dc0-0b7bdcecb61e", "e2d9ba94-8a2b-42e1-be77-0852437937b9", "59503582-ad79-4685-a003-1c1b84ae1ac5", "0959206a-ee7b-4d33-bf00-5337825e9cae", "7b566d6c-6e13-4b35-aebb-dcdf7504edb0", "525598fe-a218-4f11-bdf1-901fa523417c"], "metadata": {"title": "Transcripts/CS273A/Decision Trees (2).txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=U2A-g6-Prrs", "Link": "https://www.youtube.com/watch?v=U2A-g6-Prrs"}}, "ea172faa-1bf1-48bf-8470-6e4ea7dde048": {"node_ids": ["9418fddb-01f5-4bec-893d-75a157c6819a", "d2e44479-2d65-43dd-ad9a-6e63d3182a0b", "cc45f623-2cc3-4c80-9e99-5a571a8452e0", "710f7a71-1614-472f-ac06-d6bffc79e888"], "metadata": {"title": "Transcripts/CS273A/Linear classifiers (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=hAnfnc2dWBs", "Link": "https://www.youtube.com/watch?v=hAnfnc2dWBs"}}, "1b42695f-4068-4da2-a2b3-de44867b9bb2": {"node_ids": ["4c1cee8b-7db8-4372-bb94-78cf12e59d4e", "29b48206-89f1-45c4-97a3-4b36a0326e56", "f7ca81fd-a58f-40cf-9035-9aff931fdc00", "47d8a91e-cafd-4ea2-a255-12d43902aa3d", "febc4676-b683-43a7-a209-fe09ef9c44b9", "f597d9da-7cc4-4d85-9c00-8ee8edad52df", "0f5201a0-675a-4249-98e3-4b7759737873"], "metadata": {"title": "Transcripts/CS273A/Linear classifiers (2) Learning parameters.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=slbDMIHnT2c", "Link": "https://www.youtube.com/watch?v=slbDMIHnT2c"}}, "7994668a-0e4f-4d5d-8d53-d7ba6586a3cc": {"node_ids": ["eeae53ca-af32-4d06-9d59-3fe6d83a7314", "0e9b6eed-cafd-4bed-bef3-cb2deba8e295", "331c113b-be24-437e-b21c-fed3276897f7", "e67e2ce0-127b-4b8a-8f30-feaadd7db1b0", "eb718235-0b11-4f04-a710-9767beed3cf7"], "metadata": {"title": "Transcripts/CS273A/VC Dimension.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=puDzy2XmR5c", "Link": "https://www.youtube.com/watch?v=puDzy2XmR5c"}}, "b36b93ab-3813-4bdc-a4b0-bbfd558bccf5": {"node_ids": ["fec9bd48-de84-4037-a5db-1a1c7f9123dd", "d8f8a750-e266-4933-9267-129ee24223df", "19a53875-5b16-43fb-98c7-5918680dbf33"], "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (1) Linear SVMs, primal form.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=IOetFPgsMUc", "Link": "https://www.youtube.com/watch?v=IOetFPgsMUc"}}, "ab07d92b-bcb6-405e-b831-d099703e2bdc": {"node_ids": ["9c107ad4-d38a-4023-826e-e4edd65f7002", "bd04d83f-084a-431d-a00a-5ad7437ebc61", "64768b67-abfe-4b47-90b7-7cca631471a5", "dbf64994-6b87-40dd-aa98-60fdbbc9fb92"], "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (2) Dual & soft-margin forms.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=1aQLEzeGJC8", "Link": "https://www.youtube.com/watch?v=1aQLEzeGJC8"}}, "df21632a-c358-487b-8023-0ec7e7add3dd": {"node_ids": ["7d3ea0b0-1025-4fa4-9738-82d1d6c64ae7", "3496f612-a1bc-4b22-96f7-9d314615e8af", "45577703-9eba-4055-a6d3-b8a99e089027", "9a958742-7e86-474a-b13d-f0f00d11a708"], "metadata": {"title": "Transcripts/CS273A/Support Vector Machines (3) Kernels.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OmTu0fqUsQk", "Link": "https://www.youtube.com/watch?v=OmTu0fqUsQk"}}, "ba613405-9475-4938-ae68-036aa597a1c3": {"node_ids": ["02a8e608-a26f-47b3-a592-8c954e86ee29", "ee1a19b4-8c93-4840-a75e-2312c9f57e69", "27fe44aa-0e3b-47e7-bf07-0f04f9f0258e", "2f75a080-1144-4618-b51b-cc74e0b550f1"], "metadata": {"title": "Transcripts/CS273A/Neural Networks (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=bH6VnezBZfI", "Link": "https://www.youtube.com/watch?v=bH6VnezBZfI"}}, "847416df-5686-4eaf-8108-2a52e706903d": {"node_ids": ["10ad2bfa-9f83-44fe-a81d-e2353aa6ae38", "cfb9428d-3e32-4477-99fe-ca200021d2b1", "29793935-f7ab-4e57-bf87-c7be91afdbb1", "a601ba43-885c-4927-b04f-68311704d0c4"], "metadata": {"title": "Transcripts/CS273A/Neural Networks (2) Backpropagation.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6RUwfKNdaV0", "Link": "https://www.youtube.com/watch?v=6RUwfKNdaV0"}}, "b1b1131c-be94-4192-bc82-af8e247fa58c": {"node_ids": ["91d648f3-73ff-4350-9ad8-6b56bc5e4913", "65206850-f9ff-4992-af30-2965d3f30b20"], "metadata": {"title": "Transcripts/CS273A/Ensembles (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Yvn3--rIdZg", "Link": "https://www.youtube.com/watch?v=Yvn3--rIdZg"}}, "1376bd02-1d92-4d41-b9d9-92483544b484": {"node_ids": ["6abb9c2b-add7-466a-b907-b02bba4487e3", "636b11df-74cf-4d00-bbfb-25b4e7eac215", "b56a6812-1b91-4a46-9acd-2c3f7f90ff33", "3cc89a71-8e1c-44fd-89a1-ce324ef9e448", "b85dfca6-5620-4e73-baa9-fbc07b6d888f"], "metadata": {"title": "Transcripts/CS273A/Ensembles (2) Bagging.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=Rm6s6gmLTdg", "Link": "https://www.youtube.com/watch?v=Rm6s6gmLTdg"}}, "dcf19e6f-6bf6-4a72-b5f3-42ec9228d639": {"node_ids": ["8091657c-ac4d-4c16-818f-84580dab4ebd", "67f3c380-8ea5-42bf-8248-8c0982dd0211", "948d7d48-3fe7-4199-9a8d-386661131ac8"], "metadata": {"title": "Transcripts/CS273A/Ensembles (3) Gradient Boosting.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=sRktKszFmSk", "Link": "https://www.youtube.com/watch?v=sRktKszFmSk"}}, "fea08087-d98d-44fa-9415-8d1580ba13a2": {"node_ids": ["ce941a0f-87c1-4433-9aba-839c5a227b4d", "c10ea66d-6baf-4b42-8175-c8f215105655", "a7bacbd6-3f66-4c29-9421-d29e69ec3d4d", "e6441edd-7f7b-4260-8b90-b76f45044ae0", "59c66e4b-2a26-4bd1-b138-879e17a2e4bc", "b3e495b1-dec6-4d60-b7bf-ca0ddd27402d"], "metadata": {"title": "Transcripts/CS273A/Ensembles (4) AdaBoost.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=ix6IvwbVpw0", "Link": "https://www.youtube.com/watch?v=ix6IvwbVpw0"}}, "08bf9903-4e29-4ce8-8e26-10d4e97702ef": {"node_ids": ["9063cc09-9b45-497c-8fe9-e8845bbef073", "b150b699-b9b9-48ea-8904-98c3ef4576cf"], "metadata": {"title": "Transcripts/CS273A/Clustering (1) Basics.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=6R16reLVl3I", "Link": "https://www.youtube.com/watch?v=6R16reLVl3I"}}, "99c39f0a-04f0-4911-a220-d028fee40787": {"node_ids": ["97feb81a-b5b5-48fb-a956-b72c28aaf1e6", "019eb502-40bb-444a-aabf-5f475b752494", "fa21369e-e5e0-4542-b503-eb8db636dd87", "1510a1c6-f7de-42b3-b93f-fe7bacca89cc"], "metadata": {"title": "Transcripts/CS273A/Clustering (2) Hierarchical Agglomerative Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=OcoE7JlbXvY", "Link": "https://www.youtube.com/watch?v=OcoE7JlbXvY"}}, "9e0ebf7f-d8c9-4bd4-8a15-d7df331ee735": {"node_ids": ["80c5d81d-6c0e-4d6e-b25c-1fca89b5746a", "6570c955-9b41-419c-958c-95a57a21c90b", "896773ec-8662-4e21-b508-aa20884741f4", "fe0b072a-c451-415d-89ae-d4448683d338", "960e6665-7228-48fc-9bdf-0c68d3d3392e"], "metadata": {"title": "Transcripts/CS273A/Clustering (3) K-Means Clustering.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=mfqmoUN-Cuw", "Link": "https://www.youtube.com/watch?v=mfqmoUN-Cuw"}}, "23870692-93e9-4897-88f1-43397233b337": {"node_ids": ["4a9a3b61-d5d9-40d1-a4c4-7c384c5ac200", "d177c318-1fe3-47de-a000-0defd7fba24b", "1da52d1b-d143-4d2b-90a3-a1d86f687181", "1654acd9-de14-4d37-957b-4c5aae69b2cb", "ca32de04-7f44-4fa7-8588-65fb97e84e95"], "metadata": {"title": "Transcripts/CS273A/Clustering (4) Gaussian Mixture Models and EM.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=qMTuMa86NzU", "Link": "https://www.youtube.com/watch?v=qMTuMa86NzU"}}, "5b60632f-2c61-4d3c-9c95-65eb8fb0d6b0": {"node_ids": ["6aae3b6c-3dc7-4cb8-9129-97840dfeb038", "a016e878-2178-44c7-aa02-30bc4ce2865a", "3ad27dcc-228e-44bc-8b42-e3fe0a0a7244", "9447632b-a8ce-4682-add9-87728802e4cf", "a2bc4904-34da-4966-95ef-cfd943627715"], "metadata": {"title": "Transcripts/CS273A/PCA, SVD.txt", "category": "Video Transcription", "URL": "https://www.youtube.com/watch?v=F-nfsSq42ow", "Link": "https://www.youtube.com/watch?v=F-nfsSq42ow"}}}}