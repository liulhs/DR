00:00:00 కొన్నము పొండి ప్రిశేప్పరిందిలో వండికీగోండు కొన్నము ప్రిశేప్పరిని మార్చి సంవత్సరాన్ను ఒక పెట్లతేలు అద్భారి ప్రేషాగ్సి�
00:01:00 Okay.
00:01:06 Okay.
00:01:13 Okay.
00:01:20 Okay.
00:01:26 Okay.
00:01:33 Okay.
00:01:40 Okay.
00:01:46 Let's get started.
00:01:53 Today I'm going to...
00:02:00 review the problems of your quiz one and those problems cover the topics that are really important for your midterm exam on Wednesday.
00:02:06 So my advice is that please make sure you understand those five problems completely.
00:02:13 And if you have any questions while I'm going to cover them, please let me know.
00:02:20 Okay.
00:02:26 So let's first take a look at the first problem.
00:02:33 In this problem, you are asked to draw a roofline graph for the following two processors.
00:02:40 And also you need to specify the transition point.
00:02:46 Basically, you need to calculate the arithmetic intensity for both of the processors.
00:02:53 So for this problem, the key point we need to understand is the
00:02:59 memory bond and compute bond.
00:03:05 So you already remember the problem, okay.
00:03:11 I will move on to the next slide.
00:03:17 So that's the big graph from our lecture notes.
00:03:23 And okay, yeah, you can see.
00:03:29 So initially, since there isn't too much computations, the overall performance is bonded by memory.
00:03:35 So as we can see, the straight line, the blue lines here.
00:03:41 And at one moment, from that moment on, we can see the overall performance is bonded by the compute power.
00:03:47 And after that point, so the overall performance keeps a constant.
00:03:53 And also from our lecture notes, we know that the overall.
00:03:59 overall performance denoted by G-flops per second is the minimum of the two metrics here.
00:04:08 The top one is the peak performance denoted by G-flops per second, and the bottom one is related to the memory part.
00:04:17 So it's arithmetic intensity multiplied by the peak bandwidth denoted by the gigabytes per second.
00:04:25 Until you understand those two metrics and also the transition point from memory bunk to compute bunk, then the actual computation is pretty straightforward.
00:04:34 In order to get the arithmetic intensity, I just use the first as an example.
00:04:42 We just need to divide the peak performance by the peak bandwidth.
00:04:51 So for the first processor, we can get the overall arithmetic intensity is about 17.5 flops per byte.
00:04:59 And you should also mark the AI in the graph.
00:05:07 So any questions regarding the first problem so far?
00:05:14 Okay, so another way to interpret this problem is that like we need to make sure like you understand which is memory bound and which is compute bound as I just described.
00:05:22 And you should also be responsible to illustrate in the graph at which area corresponds to memory bound and which area maps to the compute bound.
00:05:29 So that's the whole point of this roofline model analysis.
00:05:37 Yeah, this one is relatively straightforward if you review those materials from the lecture notes and it should be easy for you to solve.
00:05:44 Okay.
00:05:52 Now let's move on to the second problem.
00:05:59 This one asks you to differentiate between convolution operation and cross-correlation operation.
00:06:06 It's more like a concept problem, right?
00:06:13 Concept question.
00:06:19 So, as we know, the convolution first needs to kind of flip, flip the image and then do the multiplication.
00:06:26 Well, for the cross-correlation, we simply multiply those two terms, the image and the filter.
00:06:33 I will add a note.
00:06:40 In our actual CNN model, the convolution functions more like cross-correlation.
00:06:46 To extend this a little bit, as is also shown on the post on Piazza, you should also be responsible for understanding those DNN taxonomy.
00:06:53 And also, different...
00:07:00 settings of CNN and DNN models, how many layers and what are those nodes, then you should be clear about, so at each layer, what computation you need to do, for example.
00:07:10 We may not directly ask this kind of straightforward question, but you need to make sure you understand how this end-to-end workflow works.
00:07:20 So far, any questions regarding the second problem?
00:07:30 Okay.
00:07:40 Do you need to know DNN taxonomy, or just like how DNN works in general?
00:07:50 Different components in terms of DNN, right, they have different layers, different operations, so here we only test you on convolution and cross-correlation, but there are many other, for example, activation functions, you need to make sure you understand both the definitions and also the actual
00:08:00 computation part right?
00:08:06 Yeah.
00:08:13 So it's still like the basic concepts.
00:08:20 Okay.
00:08:26 Okay.
00:08:33 Log scale of performance, you mean the...
00:08:40 I think for this plot, it is in the log...
00:08:46 It's already under the operation of log, right?
00:08:53 Because to take in the log, we simply simplify the notation, right?
00:09:00 So 1g means 10 to the power of 9, for example, we just simply mark 9 in our y-axis, for example.
00:09:10 We just take the numerical value, the exponential part.
00:09:20 So I think here we already marked, so there's a straight line here.
00:09:30 So in this graph, we already assume it is processed by this logarithmic operation.
00:09:40 So in your explanation, you should explicitly mark whether you use the raw values as your input x-axis and y-axis, or you already take the logarithmic part of your original output.
00:09:50 Yeah, okay.
00:10:00 If no question from question two, we can look at the third problem.
00:10:10 So in this problem, the key topic is about computation of dimensions of different outputs in CNN convolutional operation.
00:10:20 So we test here, there is like a 1D convolution, we have the input lens is 3, filter lens is 11, and stride is 1.
00:10:30 And we ask, what is output lens?
00:10:40 And second is, what if we want the output lens to be the same as the input lens?
00:10:50 And you need to elaborate on your answer.
00:11:00 So, the key point regarding this problem is the top, like, formula we need to at least memorize, right, since we are a closed-book test.
00:11:12 So it's also from our lecture notes, obviously.
00:11:24 So in this formula, we want to find the relationship between the input feature map, the dimension of input feature map, and dimension of output feature map, given certain filter size, strike size, and also the padding size.
00:11:36 Specifically, in this problem, we, for the first part, we don't, we have no padding operation, and we can see that, if you go back to the previous one, the input lens is three less than the filter lens.
00:11:48 So if we directly apply the convolution operation, so the output lens is zero, because the input lens is smaller.
00:12:00 smaller than the filter size.
00:12:06 And for the second one, we need to apply this formula by putting all those values in this equation, we can get our final padding size is 5.
00:12:13 To make sure our final output length is also 3, which is the same as the input length.
00:12:19 So far, any questions?
00:12:26 In this question part 1, when I saw this question, at that time I thought maybe the output length can be 1 for partings.
00:12:33 So therefore, by having the input length same as the filter length, I can get the output of...
00:12:39 So the first part is, what is the output length?
00:12:46 So I mean, for me, as an interpretation, I thought that maybe I can say the output length is 3.
00:12:53 
00:12:59 output length is 1, if I do padding of 4 on each side, but later on, you can verify that it is actually 0.
00:13:07 Output length is 0.
00:13:14 But I mean, depending on, since padding was not mentioned, so we can assume padding and give an answer, right?
00:13:22 So you need to assume in this way, right?
00:13:29 If you assume padding, we can add some default padding, then the padding size is a kind of hyperparameter you need to decide.
00:13:37 If you think padding is equal to 4 in your case, then you can get, for example, output length is 1.
00:13:44 But why not choose other padding size?
00:13:52 So for the second problem, if the padding size is equal to 5, then we get output length is equal to 3.
00:13:59 So, yeah, once you, before you make an assumption, so make sure if the assumption is kind of universal, right?
00:14:06 Otherwise, you need to make justification why you choose padding size equal to four instead of other values, right?
00:14:13 So don't think too complicated, because for the first one, just look at one sentence, right?
00:14:19 So no padding at all, and since input length is smaller than the filter length, so just zero output.
00:14:26 We cannot do the actual convolution part.
00:14:33 Okay.
00:14:39 Any questions?
00:14:46 Okay.
00:14:53 For question four, the problem is that we are given one image of three channels and one filter of
00:14:59 three channels, and then we are asked to come up with a solution to calculate the convolution.
00:15:11 It's still a 2D convolution here, f times image, filter times image, by only using matrix multiplication.
00:15:23 In order to approach this problem, as soon as we see matrix multiplication, we need to immediately think about Taupe's matrix, right?
00:15:35 Because we want a way to convert convolution operation into matrix multiplication, which is shown in a diagram, right?
00:15:47 From the lecture slides, we put, I want to highlight one thing.
00:15:59 In the lecture notes, we put image at the left hand side and the filter on the right hand side.
00:16:11 In this notation, we can convert our image into a complex matrix form by extending each block of images to be processed.
00:16:23 So I assume you are already familiar with the operation of a top list matrix, right?
00:16:35 Because you already, you just finished the homework regarding several different variations of top list matrix.
00:16:47 So in this way, we simply put our flattened image part on each block, three by three blocks.
00:16:59 is extended to one by nine row.
00:17:06 And the number of rows in this intermediate representation is bounded by the total number of the overall size.
00:17:13 I would say the overall size of the output feature map.
00:17:19 In this case, our output feature map is supposed to be five by five.
00:17:26 So the overall total elements is 25.
00:17:33 So we have 25 rows in term of this matrix.
00:17:39 And the number of columns is the size of the filter, which is three by three, which is nine here.
00:17:46 Similarly, we can also extend our three by three filter into a nine by one column vector here.
00:17:53 Another.
00:18:00 The next part that we want to test is the understanding of multiple channels.
00:18:10 Here I just show one channel case, right?
00:18:20 Since we have three channels in total, the easier way is to kind of stack the second channel and third channel after the first channel part.
00:18:30 If we can see, if we put image in this way, image is on the left hand side, filter is on the right hand side.
00:18:40 And if we cut the image by every four columns, we can see those four are from the first channel, the middle part is from the second channel, and the last four columns are from the third channel.
00:18:50 So make sure you understand how to transform top list matrix in the one channel case and how to extend.
00:19:00 one channel to three channels.
00:19:05 Yeah, this is pretty important from both your quiz and your programming assignment.
00:19:10 Okay.
00:19:15 So for this question, when you want to do a matrix operation, you want to make just one, like one matrix for each operation rather than split it up into one for each channel?
00:19:20 Is that how?
00:19:25 We put three channels into one matrix, one matrix.
00:19:30 Oh. Yeah.
00:19:35 And does it matter if you do row-major or column-major?
00:19:40 Or is that just by the complexity of the structure?
00:19:45 And that is row-major order, column-major order is more implementation-oriented, if you think in this way.
00:19:50 So here we conceptually, yeah, to describe how different pixels inside the image are going to put in the matrix.
00:19:55 So we.
00:20:00 don't consider the bottom storage layout of this matrix.
00:20:06 Yeah, so you just implement it into the home of the transpose.
00:20:12 That's when we decided it's.
00:20:18 Yeah, at this level, we don't need to consider row major order, column major order, yes.
00:20:24 Can we also do y equal to, I mean, it does before and with an image afterwards, or it has to be this order?
00:20:30 Yeah, that's the one I want to highlight again, right?
00:20:36 So from our lecture notes, it's image times filter.
00:20:42 But this problem, if you look at carefully, it is as filter times image, the opposite.
00:20:48 So actually, my solution posted here is not a final result.
00:20:54 We need to put the filter into a row.
00:21:00 vector and transpose the image to make sure the columns of filter, if you put it in the row vector, it would be 12, right?
00:21:07 Number of columns of filter is 12, and number of rows of the image, if you transpose this, is also 12.
00:21:15 Yeah, you need to ask the transposed version of both.
00:21:22 Yeah, it's also shown in the solution part, so post it online.
00:21:30 Okay.
00:21:37 Then for this, there's a conception that if you have more, if you have more than one filter, so basically more than one output channel, that would just be, it goes, you go downwards, right?
00:21:45 If you add more, yeah, if you have more filters, so if you increase the number of output channels, add more filters means for a specific image, we need to apply several filters to a specific block.
00:21:52 That means we need to kind of...
00:22:00 Replicate, right?
00:22:03 Replicate the number of image blocks.
00:22:07 Yeah.
00:22:11 So how would that change the image?
00:22:15 What do you think about this problem?
00:22:18 If you want to, example.
00:22:22 The filter on the right-hand side, do you need to change that thing?
00:22:26 So if they happen to...
00:22:30 Yeah, filter, this one is for like one filter.
00:22:33 If you want to apply the second one.
00:22:37 It'd be the next column, right?
00:22:41 The next column, yeah.
00:22:45 So any questions regarding question four?
00:22:48 For one of the next columns, shouldn't it be 18 plus one, then?
00:22:52 Because it's basically C that is increasing.
00:22:56 Two filters means that Z is two.
00:23:00 in the row there will be 18 numbers and the column will have 18 cross 1, so when you multiply you will essentially get the same result, you just divide it in the whole column right, it should be 81.
00:23:30 I see, I see your point, let me take a quick look, if we have, yeah your interpretation is that we need to, yeah multiple channels you already understand right, this is multiple channel case, you are asking multiple filters, correct?
00:24:00 of the solution you did.
00:24:05 You mean for this one?
00:24:10 Oh, for this one, since we have three channels, since we have three channels, we just need to stack the second channel and the third channel after the first channel.
00:24:16 If we put the image on the left-hand side.
00:24:21 Is that clear?
00:24:27 Okay, yeah, if you have any further questions, we can ask offline.
00:24:32 Okay.
00:24:38 Yeah, so from this problem, so make sure you understand how the Turbos matrix operates.
00:24:43 For this problem, can you just annotate what is the dimension size of image and for image matrix, what is the dimension size?
00:24:49 How to calculate dimension size?
00:24:54 And for the weight vector.
00:25:00 also weight matrix, how to calculate the dimension thing.
00:25:06 Dimension that is, for example, in case of image, image is equal to R into S.
00:25:12 R, and maybe the columns are equal to P into Q into, can you make this calculate?
00:25:18 It would be easy.
00:25:24 Oh, you mean to come up with a generalized version of topos matrix dimension?
00:25:30 Yes.
00:25:36 Yeah, since it's not there, I mean, from the lecture notes, right?
00:25:42 Yeah.
00:25:48 Yeah, so I think for the naive topos and columns, the dimension is, all both have, all both have the same value.
00:25:54 What about for the topos and columns?
00:26:00 If you think about it, the row dimension is output times output, the column dimension is kernel times kernel.
00:26:06 That's how you get 25 by 9.
00:26:13 And for multiple input and output channels?
00:26:20 In fact, if you calculate a channel, that would be that.
00:26:26 I'm just telling you as an example.
00:26:33 So if you think about it, in one window of convolution, you've got a single element, right?
00:26:40 So that thing is a dot product, right?
00:26:46 So in a matrix multiplication, one row and a column here, what do they do?
00:26:53 A dot product, right?
00:27:00 So you just want to convert the dot product in your one window to one line and one column here.
00:27:04 So that's the rationale behind it.
00:27:08 So in the first row, that will be the first window, right?
00:27:12 So the first window, they have three channels, right?
00:27:16 Each channel has nine elements, right?
00:27:20 So how long will be this one?
00:27:24 How many columns will be there here?
00:27:28 Nine times three, yes, 27, that's 27, right?
00:27:32 And you have how many windows?
00:27:36 It will be how many rows.
00:27:40 So it's clear here?
00:27:44 And also, this is just a case for just one filter, right?
00:27:48 So if you have another filter, let's say another filter, you will repeat the same thing for the input image, right?
00:27:52 So it feels like you just have one more column here.
00:27:56 So during the metric multiplication, we'll take this.
00:28:00 So this row, dot product with this column and dot product is another column.
00:28:06 So your output channel, how many filters of you, how many filters do you have with your P the number of columns?
00:28:12 Is this explanation clear?
00:28:18 So let's think about the, just think that simplifying operations into their building blocks, dot product.
00:28:24 And then how you put them into a list model.
00:28:30 That's about it for me.
00:28:36 Any questions?
00:28:42 Okay.
00:28:48 Now let's take a look at the last problem in the quiz.
00:28:54 So in this problem, you're asked to write, write loop, write two loops.
00:29:00 of 2D convolution, one for input stationary and the other is for weight stationary.
00:29:06 So the key point we want to test for this problem is the understanding of three types.
00:29:13 We only test two here, right?
00:29:20 Input stationary, output stationary, and weight stationary.
00:29:26 And generally, they look pretty similar, but you need to understand the ordering of different loop variables.
00:29:33 Now let's take a look at the first one, weight stationary.
00:29:40 So the output feature map specifies mpq here.
00:29:46 It's given as a formula, right?
00:29:53 The summation of like three loop variables, and the result is the multiple
00:30:00 multiplication of the image and the filter.
00:30:12 By saying weight stationary, we know that during the calculation of those loops, we need to make the weight, which is filter here, as a constant, which means we need to put those loop variables at the outer loop.
00:30:24 So filter here is specified by R and S, so those two variables, right, to the filter.
00:30:36 So here we put S and R at the very top, so the outer loop.
00:30:48 That makes sure that when we do the rest of four loops, R and S is a constant in terms of those
00:31:00 four loops.
00:31:06 And we can guarantee that this processing is weight stationary.
00:31:12 Yeah, you don't have to intentionally memorize so the order, right?
00:31:18 Because if you change to another notation, right?
00:31:24 Give a different naming, so you may get confused.
00:31:30 Just by looking at what you want to keep stationary, then you just put those two loop variables at the very outer loop.
00:31:36 So that's the key thing we want to make sure you understand.
00:31:42 Then for the second part, input stationary, as I said.
00:31:48 Input, which is an image, I, capital I here, is specified by H and W.
00:31:54 And straightforward, we put the loop variable, lowercase h.
00:32:00 and w at the outer loop.
00:32:07 However, the output image is specified by p and q, if we go back to the previous one, p and q.
00:32:15 And there's a relation between output dimension and the input dimension.
00:32:22 pq has relation with h and w, and the difference is governed by the kernel, which is s and r here.
00:32:30 So in order to use h and w notation, we need to come up with q and p, which is the output dimensions.
00:32:37 So we need to add those two lines here, and then for the rest of the formula, it is pretty straightforward.
00:32:45 So any questions at the moment?
00:32:52 Okay.
00:33:00 F is a function of m, c and r and s.
00:33:06 So, m and c also should come on top.
00:33:12 M and c come on top?
00:33:18 Yeah, basically weight is a function of m, c, r and s.
00:33:24 So, if weight should be stationary, m and c also should come at the top.
00:33:30 So the way you come up with weight theory is to make sure one weight keeps constant as long as possible.
00:33:36 If m and c changes, then the weight also changes.
00:33:42 It is not stationary.
00:33:48 Can you repeat a little bit about your argument?
00:33:54 The weight is a function of m, c, r and s.
00:34:00 Yeah, so if M and C changes, then weight also changes.
00:34:10 So it should be at the top, M and C, along with R and S.
00:34:20 Wait a minute.
00:34:30 So M is the number of filters, right?
00:34:40 And C is the number of channels.
00:34:50 So your argument is here, it's if we put M and C on top.
00:35:00 So anyone has argument regarding the student's question?
00:35:08 Or the question the students ask why not put MNC at the very top instead of writing it this way?
00:35:17 Wouldn't that be unnecessary?
00:35:25 I mean his question is that maybe PNQ need to move down and MNC need to move up.
00:35:34 PNQ has to be on the innermost loops and M, C, S and R have to be on the outer loops.
00:35:42 Oh that's what you mean.
00:35:51 So put MNC...
00:36:00 swarm pq and mc yeah because uh because in that case um uh the mc are the the index of big is not changing as frequently as other indices i see let me check for the for the input stationery for input we don't have the problem for input okay and i have one more slide about the output stationary for upper stationary we only need to put p and q at the very top right so this one looks good yeah
00:37:00 W is zero, and R is a positive number, so the P and Q will be a negative number.
00:37:06 W is zero, and?
00:37:13 And I for R is not zero.
00:37:20 Oh, if W is zero, there shouldn't be any loop.
00:37:26 You mean the capital W is equal to zero?
00:37:33 Oh, a lowercase.
00:37:40 Okay.
00:37:46 And for us, also starts from zero to a certain number, right?
00:37:53 Yeah, S is one dimension of the kernel of the filter.
00:37:59 and what if the i is, for example, two and w is zero, so the q will be minus two, right?
00:38:14 Oh, yeah, I understand your point is, what if the certain inputs, like the mapping of the input and output feature becomes invalid, right, illegal?
00:38:29 Yeah, if that's the case, we simply, we don't think that's a valid transformation, right?
00:38:44 I think if you write down, kind of give a simple example, then you will notice that the certain, for the output part, it simply cannot be generated from the input.
00:38:59 So, did I answer your question?
00:39:04 If it's an active number, it cannot be output.
00:39:09 Yeah, if Q is less than zero, right?
00:39:14 Then the index accessing becomes illegal.
00:39:19 Oh. Yeah.
00:39:24 So we're just, we're just assuming that our number here, we just only consider the qualitative number.
00:39:29 Yeah, here it doesn't check kind of the validity of Q and P here, right?
00:39:34 Because it's such a general formula.
00:39:39 We don't give a specific number.
00:39:44 But if you give like some real values here, then index select a negative number doesn't make any sense.
00:39:49 So, basically, we cannot do any kind of operation because we cannot find the corresponding pixel in the output feature map.
00:39:54 Yeah.
00:39:59 Also, I think if you want to fix this code, you can do an offset for h and w.
00:40:07 Like, say, h starts from maybe 2, but also w starts from 2w.
00:40:14 So in that case, there will be no negative q and p.
00:40:22 Because the input has to take even the less post top element also has to be considered.
00:40:29 But in that case, if w starts from...
00:40:37 Okay, I thought for the inner loop, then offset to the left at the top.
00:40:44 Because this w starts from 0, and s is moving from 0.
00:40:52 So in that case, if s is equal to 2, then q...
00:40:59 get what we feel like.
00:41:05 Yeah, I think.
00:41:10 So, if you don't ignore the photos and we look at just this formula output dimension is h minus r, w minus s.
00:41:16 Yes.
00:41:21 So, after writing this formula, we already assume the h minus r is, is not a negative number and w minus s is also not a negative number.
00:41:27 Right.
00:41:32 Otherwise, we cannot map one input image pixel to the pixel of the output map output feature map.
00:41:38 Right.
00:41:43 But just looking at the equation at the top.
00:41:49 Okay.
00:41:54 Oh, HNW, the order of HNW doesn't really matter here.
00:42:00 Yeah, so it's just height and width of the input image, the ordering, yeah.
00:42:04 R and S, yeah, the width and the height with respect to the same image or the filter.
00:42:09 Yeah, the ordering doesn't really matter.
00:42:13 So, what does it mean by M and C at the bottom?
00:42:18 Does it have anything to do with M and C?
00:42:23 Input stationary.
00:42:27 You mean the ordering of M and C or the position of M and C?
00:42:32 Yeah, M and C is connected.
00:42:36 Is it fine to be here or is it supposed to be all the way at the top?
00:42:41 Yeah, let's take a look.
00:42:46 So, input stationary, we need to make sure H and W at the very top, right?
00:42:50 The outer loop.
00:42:55 So, for R, S and I, it seems like this.
00:43:00 irrelevant, right, in terms of the input.
00:43:07 So we, yeah, so there should be like several ways in terms of those loop variables.
00:43:15 It looks like this is the biggest, so it's just see a hole in the bottom.
00:43:22 It's going to each channel, you've had desire.
00:43:30 So it's suppose the inputs on travel by each...
00:43:37 It's actually a real value.
00:43:45 Hmm, I see a point, yeah, I...
00:43:52 Yeah, since C is a variable here, we don't assume C is equal to...
00:44:00 channel is equal to one.
00:44:03 So we need to consider C, H, and W at the top, right?
00:44:07 C, H, W, R, S, and M.
00:44:11 What's up with that?
00:44:15 Or C, H, W, M.
00:44:18 Yes, C, H, W should be in a group and R, S, and M should be in the second group.
00:44:22 I think same thing with the weight statement, right?
00:44:26 So the weight statement.
00:44:30 Yeah, I think, yeah, that student already asked the question.
00:44:33 Yeah, thanks for pointing that out.
00:44:37 No, for stationary answer, M should be in the second group.
00:44:41 For output, P, Q, and M.
00:44:45 P, Q, and M.
00:44:48 Similar issue, right?
00:44:52 Similar issue as the previous one.
00:44:56 Oh, R, Q, I, C, H, and M.
00:45:00 or maybe we'll stick with this one.
00:45:04 Yeah, we will, yeah, I understand.
00:45:09 We will take a further look after discussion and we will update the solution, yeah, and also give the explanation.
00:45:14 Right, that sounds good to you?
00:45:19 Okay.
00:45:24 Okay.
00:45:29 So yeah, that's basically all I want to share for this discussion.
