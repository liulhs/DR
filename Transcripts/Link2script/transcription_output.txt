00:00:00 We'll begin with supervised learning problems, including classification and regression problems.
00:00:07 As I mentioned, machine learning may be unlike much of the programming that you might have done in the past.
00:00:15 Here, we can't simply tell the computer how to solve the problem, give it a precise algorithm to follow.
00:00:22 Instead, we'll tell it how to figure out how to solve the problem, a kind of metaprogramming task.
00:00:30 To do so, we design a flexible program called the learner, whose behavior can be modified by changing aspects of its operation, called the parameters.
00:00:37 The learner is a deterministic function that takes in the features of a new example and spits out a prediction for its target.
00:00:45 This input-output behavior is a function of these internal parameters.
00:00:52 We can then write a second program, the learning algorithm, which sets or modifies the parameters of the learner.
00:01:00 until it's doing a good job at predicting the training points that we've seen in the past, as measured by some score or cost function.
00:01:05 Some notation is useful.
00:01:10 The input features X will constitute all the information we have to make our prediction.
00:01:15 For example, if I want to predict whether you'll need to go to the hospital in the next year, I might use all the information I have about you, your age, your weight, height, the outcomes of recent medical tests, etc.
00:01:20 These are all available to me beforehand.
00:01:25 I input them into my learner and it outputs a prediction, yes or no, whether you'll be hospitalized.
00:01:30 Then later, I'll find out the answer.
00:01:35 Did you actually go to the hospital?
00:01:40 That's the true target value Y.
00:01:45 I call my prediction Y-hat, the estimate of Y, and by comparing Y and Y-hat, I can use it to determine my score.
00:01:50 I pay some penalty or cost if I'm wrong.
00:01:55 We'll denote the generic parameter
00:02:00 of our learner by theta.
00:02:06 Since data play a fundamental role in machine learning, we'll need some way of visualizing the training data along with the results of our learning algorithm.
00:02:13 Our training data consists of pairs of features and their associated target value.
00:02:20 So you can imagine plotting those data.
00:02:26 Since we can never really plot more than two dimensions, when we plot, we'll pretend that we have only one real valued x, and then the purpose of our learner is to map the feature value x into this target value y.
00:02:33 In other words, given a new feature, x new, we're supposed to predict what the y value associated with that feature would be.
00:02:39 Hopefully something that's reasonably similar to the training data.
00:02:46 Since it outputs a value for any x, our learner, whatever it is, defines a function from x to y.
00:02:53 And by evaluating every possible x,
00:02:59 We can trace out that function to see what our learner thinks is the actual relationship between X and Y.
00:03:07 In some cases, the shape or functional form of this function may be explicitly stated by the model, or it might be implicitly defined by the prediction program.
00:03:14 Let's see some simple examples of this.
00:03:22 An extremely simple predictor that we'll discuss in more detail soon is the nearest neighbor predictor.
00:03:29 Nearest neighbor is defined in a very simple rule.
00:03:37 Store the data, the red points in the scatter plot, in a database, and when asked to predict a new point X new, just find the most similar point in the database, looking only at the feature values, and predict whatever that closest point's value Y is.
00:03:44 So this is basically a memorize and regurgitate kind of procedure.
00:03:52 If we follow that procedure for every possible X, we'll trace out a set of predictions.
00:03:59 implicitly defines a function mapping X to Y.
00:04:08 You can notice the properties of this function.
00:04:17 It's flat, constant, near any training example, since that example is the nearest point, and it continues to predict that Y, and then it abruptly changes the moment that X becomes closer to the next training example, so halfway between the two points.
00:04:25 In contrast, here's another predictor we'll spend more time on, the linear predictor.
00:04:34 Here, the predictor evaluates a linear function of the feature, computing some value, say theta zero, and adding theta one times X, and then outputting that prediction.
00:04:42 Tracing out the values of this prediction on various values of X exactly shows this equation.
00:04:51 The functional form of f of X is explicitly defined within this procedure.
00:04:59 It's then quite easy to see what the effect of changing the parameters theta is.
00:05:07 It changes f of x within some parametric family of possible functions such as lines here.
00:05:14 The goal of the learning algorithm is then to modify these parameters until it finds a good function f of x within that family.
00:05:22 Obviously our predictions may not perfectly match the data points.
00:05:29 For any data point i, we can measure the difference between the observed target value yi and our prediction y-hat-i, and we just measure this difference and we can call it the error residual.
00:05:37 For an accurate predictor, these residuals should be small.
00:05:44 A common way to measure the total amount of error is the mean squared error, which just averages the square of the error residuals on the data.
00:05:52 So the squaring makes them all positive, averaging tells me overall
00:05:59 how I'm doing.
00:06:07 Again, since we're restricted to looking at 2D plots when we actually visualize things, we'll only be able to draw very simple examples for visualization.
00:06:14 For regression, we're essentially forced to look at one feature x versus the real value target y, even though in practice, of course, we'll probably use many more than one feature.
00:06:22 For classification, however, we're predicting a discrete valued y, often, say, a binary one, y equals 0 versus y equals 1, spam or not spam, say.
00:06:29 We can similarly plot x and y just like we did in regression, but since y is discrete, this makes for a pretty boring plot.
00:06:37 Instead, we can just use colors or symbols on the data points to indicate their y value, and then we only need to plot x.
00:06:44 We don't actually need to plot y at all.
00:06:52 So this means we can get away with plotting two.
00:06:59 features instead of just one.
00:07:08 We can plot a scatter plot of feature one versus feature two, and we indicate the class, the value of y, using a symbol or a color, here again, blue or red.
00:07:17 Again, any learner should learn a map, a way to map locations, x1, x2, into a prediction.
00:07:25 Now a discrete class, say 0 or 1, if we do that for every possible x1, x2 place on the plane, we'll now get a three-dimensional function whose value we can indicate with color.
00:07:34 Where now red shows all the points where we predict class 1, and blue, all those where we decide the other class, say minus 1.
00:07:42 An important concept for classification is that the function transitions immediately from one value to the other.
00:07:51 And this means that there's a set of points at which the function changes.
00:07:59 abruptly from one value to the other.
00:08:07 This is called the decision boundary.
00:08:14 In some sense, the learner, the classifier, can be characterized completely by the decision boundaries it's able to represent.
00:08:22 Note again that this is a function, y hat, of two variables, x1 and x2, whose value is visualized with color.
00:08:29 Again, in classification, it may be that our predictions don't necessarily match all of the observed data.
00:08:37 For classification problems, the most common method of assessing accuracy is called the error rate, which is the probability of making an incorrect prediction.
00:08:44 The empirical error rate is just given by the fraction of data points, i, at which our predictor, y hat, makes an incorrect prediction, i.e., a prediction that differs from the observed value, y.
00:08:52 So here, we have a blue point where we're predicting red, a red point where we're.
00:08:59 we're predicting blue, so we're making two out of 10 errors.
00:09:12 In summary, we started with supervised learning, which are prediction problems in which we're given examples of what our function should output in the form of training data with input features X and a labeled desired output value Y.
00:09:25 We saw two examples of these types of problems, regression problems, where we predict real valued numbers, and we'll visualize these as a function mapping a single feature X to its predicted Y value.
00:09:38 Classification problems, on the other hand, predict discrete valued targets, and so we'll usually visualize them using two input features, X1 and X2, and drawing the output Y values using colors or symbols or the decision boundary transitioning from one prediction value to another.
